
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />

        <title>OpenStack Swift Architecture | SwiftStack</title>

        <meta name="keywords" content="" />
        <meta name="description" content="" />
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="viewport" content="user-scalable=no, width=device-width, initial-scale=1.0, maximum-scale=1.0;" />

        <link rel="icon" type="image/png" href="/static/global/images/favicon.png">

        <link href="https://fnt.webink.com/wfs/webink.css/?project=69DFAD23-CF7E-499C-86F6-1F982FFF1E39&fonts=D4415F73-82C5-F5CD-E9F4-C6C761E998C6:f=EffraMedium-Italic,20C2EE91-4B20-1A0B-4F94-B9BD9FAAC3E5:f=Effra-BoldItalic,6EE89022-E6CF-348D-2980-778764FA1EA3:f=Effra-Light,7F15F306-2FC1-17E6-F8F3-ECC92BC5E5D4:f=Effra-Regular,15E11C5A-4E44-F8BD-8CFA-01C4976587E0:f=EffraHeavy-Italic,D2CD5BE2-DFFE-F195-E895-8C88F29CBB74:f=EffraLight-Italic,99AD4633-4BD2-D8E2-BF69-2BA4787B4EAB:f=Effra-Medium,E4813D4A-2BFD-6805-5A6D-D89719431F8B:f=Effra-Italic,52D1F108-9AD9-9E61-981E-06103031D38E:f=Effra-Bold,C0DE56B1-F475-8CCD-9C72-E3B5E05DB852:f=Effra-Heavy" rel="stylesheet" type="text/css"/>

        <link rel="stylesheet" type="text/less" href="/static/global/css/animate.min.css">

        <link rel="stylesheet" href="/static/CACHE/css/49eac57edb92.css" type="text/css" />

        

        <script type="text/javascript" src="/static/CACHE/js/3ace859aba86.js"></script>

        

        <!-- begin olark code -->
<script data-cfasync="false" type='text/javascript'>/*<![CDATA[*/window.olark||(function(c){var f=window,d=document,l=f.location.protocol=="https:"?"https:":"http:",z=c.name,r="load";var nt=function(){
f[z]=function(){
(a.s=a.s||[]).push(arguments)};var a=f[z]._={
},q=c.methods.length;while(q--){(function(n){f[z][n]=function(){
f[z]("call",n,arguments)}})(c.methods[q])}a.l=c.loader;a.i=nt;a.p={
0:+new Date};a.P=function(u){
a.p[u]=new Date-a.p[0]};function s(){
a.P(r);f[z](r)}f.addEventListener?f.addEventListener(r,s,false):f.attachEvent("on"+r,s);var ld=function(){function p(hd){
hd="head";return["<",hd,"></",hd,"><",i,' onl' + 'oad="var d=',g,";d.getElementsByTagName('head')[0].",j,"(d.",h,"('script')).",k,"='",l,"//",a.l,"'",'"',"></",i,">"].join("")}var i="body",m=d[i];if(!m){
return setTimeout(ld,100)}a.P(1);var j="appendChild",h="createElement",k="src",n=d[h]("div"),v=n[j](d[h](z)),b=d[h]("iframe"),g="document",e="domain",o;n.style.display="none";m.insertBefore(n,m.firstChild).id=z;b.frameBorder="0";b.id=z+"-loader";if(/MSIE[ ]+6/.test(navigator.userAgent)){
b.src="javascript:false"}b.allowTransparency="true";v[j](b);try{
b.contentWindow[g].open()}catch(w){
c[e]=d[e];o="javascript:var d="+g+".open();d.domain='"+d.domain+"';";b[k]=o+"void(0);"}try{
var t=b.contentWindow[g];t.write(p());t.close()}catch(x){
b[k]=o+'d.write("'+p().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};ld()};nt()})({
loader: "static.olark.com/jsclient/loader0.js",name:"olark",methods:["configure","extend","declare","identify"]});
/* custom configuration goes here (www.olark.com/documentation) */
olark.identify('8315-998-10-1635');/*]]>*/</script><noscript><a href="https://www.olark.com/site/8315-998-10-1635/contact" title="Contact us" target="_blank">Questions? Feedback?</a> powered by <a href="http://www.olark.com?welcome" title="Olark live chat software">Olark live chat software</a></noscript>
<!-- end olark code -->


        
        <!-- Google Analytics -->
        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-27843774-1']);
          _gaq.push(['_trackPageview']);
          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
        'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(ga, s);
          })();
        </script>
        
    </head>

    <body id=""  data-spy="scroll" data-target="#jobs_nav" data-offset="50">
        

        

<div id="nav" class="navbar navbar-default navbar-static-top">
    <div id="nav_container" class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">SwiftStack</a>
        </div>
        <div id="nav" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li><a class="product" href="/product/">SwiftStack Product</a></li>
                <li><a class="service" href="/swiftstack-management-service/">SwiftStack Management Service</a></li>
                <li><a class="support" href="/support/">Support</a></li>
                <li><a class="about" href="/about/">About</a></li>
                <li><a class="blog" href="/blog/">Blog</a></li>
                
                    
                    <li class="subnav-guest"><a href="/account/login/">Customer Log In</a></li>
                
            </ul>
            <ul class="nav navbar-subnav">
                
                    <li><a href="/customer/signup/">Free Signup and Download</a></li>
                    <li><a href="/account/login/">Customer Log In</a></li>
                
                <li><a href="/docs/">Docs</a></li>
                <li><a href="/openstack-swift/"><img width="19px" src="/static/global/images/openstack-o.png"></a></li>
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</div>




        <div class="push-down"></div>
        
<div class="container">
    <div class="row">
        <div class="col-lg-4">
            <div class="jumbotron small-hero">
                <h1>OpenStack Swift Architecture</h1>
            </div>
        </div>
        <div class="col-lg-8">
            <p>SwiftStack is an object storage system which includes an unmodified, 100% open-source release of OpenStack Swift at the core. In addition to Swift, SwiftStack provides extensive functionality for deploying, integrating, upgrading and managing single and multi-region Swift clusters, supported by SwiftStack 24x7. To understand how Swift works, this overview provides a high level description of it’s architecture. To learn more, we recommend <a href="https://swiftstack.com/book"><em>Object Storage with Swift</em></a> by Joe Arnold and the SwiftStack team, published by O’Reilly.</p>
<p><img alt="Object Storage With Swift cover" src="/static/global/images/book_cover_smaller.png" /></p>
<h1>OpenStack Swift Overview</h1>
<p>Swift is a widely-used and popular object storage system provided under the Apache 2 open source license. Swift is designed to store files, videos, analytics data, web content, backups, images, virtual machine snapshots and other unstructured data at large scale with high availability and 12 nines of durability. Swift can be configured with as few as two nodes with a handful of disk drives and scale to thousands of machines providing hundreds of Petabytes of storage distributed in geographically distant regions. Swift is designed from the ground up to scale horizontally without any single point of failure.</p>
<p>Here is a quick summary of Swift’s characteristics:</p>
<ul>
<li>
<p>Swift is open-source and freely available</p>
</li>
<li>
<p>Swift currently powers the largest object storage clouds, including Rackspace Cloud Files, the HP Cloud, IBM Softlayer Cloud and countless private object storage clusters</p>
</li>
<li>
<p>Swift can be used as a stand-alone storage system or as part of a cloud compute environment.</p>
</li>
<li>
<p>Swift runs on standard Linux distributions and on standard x86 server hardware</p>
</li>
<li>
<p>Swift&mdash;like Amazon S3&mdash;has an eventual consistency architecture, which make it ideal for building massive, highly distributed infrastructures with lots of unstructured data serving global sites.</p>
</li>
<li>
<p>All objects, or files, stored in Swift have a URL</p>
</li>
<li>
<p>Applications store and retrieve data in Swift via an industry-standard RESTful http API</p>
</li>
<li>
<p>Objects can have extensive metadata, which can be indexed and searched</p>
</li>
<li>
<p>All objects are stored with multiple copies and are replicated in as-unique-as-possible availability zones and/or regions</p>
</li>
<li>
<p>Swift is scaled by adding additional nodes, which allows for a cost-effective linear storage expansion</p>
</li>
<li>
<p>When adding or replacing hardware, data does not have to be migrated to a new storage system, i.e. there are no fork-lift upgrades</p>
</li>
<li>
<p>Failed nodes and drives can be swapped out while the cluster is running with no downtime. New nodes and drives can be added the same way.</p>
</li>
</ul>
<p>Swift enables users to store, retrieve, and delete objects (with their associated metadata) in containers via a RESTful HTTP API. Swift can be accessed with HTTP requests directly to the API or by using one of the many Swift client libraries such as Java, Python, Ruby, or JavaScript.  This makes it ideal as a primary storage system for data that needs to be stored and accessed via web based clients, devices and applications.</p>
<p>According to the industry analyst firm Gartner, Swift is the most widely used OpenStack project as it is commonly used with other cloud computing frameworks and as a stand-alone storage system. Swift’s open design also enables it to be integrated with enterprise authentication system and IT management tools. Swift’s increasing adoption is reflected by how many of the most popular backup and content management applications now support Swift’s HTTP API.</p>
<h2>Massive Scaling with Eventual Consistency</h2>
<p>Storage systems use one of two different architectural approaches to provide scalability, performance and resiliency: eventual consistency or strong consistency. Object storage systems such as Amazon S3 and Swift are eventually consistent, which provide massive scalability and ensures high availability to data even during hardware failures. Block storage systems and filesystems are strongly consistent, which is required for databases and other real-time data, but limits their scalability and may reduce availability to data when hardware failures occur.</p>
<table cellpadding="3"  cellspacing="3"  border="1"><tbody><tr><td><p ><strong>Eventually Consistent Storage Systems</strong></p></td><td ><p><span><strong>Strongly Consistent Storage Systems</strong></span></p></td></tr><tr ><td><p><span>Amazon S3</span></p><p>OpenStack Swift</p></td><td><p><span>Block storage</span></p><p>Filesystems</p></td></tr></tbody></table>

<p>A key reason why Swift serves so well for highly-available, unstructured application data is that its design, just like Amazon S3, incorporates eventual consistency. In Swift, objects are protected by storing multiple copies of data so that if one node fails, the data can be retrieved from another node. Even if multiple nodes fail, data will remain available to the user. Swift’s design for eventual consistency means that there is a guarantee that the system will eventually become consistent and have the most up-to-date version of data for all copies of the data but still provide availability to data should hardware fail.  This design makes it ideal when performance and scalability are critical, particularly for massive, highly distributed infrastructures with lots of unstructured data serving global sites.</p>
<p>Strong consistency is required when all reads needs to be guaranteed to return the most recent data. With this approach, all nodes in the storage system must be queried to ensure that all updates have been written to all nodes and the read is returning the most recent copy.  While databases with transactions require strong consistency, backup files, log files, and unstructured data do not need that same consistency. Based on this architecture, storage systems with strong consistency are difficult to scale, especially when it comes to multi-site configurations. This means that as the data grows, becomes more distributed&mdash;such as over multiple regions&mdash;or when there is a hardware failure, the chance for data not being available in a strongly consistent storage system increases. By using an eventual consistency design, Swift does not have those drawbacks, which makes it an ideal choice for for highly scalable, distributed storage of unstructured data.</p>
<p>Each of these architectural approaches has its own definition, appropriate use cases, and tradeoffs&mdash;all of which need to be understood to appropriately identify which architecture is most appropriate for your data.</p>
<h1>Swift Requests</h1>
<p>A foundational premise of Swift is that requests are made via HTTP using a RESTful API. All requests sent to Swift are made up of at least three parts:</p>
<ul>
<li>HTTP verb (e.g., GET, PUT, DELETE)</li>
<li>Authentication information</li>
<li>Storage URL</li>
<li>Optional: any data or metadata to be written</li>
</ul>
<p>The HTTP verb provides the action of the request. I want to PUT this object into the cluster. I want to GET this account information out of the cluster. etc.</p>
<p>The authentication information allows the request to be fulfilled.</p>
<p>A storage URL in Swift for an object looks like this:</p>
<pre><code>https://swift.example.com/v1/account/container/object
</code></pre>
<p>The storage URL has two basic parts: cluster location and storage location. This is because the storage URL has two purposes. It’s the cluster address where the request should be sent and it’s the location in the cluster where the requested action should take place.</p>
<p>Using the example above, we can break the storage URL into its two main<br />
parts:</p>
<ul>
<li>Cluster location: swift.example.com/v1/</li>
<li>Storage location (for an object): /account/container/object</li>
</ul>
<p>A storage location is given in one of three formats:</p>
<ul>
<li>
<p>/account</p>
<ul>
<li>The account storage location is a uniquely named storage area that contains the metadata (descriptive information) about the account itself as well as the list of containers in the account. </li>
<li>Note that in Swift, an account is not a user identity. When you hear account, think storage area.</li>
</ul>
</li>
<li>
<p>/account/container</p>
<ul>
<li>The container storage location is the user-defined storage area within an account where metadata about the container itself and the list of objects in the container will be stored.</li>
</ul>
</li>
<li>
<p>/account/container/object</p>
<ul>
<li>The object storage location is where the data object and its metadata will be stored.</li>
</ul>
</li>
</ul>
<p><img alt="account, container, object" src="/static/global/images/swift_architecture_aco.jpg" /></p>
<h2>The Swift HTTP API</h2>
<p>A command–line client interface, such as cURL, is all you need to perform simple operations on your Swift cluster, but many users require more sophisticated client applications. Behind the scenes, all Swift applications, including the command–line clients, use Swift's HTTP API to access the cluster.</p>
<p>Swift's HTTP API is RESTful, meaning that it exposes every container and object as a unique URL, and maps HTTP methods (like PUT, GET, POST, and DELETE) to the common data management operations (Create, Read, Update, and Destroy—collectively known as CRUD).</p>
<p>Swift makes use of most HTTP verbs including:</p>
<ul>
<li>
<p>GET&mdash;downloads objects, lists the contents of containers or accounts</p>
</li>
<li>
<p>PUT&mdash;uploads objects, creates containers, overwrites metadata headers</p>
</li>
<li>
<p>POST&mdash;creates containers if they don't exist, updates metadata (accounts or containers), overwrites metadata (objects)</p>
</li>
<li>
<p>DELETE&mdash;deletes objects and containers that are empty</p>
</li>
<li>
<p>HEAD&mdash;retrieves header information for the account, container or object.</p>
</li>
</ul>
<p>Let's look at some sample HTTP GET requests to see how cURL would be used for objects, containers, or accounts. Here we will use the storage URL http://swift.example.com/v1/account. Common tasks a user might perform with GET include:</p>
<p>Downloading an object with a GET request to the object’s storage URL:</p>
<pre><code>curl -X GET https://swift.example.com/v1/account/container/object
</code></pre>
<p>Listing objects a container with a GET request to the container’s storage URL:</p>
<pre><code>curl -X GET https://swift.example.com/v1/account/container/
</code></pre>
<p>Listing of all containers in an account with a GET request to the account’s storage URL:</p>
<pre><code>curl -X GET https://swift.example.com/v1/account/
</code></pre>
<p>While a CLI like cURL is all that is needed to perform simple operations on a Swift cluster, many people will want to use Swift client libraries to have applications make those underlying HTTP requests.</p>
<h2>Client Libraries</h2>
<p>Application developers can construct HTTP requests and parse HTTP responses using their programming language's HTTP client or they may choose to use open-source language bindings to abstract away the details of the HTTP interface.</p>
<p>Open–source client libraries are available for most modern programming languages, including:</p>
<ul>
<li><a href="https://pypi.python.org/pypi/python-swiftclient">Python</a></li>
<li>Ruby</li>
<li>PHP</li>
<li>C#/.NET</li>
<li>Java</li>
<li>JavaScript</li>
</ul>
<p>Once a request is sent to the cluster, what happens to it?</p>
<p>Before we take a look at how the cluster handles a request, first let’s look at how a cluster is put together.</p>
<h1>Swift Overview&mdash;Processes</h1>
<p>A Swift cluster is the distributed storage system used for object storage. It is a collection of machines that are running Swift’s server processes and consistency services. Each machine running one or more Swift’s processes and services is called a node.</p>
<p>The four Swift server processes are proxy, account, container and object. When a node has only the proxy server process running it is called a proxy node. Nodes running one or more of the other server processes (account, container, or object) will often be called a storage node. Storage nodes contain the data that incoming requests wish to affect, e.g. A PUT request for an object would go to the appropriate nodes running the object server processes. Storage nodes will also have a number of other services running on them to maintain data consistency.</p>
<p><img alt="services" src="/static/global/images/swift_architecture_services.jpg" /></p>
<p>When talking about the same server processes running on the nodes in a cluster we call it the server process layer. e.g., proxy layer, account layer, container layer or object layer.</p>
<p>Let’s look a little more closely at the server process layers.</p>
<h2>Server Process Layers</h2>
<h3>Proxy Layer</h3>
<p>The Proxy server processes are the public face of Swift as they are the only ones that communicate with external clients. As a result they are the first and last to handle an API request. All requests to and responses from the proxy use standard HTTP verbs and response codes.</p>
<p>Proxy servers use a shared-nothing architecture and can be scaled as needed based on projected workloads. A minimum of two proxy servers should be deployed for redundancy. Should one proxy server fail, the others will take over.</p>
<p>For example if a valid request is sent to Swift, then the proxy server will verify the request and then determine the correct storage nodes responsible for the data (based on a hash of the object name) and sends the request to those servers concurrently. If one of the primary storage nodes is unavailable, the proxy will choose an appropriate hand-off node to send the request to. The nodes will return a response and the proxy will in turn return all responses (and data if it was requested) to the requester.</p>
<p>The proxy server process is looking up multiple locations because Swift provides data durability by writing multiple–typically 3–complete copies of the data and storing them in the system.</p>
<h3>Account Layer</h3>
<p>The account server process handles requests to regarding metadata for the individual accounts or the list of the containers within each account. This information is stored by the account server process in SQLite databases on disk.</p>
<h3>Container Layer</h3>
<p>The container server process handles requests regarding container metadata or the list of objects within each container. It’s important to note that the list of objects doesn’t contain information about the location of the object, simply that it belong to a specific container.  Like accounts, the container information in stored as SQLite databases.</p>
<h3>Object Layer</h3>
<p>The object server process is responsible for the actual storage of objects on the drives of it’s node. Objects are stored as binary files on the drive using a path that is made up in part of its associated partition (which we will discuss shortly) and the operation's timestamp.  The timestamp is important as it allows the object server to store multiple versions of an object but only providing the latest version for a typical download (GET) request. The object’s metadata (standard and custom) is stored in the file’s extended attributes (xattrs) which means the data and metadata are stored together and copied as a single unit.</p>
<h2>Consistency Services</h2>
<p>A key aspect of Swift is that it acknowledges that failures happen and is built to work around them. When account, container or object server processes are running on node, it means that data is being stored there.  That means consistency services will also be running on those nodes to ensure the integrity and availability of the data.</p>
<p>The two main consistency services are auditors and replicators. There are also a number of specialized services that run in support of individual server process, e.g., the account reaper that runs where account server processes are running.</p>
<h3>Auditors</h3>
<p>Auditors run in the background on every storage node in a Swift cluster and continually scan the disks to ensure that the data stored on disk has not suffered any bit-rot or file system corruption. There are account auditors, container auditors and object auditors which run to support their corresponding server process.</p>
<p>If an error is found, the auditor moves the corrupted object to a quarantine area.</p>
<p><img alt="quarantine" src="/static/global/images/swift_architecture_quarantine.jpg" /></p>
<p><em>Auditors examine data and move to a quarantine area if any errors are found</em></p>
<h3>Replicators</h3>
<p>Account, container, and object replicator processes run in the background on all nodes that are running the corresponding services. A replicator will continuously examine its local node and compare the accounts, containers, or objects against the copies on other nodes in the cluster. If one of other nodes has an old or missing copy, then the replicator will send a copy of its local data out to that node.  Replicators only push their local data out to other nodes; they do not pull in remote copies in if their local data is missing or out of date.</p>
<p>The replicator also handles object and container deletions. Object deletion starts by creating a zero-byte tombstone file that is the latest version of the object. This version is then replicated to the other nodes and the object is removed from the entire system.</p>
<p>Container deletion can only happen with an empty container. It will be marked as deleted and the replicators push this version out.</p>
<p><img alt="replicator" src="/static/global/images/swift_architecture_replicator.jpg" /></p>
<p><em>Replicators examine the checksums of partitions</em></p>
<h3>Specialized Consistency Services</h3>
<h4>Container and Object Updaters</h4>
<p>The container updater service runs to support accounts, it will update:</p>
<ul>
<li>container listings in the accounts</li>
<li>account metadata<ul>
<li>object count</li>
<li>container count</li>
<li>bytes used</li>
</ul>
</li>
</ul>
<p>The object updater runs to support containers, but as a redundant service. The object server process is the primary updater. Only if it fails with an update attempt will the object updater take over and then update:</p>
<ul>
<li>object listing in the containers</li>
<li>container metadata<ul>
<li>object count</li>
<li>bytes used</li>
</ul>
</li>
</ul>
<h4>Object Expirer</h4>
<p>The object expirer service purges data that is designated as expired.</p>
<h4>Account Reaper</h4>
<p>When an account reaper service makes its rounds on a node and finds an account marked as deleted, it starts stripping out all objects and containers associated with the account. With each pass it will continue to dismantle the account until it is emptied and removed. The reaper has a delay value that can be configured so the reaper will wait before it starts deleting data&mdash;this is used to guard against erroneous deletions.</p>
<h1>Swift Overview&mdash;Cluster Architecture</h1>
<h2>Nodes</h2>
<p>A node is a machine that is running one or Swift processes. When there are multiple nodes running that provide all the processes needed for Swift to act as a distributed storage system they are considered to be a cluster.</p>
<p>Within a cluster the nodes will also belong to two logical groups: regions and nodes. Regions and nodes are user-defined and identify unique characteristics about a collection of nodes-- usually geography location and points of failure, such as all the power running to one rack of nodes. These ensure Swift can place data across different parts of the cluster to reduce risk.</p>
<h2>Regions</h2>
<p>Regions are user-defined and usually indicate when parts of the cluster are physically separate --usually a geographical boundary. A cluster has a minimum of one region and there are many single region clusters as a result. A cluster that is using two or more regions is a multi-region cluster.</p>
<p>When a read request is made, the proxy layer favors nearby copies of the data as measured by latency. When a write request is made the proxy layer, by default, writes to all the locations simultaneously. There is an option called write affinity that when enabled allows the cluster to write all copies locally and then transfer them asynchronously to the other regions.</p>
<p><img alt="regions" src="/static/global/images/swift_architecture_regions.jpg" /><br />
<em>Storage zones can be deployed across geographic regions</em></p>
<h2>Zones</h2>
<p>Within regions, Swift allows allows availability zones to be configured to isolate failure boundaries. An availability zone should be defined by a distinct set of physical hardware whose failure would be isolated from other zones. In a large deployment, availability zones may be defined as unique facilities in a large data center campus. In a single datacenter deployment, the availability zones may be different racks. While there does need to be at least one zone in a cluster, it is far more common for a cluster to have many zones.</p>
<p><img alt="zones" src="/static/global/images/swift_architecture_zones.jpg" /></p>
<p><em>Storage can be placed in distinct fault-tolerant zones</em></p>
<h1>Swift Overview&mdash;Data Placement</h1>
<p>We have referenced several times that there are several locations for data because Swift makes copies and stores them across the cluster. This section cover the process in further detail.</p>
<p>When the server processes or the consistency services need to locate data it will look at the storage location (/account, /account/container, /account/container/object) and consult one of the three rings: account ring, container ring or object ring.</p>
<p>Each Swift ring is a modified consistent hashing ring that is distributed to every node in the cluster. The boiled down version is that a modified consistent hashing ring contains a pair of lookup tables that the Swift processes and services use to determine data locations.  One table has the information about the drives in the cluster and the other has the table used to look up where any piece of account, container or object data should be placed. That second table&mdash;where to place things&mdash;is the more complicated one to populate. Before we discuss the rings and how they are built any further we should cover partitions and replicas as they are critical concepts to understanding the rings.</p>
<h2>Partitions</h2>
<p>Swift wants to store data uniformly across the cluster and have it be available quickly for requests. Never ones to shy away from a good idea, the developers of Swift have tried various methods and designs before settling on the current variation of the modified consistent hashing ring.</p>
<p>Hashing is the key to the data locations. When a process, like a proxy server process, needs to find where data is stored for a request, it will call on the appropriate ring to get a value that it needs to correctly hash the storage location (the second part of the storage URL). The hash value of the storage location will map to a partition value.</p>
<p>This hash value will be one of hundreds or thousands of hash values that could be calculated when hashing storage locations. The full range of possible hash values is the “hashing ring” part of a modified consistent hashing ring.</p>
<p>The “consistent” part of a modified consistent hashing ring is where partitions come into play. The hashing ring is chopped up into a number of parts, each of which gets a small range of the hash values associated to it. These parts are the partitions that we talk about in Swift.</p>
<p>One of the modifications that makes Swift’s hash ring a modified consistent hashing ring is that the partitions are a set number and uniform in size. As a ring is built the partitions are assigned to drives in the cluster. This implementation is conceptually simple—a partition is just a directory sitting on a disk with a corresponding hash table of what it contains.</p>
<p><img alt="partition" src="/static/global/images/swift_architecture_partition.jpg" /></p>
<p>The relationship of a storage node, disk and a partition. Storage nodes have disks. Partitions are represented as directories on each disk.</p>
<p>While the size and number of partitions does not change, the number of drives in the cluster does. The more drives in a cluster the fewer partitions per drive. For a simple example, if there were 150 partitions and 2 drives then each drive would have 75 partitions mapped to it. If a new drive is added then each of the 3 drives would have 50 partitions.</p>
<p><img alt="ring" src="/static/global/images/swift_architecture_ring.jpg" /></p>
<p>New storage will receive a proportion of the existing partition space</p>
<p>Partitions are the smallest unit that Swift likes to work with&mdash;data is added to partitions, consistency processes will check partitions, partitions are moved to new drives etc. By having many of the actions happen at the partition level Swift is able to keep processor and network traffic low. This also means that as the system scales up, behavior continues to be predictable as the number of partitions remains fixed.</p>
<h2>Replicas</h2>
<p>Swift’s durability and resilience to failure depends in large part on its replicas. The more replicas used, the more protection against losing data when there is a failure. This is especially true in clusters that have separate datacenters and geographic regions to spread the replicas across.</p>
<p>When we say replicas, we mean partitions that are replicated. Most commonly a replica count of three is chosen. During the initial creation of the Swift rings, every partition is replicated and each replica is placed as uniquely as possible across the cluster. Each subsequent rebuilding of the rings will calculate which, if any, of the replicated partitions need to be moved to a different drive. Part of partition replication including designating handoff drives. When a drive fails, the replication/auditing processes notice and push the missing data to handoff locations. The probability that all replicated partitions across the system will become corrupt (or otherwise fail) before the cluster notices and is able to push the data to handoff locations is very small, which is why we say that Swift is durable.</p>
<p>Previously we talked about a proxy server processes using a hash of the data’s storage location to determine where in the cluster that data was located. We can now be more precise and say that the proxy server process is locating the three replicated partitions each of which contains a copy of the data.</p>
<p><img alt="ring to partition" src="/static/global/images/swift_architecture_ring_map.jpg" /></p>
<p><em>An object ring enables a path /account/container/object path to be mapped to partitions</em></p>
<h2>The Rings</h2>
<p>With partitions and replicas defined, we can take a look the data structure of the rings. Each of the Swift rings is a modified consistent hashing ring. This ring data structure includes the partition shift value which processes and services use to determine the hash of a storage location. It also has two important internal data structures: the devices list and the devices lookup table</p>
<p>The devices list is populated with all the devices that have been added to a special ring building file. Each entry for a drive includes its ID number, zone, weight, IP, port, and device name.</p>
<p>The devices lookup table has one row per replica and one column per partition in the cluster. This generates a table that is typically three rows by thousands of columns. During the building of a ring, Swift calculates the best drive to place each partition replica on using the drive weights and the unique-as-possible placement algorithm. It then records that drive in the table.</p>
<p>Referring back to that proxy server process that was looking up data.  The proxy server process calculated the hash value of the storage location which maps to a partition value. The proxy server process uses this partition value on the Devices lookup table. The process will check the first replica row at the partition column to determine the device ID where the first replica is located. The process will search the next two rows to get the other two locations. In our figure the partition value was 2 and the process found that the data was located on drives 1, 8 and 10.</p>
<p><img alt="replicas" src="/static/global/images/swift_architecture_ring_replicas.png" /></p>
<p>The proxy server process can then make a second set of searches on the Devices list to get the information about all three drives, including ID numbers, zones, weights, IPs, ports, and device names. With this information the process can call on the correct drives. In our examle figure, the process determined the ID number, zone, weight, IP, port, and device name for device 1.</p>
<p><img alt="devices" src="/static/global/images/swift_architecture_ring_devices.png" /></p>
<p>Let's take a closer look at how partitions are calculated and how they are mapped to drives.</p>
<h2>Building a Ring</h2>
<p>When a ring is being built total number of partitions is calculated with a value called the partition power. Once set during the initial creation of a cluster, the partition power should not be changed. This means the total number of partitions will remain the same in the cluster. The formula used is 2 raised to the partition power. For example if a partition power of 13 is picked, then the total partitions in a cluster is 2<sup>13</sup> or 8192.</p>
<p>During the very first building of the rings, all partitions will need to be assigned to the available drives. When the rings are rebuilt, called rebalancing, only partitions that need to be moved to different drives, usually because drives were added or removed, will be affected.</p>
<p>The placement of the partitions is determined by a combination of replica count, replica lock, and data distribution mechanisms such as drive weight and unique-as-possible placement.</p>
<h3>Replica Count</h3>
<p>It should be remembered that it is not just partitions but also the replicated copies that must be placed on the drives. For a cluster with a partition power of 13 and a replica count of 3, there will be a total of 8192 partitions and 24576 (which is 3*8192) total replicated partitions that will be placed on the drives.</p>
<h2>Replica Lock</h2>
<p>While a partition is being moved, Swift will lock that partition's replicas so that they are not eligible to be moved for a period of time to ensure data availability. This is used both when the rings are being updated as well as operationally when data is moved. It is not used with the very first building of the rings. The exact length of time to lock a partition is set by the min_part_hours configuration option which often set to a default of 24 hours.</p>
<h2>Weight</h2>
<p>Swift uses a value called weight for each a drive in the cluster. This user-defined value, set when the drive is added, is the drive's relative weight compared to the other drives in the ring. The weight helps the cluster calculate how many partitions should be assigned to the drive. The higher the weight, the greater number of partitions Swift should assign to the drive.</p>
<h2>Unique-as-possible Placement</h2>
<p>To ensure the cluster is storing data evenly across its defined spaces (regions, zones, nodes, and disks), Swift assigns partitions using unique-as-possible placement algorithm. The algorithm identifies the least-used place in the cluster to place a partitions. First it looks for the least used region, if all the regions contain a partition then it looks for the least used zone, then server (IP:port), and finally the least-used disk and places the partition there. The least-used formula also attempts to place the partition replicas as far from each other as possible.</p>
<p>Once Swift calculates and records the placement of all partitions, the ring can be created. One account ring will be generated for a cluster and be used to determine where the account data is located. One container ring will be generated for a cluster and be used to determine where the container data is located. One object ring will be generated for a cluster and be used to determine where the object data is located.</p>
<p><img alt="partition space" src="/static/global/images/swift_architecture_ring_partition_space.jpg" /></p>
<p><em>The partition space is distributed across all available storage.</em></p>
<p>There is a great deal to say about how Swift works internally and we encourage those who are interested in learning more to read the OpenStack Swift documentation.</p>
<h1>Swift HTTP Requests: A Closer Look</h1>
<p>Now that we have covered the basics of Swift, we can take all look at how to all works together. Let’s take a closer look at how a cluster handles an incoming request.</p>
<p>As mentioned earlier, all requests sent to Swift are made up of at least three parts:</p>
<ul>
<li>HTTP verb (e.g., GET, PUT, DELETE)</li>
<li>Authentication information</li>
<li>
<p>Storage URL (swift.example.com/v1/account)</p>
</li>
<li>
<p>Cluster location: swift.example.com/v1/</p>
</li>
<li>
<p>Storage location (for an object): /account/container/object</p>
</li>
<li>
<p>Optional: any data or metadata to be written</p>
</li>
</ul>
<p>The request is sent to the cluster location which is a hook into the proxy layer. The proxy layer first handles the request verifying auth.  Once the request passes auth the proxy layer will route the incoming request to the appropriate storage nodes.</p>
<p><img alt="requests" src="/static/global/images/swift_architecture_requests.jpg" /></p>
<p>For our examples below, we will assume that the client has valid credentials and permission for the actions being taken and that the cluster uses three replicas.</p>
<h2>Example: PUT</h2>
<p>A client uses the Swift API to make an HTTP request to PUT an object into an existing container. Swift receives the request and one of the proxy server processes will handle it. First the proxy server process will verify auth and then it will take the hash of the storage location and look up all three partitions locations, the drives, of where the data should be stored using the object ring. The process then uses the object ring to look up the IP and other information for those three devices.</p>
<p>Having determined the location of all three partitions, the proxy server process sends the object to each storage node where it is placed in the appropriate partition. When a quorum is reached, in this case at least two of the three writes are returned as successful, then the proxy server process will notify the client is notified that the upload was successful.</p>
<p><img alt="quorum writes" src="/static/global/images/swift_architecture_quorum.jpg" /></p>
<p><em>Quorum writes ensure durability</em></p>
<p>Next, the container database is updated asynchronously to reflect the new object in it.</p>
<h2>Example: GET</h2>
<p>A client uses the Swift API to make an HTTP request to GET an object from the cluster. Swift receives the request and one of the proxy server processes will handle it. First the proxy server process will verify auth and then it will take the hash of the storage location and look up all three partitions locations, the drives, of where the data should be stored using the object ring. The process then uses the object ring to look up the IP and other information for those three devices.</p>
<p>Having determined the location of all three partitions, the proxy server process will request the object from each storage node and return the object to the client.</p>
<p><img alt="get" src="/static/global/images/swift_architecture_get.jpg" /></p>
<p><em>Get requests are handled by one of the storage nodes</em></p>
<h1>Natively Built for the Web</h1>
<p>Beyond the Swift’s core functionality to store and serve data durably at large scale, Swift has many built-in features that makes it easy for web application developers and end-users to use. Since Swift is written in Python, it is very flexible and can be extended with middleware that plug into the WSGI pipeline. By adding middleware in Swift’s proxy layer, it can be extended with additional features not possible in other storage systems and integrations with enterprise authentication systems, such as LDAP and Active Directory. Some of these features and integrations include:</p>
<ul>
<li>
<p>Static website hosting&mdash;Users can host static websites, including javascript and css, directly from Swift. Swift also supports custom error pages and auto-generate listings.</p>
</li>
<li>
<p>Automatically expiring objects&mdash;Objects can be given an expiry time after which they are no longer available and will be deleted. This is very useful for preventing stale data from remaining available and to comply with data retention policies.</p>
</li>
<li>
<p>Time-limited URLs&mdash;URLs can be generated that are valid for only a limited period of time. These URLs can prevent hotlinking or enable temporary write permissions without needing to hand out full credentials to an untrusted party.</p>
</li>
<li>
<p>Quotas&mdash;Storage limits can be set on containers and accounts.</p>
</li>
<li>
<p>Direct-from-HTML-form uploads&mdash;Users can generate web forms that upload data directly into Swift so that it doesn’t have to be proxied through another server</p>
</li>
<li>
<p>Versioned writes&mdash;Users can write a new version of an object and keep all older versions of the object.</p>
</li>
<li>
<p>Support for chunked Transfer-Encoding&mdash;Users can upload data to Swift without knowing ahead of time how large the object is.</p>
</li>
<li>
<p>Multi-Range reads&mdash;Users can read one or more sections of an object with only one read request</p>
</li>
<li>
<p>Access control lists&mdash;Users can configure access to their data to enable or prevent others ability to read or write the data</p>
</li>
<li>
<p>Programmatic access to data locality&mdash;Deployers can integrate Swift with systems like Hadoop and take advantage of locality information to lower network requirements when processing data.</p>
</li>
</ul>
<h1>Upcoming Features</h1>
<p>The Swift developer community is also working on many additional features that will be added to upcoming releases of Swift, such as storage policies and support for erasure coding. Storage policies will allow deployers and users to choose what hardware data is on, how the data is stored across that hardware, and which region the data resides.  The erasure coding support in Swift will enable deployers to store data with erasure coding instead of or in addition to Swift’s standard replica model. The design goal is to be able to have erasure-coded storage plus replicas coexisting in a single Swift cluster. This will allow a choice in how to store data and will allow applications to make the right tradeoffs based on their use case.</p>
<h1>SwiftStack, with Swift at the Core</h1>
<p>SwiftStack is an object storage system which includes an unmodified, 100% open-source release of OpenStack Swift at the core. In addition to Swift, SwiftStack provides extensive functionality for deploying, integrating, upgrading and managing single and multi-region Swift clusters coupled with enterprise support. SwiftStack object storage software is licensed separately and provides the following functionality above and beyond core Swift:</p>
<ul>
<li>Swift cluster management &amp; configuration</li>
<li>Automated install process</li>
<li>Drive inventory management</li>
<li>Gradual capacity adjustment</li>
<li>Global cluster management</li>
<li>Capacity Management</li>
<li>No downtime, rolling upgrades</li>
<li>Enterprise LDAP and Active Directory integration</li>
<li>Utilization API</li>
</ul>
<p>A key characteristic of SwiftStack is to decouple the control, management and configuration of the Swift storage nodes from the physical hardware. Although the actual storage services run on the servers where Swift is installed,deployment, management, and monitoring are conducted out-of-band by a separate storage controller, which can manages one or more clusters.</p>
<p><img alt="controller" src="/static/global/images/swift_architecture_control.jpg" /></p>
<p>This approach has many benefits because operators can now manage multiple, geographically distributed storage clusters from a single management system, dynamically tune clusters to optimize performance, respond to hardware failures, upgrade clusters while they are still running&mdash;all driven programmatically for the entire storage tier, independent of where the storage resources are deployed.</p>
<h1>To Learn More?</h1>
<p>If you’d like to learn more about OpenStack Swift and SwiftStack, contact us at <a href="mailto:contact@swiftstack.com">contact@swiftstack.com</a>.</p>
        </div>
    </div>
</div>


        <div id="footer">
            
    <div class="container">
        <div class="row">
            <div class="col-lg-5 col-md-12 col-sm-12 col-xs-12">
                <div id="homepage_footer_feature" class="text-center">
                    <br />
                    <br />
                    <p><img src="/static/global/images/storage-powered-by-openstack.png" alt="Storage powered by OpenStack"></p>
                    <br />
                    <br />
                </div>
            </div>
            <div class="col-lg-2 col-md-6 col-sm-6 col-xs-6">
                <p><strong>Product</strong></p>
                <ul>
                    <li><a href="/product/">SwiftStack Product</a></li>
                    <li><a href="/swiftstack-management-service/">SwiftStack Management Service</a></li>
                    <li><a href="/support/">Support</a></li>
                    <li><a href="/docs/">Documentation</a></li>
                    <li><a href="/openstack-swift/">OpenStack Swift</a></li>
                </ul>
            </div>
            <div class="col-lg-2 col-md-6 col-sm-6 col-xs-6">
                <p><strong>Company</strong></p>
                <ul>
                    <li><a href="/about/">About</a></li>
                    <li><a href="/blog/">Blog</a></li>
                    <li><a href="/news/">News</a></li>
                    <li><a href="/events/">Events</a></li>
                    <li><a href="/jobs/">Jobs</a></li>
                    <li><a href="/book/">Book: OpenStack Swift by O'Reilly</a></li>
                </ul>
            </div>

            <div class="col-lg-2 col-md-6 col-sm-6 col-xs-6">
                <p><strong>Contact</strong></p>
                <ul>
                    <li><a href="http://www.twitter.com/swiftstack"><i class="icon-twitter"></i> Twitter</a></li>
                    <li><a href="https://plus.google.com/102762415907942415236" rel="publisher">Google+</a></li>
                    <li><a href="/contact/">Contact</a></li>
                </ul>
                <p><strong>Legal</strong></p>
                <ul>
                    <li><a href="/website-terms-of-use/">Website terms of use</a></li>
                    <li><a href="/privacy/">Privacy policy</a></li>
                    <li><a href="/support-policy/">Support policy</a></li>
                </ul>

            </div>
        </div>
    </div>


        </div>

    
      <!-- Start of Async HubSpot Analytics Code -->
    <script type="text/javascript">
        (function(d,s,i,r) {
            if (d.getElementById(i)){return;}
            var n=d.createElement(s),e=d.getElementsByTagName(s)[0];
            n.id=i;n.src='//js.hs-analytics.net/analytics/'+(Math.ceil(new Date()/r)*r)+'/383367.js';
            e.parentNode.insertBefore(n, e);
        })(document,"script","hs-analytics",300000);
    </script>
<!-- End of Async HubSpot Analytics Code -->
    
    

    </body>
</html>
