From mcolonno at stanford.edu  Fri Feb  1 00:00:42 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 31 Jan 2013 16:00:42 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <CAA3XVTzPQCE1eY6WP5hD97jJaKC6WVKjDPWW6rL2EBXSU_m0+A@mail.gmail.com>
References: <007f01cdffd7$504ff000$f0efd000$@hpccloudsolutions.com>
	<CAA3XVTzPQCE1eY6WP5hD97jJaKC6WVKjDPWW6rL2EBXSU_m0+A@mail.gmail.com>
Message-ID: <04d901ce000f$2e35a4a0$8aa0ede0$@stanford.edu>

            Do I need to blow up and rebuild the brick to make that happen
or can this be set on the fly? Possibly relevant fact: I do not have my IB
fabric in place yet but I'm happy to use IPoIB for this deployment when I
do. I included it as a placeholder. 

 

            Related: any official word on full RDMA support in Gluster 3.x? 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Bryan Whitehead
Sent: Thursday, January 31, 2013 3:52 PM
To: Michael Colonno
Cc: gluster-users
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

 

remove the transport rdma and try again. When using RDMA I've also had
extremely bad CPU eating issues.

 

I currently run gluster with IPoIB to get the speed of infiniband and the
non-crazy cpu usage of rdma gluster.

 

On Thu, Jan 31, 2013 at 9:20 AM, Michael Colonno
<mike at hpccloudsolutions.com> wrote:

        Hi All ~

        I created an eight-brick gluster 3.3.1 volume (2x replication) on
eight CentOS 6.3 x64 systems. I was able to form and start the volume
without issue. I was also able to mount it through /etc/fstab as a native
glusterfs mount. I have a couple questions issues at this point:

        - On a client machine, "glusterfs" is not recognized as a valid type
unless gluster-server is installed. This seems to contradict the
documentation - wanted to make sure I'm not doing something wrong. More a
clarification than issue here.

        - The glusterfs process is taking between 50% and 80% CPU on both
the brick and client systems (these are fairly powerful, brand new servers).


        - No doubt linked to the above, the mounted filesystem hangs
indefinitely when accessed. I tried an "ls -a" on the mounted filesystem,
for example, which hangs forever. I tested this by mounting a brick system
to itself and to a client which is not a brick and the same behavior was
observed. Both were glusterfs mounts.

        There is nothing special about my deployment except for the use of
transport = tcp,rdma. I am running on Ethernet now but will be migrating to
Infiniband after this is debugged.

        Thanks for any advice,
        ~Mike C.




_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130131/c9b2b096/attachment.html>

From lanning at lanning.cc  Fri Feb  1 00:28:12 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Thu, 31 Jan 2013 16:28:12 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>
	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>
Message-ID: <510B0C1C.2060905@lanning.cc>

On 01/31/13 15:54, Michael Colonno wrote:
> Updating this thread: I tried to switch over to NFS mounting. When I try
> to mount the volume with the following line in fstab:
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0

node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0

-- 
Mr. Flibble
King of the Potato People

From joe at julianfamily.org  Fri Feb  1 00:34:50 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 31 Jan 2013 16:34:50 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <04d901ce000f$2e35a4a0$8aa0ede0$@stanford.edu>
References: <007f01cdffd7$504ff000$f0efd000$@hpccloudsolutions.com>
	<CAA3XVTzPQCE1eY6WP5hD97jJaKC6WVKjDPWW6rL2EBXSU_m0+A@mail.gmail.com>
	<04d901ce000f$2e35a4a0$8aa0ede0$@stanford.edu>
Message-ID: <510B0DAA.4010606@julianfamily.org>

On 01/31/2013 04:00 PM, Michael Colonno wrote:
>
> Related: any official word on full RDMA support in Gluster 3.x?
>
> Thanks,
>
> ~Mike C.
>
>
The last official word I got from Raghavendra Bhat defined the state of 
RDMA in 3.3.1 as "Tech Preview". As I understand it, this is a Red Hat 
term for "it hasn't gone through sufficient QA testing for us to support 
it commercially". There's no less support in 3.3 than there was in 3.1 
or 3.2.

The people that I've seen use RDMA successfully used bleeding edge 
kernels and drivers, fwiw.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130131/d186e8b0/attachment.html>

From mcolonno at stanford.edu  Fri Feb  1 00:50:14 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 31 Jan 2013 16:50:14 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <510B0C1C.2060905@lanning.cc>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>
	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>
	<510B0C1C.2060905@lanning.cc>
Message-ID: <054801ce0016$19464840$4bd2d8c0$@stanford.edu>

	Tried the proto=tcp setting, same error unfortunately...

	Thanks,
	~Mike C. 

-----Original Message-----
From: Robert Hajime Lanning [mailto:lanning at lanning.cc] 
Sent: Thursday, January 31, 2013 4:28 PM
To: Michael Colonno
Cc: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

On 01/31/13 15:54, Michael Colonno wrote:
> Updating this thread: I tried to switch over to NFS mounting. When I 
> try to mount the volume with the following line in fstab:
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0

node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0

--
Mr. Flibble
King of the Potato People


From joe at julianfamily.org  Fri Feb  1 01:32:07 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 31 Jan 2013 17:32:07 -0800
Subject: [Gluster-users] NFS availability
In-Reply-To: <20130201005731.85cfe8f5.skraw@ithnet.com>
References: <201301311128028910403@163.com> <1629505.e92XoO53SD@stunted>
	<20130131091826.30755f5c.skraw@ithnet.com>
	<20130131124730.GA90075@nsrc.org>
	<20130131173818.f69ff8cd.skraw@ithnet.com>
	<510AA4E6.9020105@julianfamily.org>
	<20130131195921.fcec73cc.skraw@ithnet.com>
	<510AC34C.9040403@redhat.com>
	<20130131205754.51cfd281.skraw@ithnet.com>
	<510ADB76.2050408@redhat.com>
	<20130201005731.85cfe8f5.skraw@ithnet.com>
Message-ID: <510B1B17.1010409@julianfamily.org>

On 01/31/2013 03:57 PM, Stephan von Krawczynski wrote:
> On Thu, 31 Jan 2013 16:00:38 -0500
> Jeff Darcy <jdarcy at redhat.com> wrote:
>
>>> Most common network errors are not a matter of design, but of dead
>>> iron.
>> It's usually both - a design that is insufficiently tolerant of
>> component failure, plus a combination of component failures that exceeds
>> that tolerance.  You seem to have a very high standard for filesystems
>> continuing to maintain 100% functionality - and I suppose 100%
>> performance as well - if there's any possibility whatsoever that they
>> could do so.  Why don't you apply that same standard to the part of the
>> system that you're responsible for designing?  Running any distributed
>> system on top of a deficient network infrastructure will lead only to
>> disappointment.
> I am sorry that glusterfs is part of the design and your critics.
This sentence is incomprehensible. GlusterFS is part of the design and 
his/our critics? I haven't seen anything from GlusterFS criticizing anyone.
> Everyone working sufficiently long with networks of all kinds of sizes and
> components can tell you that in the end you want a design for a file service
> that works as long as possible. This means it should survive even if there is
> only one client and server and network path left.
Most users, if they're down to one client and one server, they're out of 
business anyway. I know we can't run our whole company on one or two 
computers, and we're not even all that big.
> At least that is what is expected from glusterfs. Unfortunately sometimes you
> get disappointed. We saw just about everything happening when switching off
> all but one reliable network path including network hangs and server hangs
> (the last one) (read the list for examples by others).
After taking a power hit to the building which was able to get through 
our commercial ups and line conditioners (which also had to be replaced 
after that), our switches all had to be replaced. Part of my testing was 
to do just that. I pulled the plugs on all the switches, leaving only 
one data path then changing that path. All the while I had full activity 
on all my volumes, including innodb transactions, vm image activity, 
web, samba, and workstation home directories. I ran multiple dd tests 
from multiple clients during these tests. Not once did they even slow down.
> On the other end of the story clients see servers go offline if you increase
> the non-gluster traffic on the network. Main (but not only) reason is the very
> low default ping time (read the list for examples by others).
42 seconds is low? I've never (and I used to have really crappy dual 
32bit xeon servers) saturated my systems to the point where it took 42 
seconds for a server to respond. If you're switches can't handle that 
kind of traffic, perhaps you're using the wrong hardware.
> All these seen effects show clearly that noone ever tested this to an extent I
> would have done writing this kind of software. After all this is a piece of
> software whose merely only purpose is surviving dead servers and networks.
> It is no question of design, because on paper everything looks promising.
>
> Sometimes your arguments let me believe you want glusterfs working like a ford
> car. A lot of technical gameplay built in but the idea that a car should be a
> good car in the first place got lost on the way somewhere. Quite a lot of the
> features built in lately have the quality of an mp3-player in your ford. Nice
> to have but does not help you a lot driving 200 and a rabbit crossing.
> And this is why I am requesting the equivalent of a BMW.
>
Using your own analogy, what good is a BMW if you've got no roads to 
drive it on?

You're talking in terms of single entities. Most (if not all) of the 
sysadmins I work with on a daily basis, my peers in the industry, 
members of LOPSA... we work in systems. We know how to build in 
redundancy and plan for and survive failures. There's not a week that 
goes by where something in my system hasn't encountered some issue, yet 
our company has not lost any productivity because of it. We have the 
best availability of systems of all our competitors (and I do monitor 
their systems).

I'm not saying you're doing it wrong. Be as asssured as you feel is 
appropriate for your system requirements. Just be aware that the 
majority of the industry does not share those requirements. I'm not 
speaking from my "gut instinct" but I am a member of several 
professional organizations. I attend functions on a weekly basis 
attended by members of organizations like Expedia, Ebay, Amazon, Google, 
Boeing, Starbucks, etc, etc. and we talk about this stuff. Fault 
tolerance and recovery is a big part of what we do, probably the 
biggest, and I still advise the way I do, not through just my own 
experiences, but through the experiences of my peers.

Offer advice and back it up with facts, anecdotes, and/or tests, but 
accept that there are as many ways of managing systems as there are 
systems to be managed. Accept that there are professionals in the world 
that have been doing it longer, have more experience, and (and this is 
not to say anything negative about yourself) are smarter.

From mcolonno at stanford.edu  Fri Feb  1 01:47:41 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 31 Jan 2013 17:47:41 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <054801ce0016$19464840$4bd2d8c0$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>	<510B0C1C.2060905@lanning.cc>
	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
Message-ID: <058301ce001e$1fbf9070$5f3eb150$@stanford.edu>

	I rebuilt the volume with only TCP transport; same issues occurring
so RDMA must not be the issue here. With a glusterfs mount (which is
successful) I can copy files into the mounted volume but any command to list
the contents (ls or ll) hangs forever while ls (for example uses ~ 5% of the
system's CPU and glusterfs uses ~ 50% of the system's CPU). I've never seen
this behavior before with my other Gluster deployments, even much larger /
more complicated ones. This one is pretty ordinary. Any advice appreciated. 

	Thanks,
	~Mike C. 

-----Original Message-----
From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Thursday, January 31, 2013 4:50 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

	Tried the proto=tcp setting, same error unfortunately...

	Thanks,
	~Mike C. 

-----Original Message-----
From: Robert Hajime Lanning [mailto:lanning at lanning.cc]
Sent: Thursday, January 31, 2013 4:28 PM
To: Michael Colonno
Cc: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

On 01/31/13 15:54, Michael Colonno wrote:
> Updating this thread: I tried to switch over to NFS mounting. When I 
> try to mount the volume with the following line in fstab:
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0

node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0

--
Mr. Flibble
King of the Potato People

_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users


From anand.avati at gmail.com  Fri Feb  1 01:55:05 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Thu, 31 Jan 2013 17:55:05 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>
	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>
	<510B0C1C.2060905@lanning.cc>
	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
Message-ID: <CAFboF2y+NMu_SnDU4vfXaAh-0Hv4Ou6T7UBnOmvxr4BB44gJvg@mail.gmail.com>

Looks like you are using a recent kernel with ext4 as the brick filesystem.
It is a known issue with ext4 (
http://joejulian.name/blog/glusterfs-bit-by-ext4-structure-change/). Please
use XFS as the brick filesystem for now.

Avati

On Thu, Jan 31, 2013 at 5:47 PM, Michael Colonno <mcolonno at stanford.edu>wrote:

>         I rebuilt the volume with only TCP transport; same issues occurring
> so RDMA must not be the issue here. With a glusterfs mount (which is
> successful) I can copy files into the mounted volume but any command to
> list
> the contents (ls or ll) hangs forever while ls (for example uses ~ 5% of
> the
> system's CPU and glusterfs uses ~ 50% of the system's CPU). I've never seen
> this behavior before with my other Gluster deployments, even much larger /
> more complicated ones. This one is pretty ordinary. Any advice appreciated.
>
>         Thanks,
>         ~Mike C.
>
> -----Original Message-----
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> Sent: Thursday, January 31, 2013 4:50 PM
> To: gluster-users at gluster.org
> Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)
>
>         Tried the proto=tcp setting, same error unfortunately...
>
>         Thanks,
>         ~Mike C.
>
> -----Original Message-----
> From: Robert Hajime Lanning [mailto:lanning at lanning.cc]
> Sent: Thursday, January 31, 2013 4:28 PM
> To: Michael Colonno
> Cc: gluster-users at gluster.org
> Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)
>
> On 01/31/13 15:54, Michael Colonno wrote:
> > Updating this thread: I tried to switch over to NFS mounting. When I
> > try to mount the volume with the following line in fstab:
> >
> > node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0
>
> --
> Mr. Flibble
> King of the Potato People
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130131/4aaf88c5/attachment.html>

From mcolonno at stanford.edu  Fri Feb  1 01:58:41 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 31 Jan 2013 17:58:41 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>	<510B0C1C.2060905@lanning.cc>	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
Message-ID: <058801ce001f$a95c55b0$fc150110$@stanford.edu>

	Doing more searching, it appears there are several posts on various
forums describing the same issue with Gluster deployments of 4+ nodes (e.g.
http://serverfault.com/questions/453345/certain-operations-like-ls-hang-in-4
-node-gluster-setup). None of these posts are answered which doesn't fill me
with confidence. Can anyone confirm a 4+ node cluster with replication that
does not show this (indefinite hang on ls or ll) behavior? 

	Thanks,
	~Mike C. 

-----Original Message-----
From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Thursday, January 31, 2013 5:48 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

	I rebuilt the volume with only TCP transport; same issues occurring
so RDMA must not be the issue here. With a glusterfs mount (which is
successful) I can copy files into the mounted volume but any command to list
the contents (ls or ll) hangs forever while ls (for example uses ~ 5% of the
system's CPU and glusterfs uses ~ 50% of the system's CPU). I've never seen
this behavior before with my other Gluster deployments, even much larger /
more complicated ones. This one is pretty ordinary. Any advice appreciated. 

	Thanks,
	~Mike C. 

-----Original Message-----
From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Thursday, January 31, 2013 4:50 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

	Tried the proto=tcp setting, same error unfortunately...

	Thanks,
	~Mike C. 

-----Original Message-----
From: Robert Hajime Lanning [mailto:lanning at lanning.cc]
Sent: Thursday, January 31, 2013 4:28 PM
To: Michael Colonno
Cc: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

On 01/31/13 15:54, Michael Colonno wrote:
> Updating this thread: I tried to switch over to NFS mounting. When I 
> try to mount the volume with the following line in fstab:
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0

node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0

--
Mr. Flibble
King of the Potato People

_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users


From mcolonno at stanford.edu  Fri Feb  1 02:02:26 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 31 Jan 2013 18:02:26 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <CAFboF2y+NMu_SnDU4vfXaAh-0Hv4Ou6T7UBnOmvxr4BB44gJvg@mail.gmail.com>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>	<510B0C1C.2060905@lanning.cc>	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
	<CAFboF2y+NMu_SnDU4vfXaAh-0Hv4Ou6T7UBnOmvxr4BB44gJvg@mail.gmail.com>
Message-ID: <058d01ce0020$2f2dfea0$8d89fbe0$@stanford.edu>

            Many thanks - I'll give this a try shortly and report results
for the thread. 

 

            ~Mike C. 

 

From: Anand Avati [mailto:anand.avati at gmail.com] 
Sent: Thursday, January 31, 2013 5:55 PM
To: Michael Colonno
Cc: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

 

Looks like you are using a recent kernel with ext4 as the brick filesystem.
It is a known issue with ext4
(http://joejulian.name/blog/glusterfs-bit-by-ext4-structure-change/). Please
use XFS as the brick filesystem for now.

 

Avati

On Thu, Jan 31, 2013 at 5:47 PM, Michael Colonno <mcolonno at stanford.edu>
wrote:

        I rebuilt the volume with only TCP transport; same issues occurring
so RDMA must not be the issue here. With a glusterfs mount (which is
successful) I can copy files into the mounted volume but any command to list
the contents (ls or ll) hangs forever while ls (for example uses ~ 5% of the
system's CPU and glusterfs uses ~ 50% of the system's CPU). I've never seen
this behavior before with my other Gluster deployments, even much larger /
more complicated ones. This one is pretty ordinary. Any advice appreciated.


        Thanks,
        ~Mike C.

-----Original Message-----

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno

Sent: Thursday, January 31, 2013 4:50 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

        Tried the proto=tcp setting, same error unfortunately...

        Thanks,
        ~Mike C.

-----Original Message-----
From: Robert Hajime Lanning [mailto:lanning at lanning.cc]
Sent: Thursday, January 31, 2013 4:28 PM
To: Michael Colonno
Cc: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

On 01/31/13 15:54, Michael Colonno wrote:
> Updating this thread: I tried to switch over to NFS mounting. When I
> try to mount the volume with the following line in fstab:
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0

node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0

--
Mr. Flibble
King of the Potato People

_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130131/d8537f0a/attachment.html>

From anand.avati at gmail.com  Fri Feb  1 02:12:21 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Thu, 31 Jan 2013 18:12:21 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <058801ce001f$a95c55b0$fc150110$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>
	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>
	<510B0C1C.2060905@lanning.cc>
	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
	<058801ce001f$a95c55b0$fc150110$@stanford.edu>
Message-ID: <CAFboF2wMQDU0-az_+Cs4L7o0rmkjhsVwnp0GK1SsZ3oxUh33AA@mail.gmail.com>

That's probably because in a 2-node replicated environment, there is no
d_off transformation (at the DHT level) and therefore the bug is "avoided".

Avati

On Thu, Jan 31, 2013 at 5:58 PM, Michael Colonno <mcolonno at stanford.edu>wrote:

>         Doing more searching, it appears there are several posts on various
> forums describing the same issue with Gluster deployments of 4+ nodes (e.g.
>
> http://serverfault.com/questions/453345/certain-operations-like-ls-hang-in-4
> -node-gluster-setup). None of these posts are answered which doesn't fill
> me
> with confidence. Can anyone confirm a 4+ node cluster with replication that
> does not show this (indefinite hang on ls or ll) behavior?
>
>         Thanks,
>         ~Mike C.
>
> -----Original Message-----
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> Sent: Thursday, January 31, 2013 5:48 PM
> To: gluster-users at gluster.org
> Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)
>
>         I rebuilt the volume with only TCP transport; same issues occurring
> so RDMA must not be the issue here. With a glusterfs mount (which is
> successful) I can copy files into the mounted volume but any command to
> list
> the contents (ls or ll) hangs forever while ls (for example uses ~ 5% of
> the
> system's CPU and glusterfs uses ~ 50% of the system's CPU). I've never seen
> this behavior before with my other Gluster deployments, even much larger /
> more complicated ones. This one is pretty ordinary. Any advice appreciated.
>
>         Thanks,
>         ~Mike C.
>
> -----Original Message-----
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> Sent: Thursday, January 31, 2013 4:50 PM
> To: gluster-users at gluster.org
> Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)
>
>         Tried the proto=tcp setting, same error unfortunately...
>
>         Thanks,
>         ~Mike C.
>
> -----Original Message-----
> From: Robert Hajime Lanning [mailto:lanning at lanning.cc]
> Sent: Thursday, January 31, 2013 4:28 PM
> To: Michael Colonno
> Cc: gluster-users at gluster.org
> Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)
>
> On 01/31/13 15:54, Michael Colonno wrote:
> > Updating this thread: I tried to switch over to NFS mounting. When I
> > try to mount the volume with the following line in fstab:
> >
> > node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3 0 0
>
> node1:/Volume /tmp/mnt nfs defaults,_netdev,vers=3,proto=tcp 0 0
>
> --
> Mr. Flibble
> King of the Potato People
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130131/97788b1f/attachment-0001.html>

From lanning at lanning.cc  Fri Feb  1 03:01:53 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Thu, 31 Jan 2013 19:01:53 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <058801ce001f$a95c55b0$fc150110$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>	<510B0C1C.2060905@lanning.cc>	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
	<058801ce001f$a95c55b0$fc150110$@stanford.edu>
Message-ID: <510B3021.8090904@lanning.cc>

On 01/31/13 17:58, Michael Colonno wrote:
> 	Doing more searching, it appears there are several posts on various
> forums describing the same issue with Gluster deployments of 4+ nodes (e.g.
> http://serverfault.com/questions/453345/certain-operations-like-ls-hang-in-4
> -node-gluster-setup). None of these posts are answered which doesn't fill me
> with confidence. Can anyone confirm a 4+ node cluster with replication that
> does not show this (indefinite hang on ls or ll) behavior?
>

Volume Name: vol01
Type: Distributed-Replicate
Volume ID: 3d5d1791-044e-4cf3-b17b-79c74578e590
Status: Started
Number of Bricks: 24 x 2 = 48
Transport-type: tcp

4 bricks per server * 12 servers
All bricks are 37TB logical volumes formated with XFS.

Filesystem            Size  Used Avail Use% Mounted on
gfs-vol:/vol01        888T  119G  888T   1% /glusterfs/vol01

No issues.

-- 
Mr. Flibble
King of the Potato People

From lanning at lanning.cc  Fri Feb  1 03:12:23 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Thu, 31 Jan 2013 19:12:23 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <510B3021.8090904@lanning.cc>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>	<510B0C1C.2060905@lanning.cc>	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
	<058801ce001f$a95c55b0$fc150110$@stanford.edu>
	<510B3021.8090904@lanning.cc>
Message-ID: <510B3297.4040600@lanning.cc>

On 01/31/13 19:01, Robert Hajime Lanning wrote:
> On 01/31/13 17:58, Michael Colonno wrote:
>> Doing more searching, it appears there are several posts on various
>> forums describing the same issue with Gluster deployments of 4+ nodes
>> (e.g.
>> http://serverfault.com/questions/453345/certain-operations-like-ls-hang-in-4
>>
>> -node-gluster-setup). None of these posts are answered which doesn't
>> fill me
>> with confidence. Can anyone confirm a 4+ node cluster with replication
>> that
>> does not show this (indefinite hang on ls or ll) behavior?
>>
>
> Volume Name: vol01
> Type: Distributed-Replicate
> Volume ID: 3d5d1791-044e-4cf3-b17b-79c74578e590
> Status: Started
> Number of Bricks: 24 x 2 = 48
> Transport-type: tcp
>
> 4 bricks per server * 12 servers
> All bricks are 37TB logical volumes formated with XFS.
>
> Filesystem Size Used Avail Use% Mounted on
> gfs-vol:/vol01 888T 119G 888T 1% /glusterfs/vol01
>
> No issues.
>

Sorry, should have included:
CentOS 6.3
GlusterFS 3.3.1

-- 
Mr. Flibble
King of the Potato People

From mcolonno at stanford.edu  Fri Feb  1 06:38:09 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 31 Jan 2013 22:38:09 -0800
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <510B3297.4040600@lanning.cc>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>	<510B0C1C.2060905@lanning.cc>	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>	<058801ce001f$a95c55b0$fc150110$@stanford.edu>	<510B3021.8090904@lanning.cc>
	<510B3297.4040600@lanning.cc>
Message-ID: <05d101ce0046$b378c430$1a6a4c90$@stanford.edu>

	To close out this thread I rebuilt the entire volume with ZFS
instead of ext4 and it now appears to work well. I have not loaded it up yet
but the ls / ll hanging problem is gone and I can move data to and from the
mounted filesystem via a glusterfs mount. 

	Thanks,
	~Mike C. 

-----Original Message-----
From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Robert Hajime
Lanning
Sent: Thursday, January 31, 2013 7:12 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)

On 01/31/13 19:01, Robert Hajime Lanning wrote:
> On 01/31/13 17:58, Michael Colonno wrote:
>> Doing more searching, it appears there are several posts on various 
>> forums describing the same issue with Gluster deployments of 4+ nodes 
>> (e.g.
>> http://serverfault.com/questions/453345/certain-operations-like-ls-ha
>> ng-in-4
>>
>> -node-gluster-setup). None of these posts are answered which doesn't 
>> fill me with confidence. Can anyone confirm a 4+ node cluster with 
>> replication that does not show this (indefinite hang on ls or ll) 
>> behavior?
>>
>
> Volume Name: vol01
> Type: Distributed-Replicate
> Volume ID: 3d5d1791-044e-4cf3-b17b-79c74578e590
> Status: Started
> Number of Bricks: 24 x 2 = 48
> Transport-type: tcp
>
> 4 bricks per server * 12 servers
> All bricks are 37TB logical volumes formated with XFS.
>
> Filesystem Size Used Avail Use% Mounted on
> gfs-vol:/vol01 888T 119G 888T 1% /glusterfs/vol01
>
> No issues.
>

Sorry, should have included:
CentOS 6.3
GlusterFS 3.3.1

--
Mr. Flibble
King of the Potato People
_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users


From johnmark at johnmark.org  Fri Feb  1 08:16:16 2013
From: johnmark at johnmark.org (John Mark Walker)
Date: Fri, 1 Feb 2013 09:16:16 +0100
Subject: [Gluster-users] hanging of mounted filesystem (3.3.1)
In-Reply-To: <05d101ce0046$b378c430$1a6a4c90$@stanford.edu>
References: <039f01ce0004$14f23590$3ed6a0b0$@stanford.edu>
	<04ab01ce000e$4bfb36e0$e3f1a4a0$@stanford.edu>
	<510B0C1C.2060905@lanning.cc>
	<054801ce0016$19464840$4bd2d8c0$@stanford.edu>
	<058301ce001e$1fbf9070$5f3eb150$@stanford.edu>
	<058801ce001f$a95c55b0$fc150110$@stanford.edu>
	<510B3021.8090904@lanning.cc> <510B3297.4040600@lanning.cc>
	<05d101ce0046$b378c430$1a6a4c90$@stanford.edu>
Message-ID: <CAMUFnfZWRfBFu=5xUE-mQ4rGtzXu0L8kKfzK=sqNAjvB+Svjpg@mail.gmail.com>

I suspect you hit the Ext4 bug.

-JM



On Fri, Feb 1, 2013 at 7:38 AM, Michael Colonno <mcolonno at stanford.edu>wrote:

>         To close out this thread I rebuilt the entire volume with ZFS
> instead of ext4 and it now appears to work well. I have not loaded it up
> yet
> but the ls / ll hanging problem is gone and I can move data to and from the
> mounted filesystem via a glusterfs mount.
>
>         Thanks,
>         ~Mike C.
>
> -----Original Message-----
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Robert Hajime
> Lanning
> Sent: Thursday, January 31, 2013 7:12 PM
> To: gluster-users at gluster.org
> Subject: Re: [Gluster-users] hanging of mounted filesystem (3.3.1)
>
> On 01/31/13 19:01, Robert Hajime Lanning wrote:
> > On 01/31/13 17:58, Michael Colonno wrote:
> >> Doing more searching, it appears there are several posts on various
> >> forums describing the same issue with Gluster deployments of 4+ nodes
> >> (e.g.
> >> http://serverfault.com/questions/453345/certain-operations-like-ls-ha
> >> ng-in-4
> >>
> >> -node-gluster-setup). None of these posts are answered which doesn't
> >> fill me with confidence. Can anyone confirm a 4+ node cluster with
> >> replication that does not show this (indefinite hang on ls or ll)
> >> behavior?
> >>
> >
> > Volume Name: vol01
> > Type: Distributed-Replicate
> > Volume ID: 3d5d1791-044e-4cf3-b17b-79c74578e590
> > Status: Started
> > Number of Bricks: 24 x 2 = 48
> > Transport-type: tcp
> >
> > 4 bricks per server * 12 servers
> > All bricks are 37TB logical volumes formated with XFS.
> >
> > Filesystem Size Used Avail Use% Mounted on
> > gfs-vol:/vol01 888T 119G 888T 1% /glusterfs/vol01
> >
> > No issues.
> >
>
> Sorry, should have included:
> CentOS 6.3
> GlusterFS 3.3.1
>
> --
> Mr. Flibble
> King of the Potato People
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130201/90c8cf1d/attachment.html>

From samu60 at gmail.com  Fri Feb  1 09:22:17 2013
From: samu60 at gmail.com (samuel)
Date: Fri, 1 Feb 2013 10:22:17 +0100
Subject: [Gluster-users] Striped Replicated Volumes: create files error.
In-Reply-To: <F26C21A0DAA59849B52402F69712798D03619A63@MANA1.netrtlsrv.com>
References: <F26C21A0DAA59849B52402F69712798D03619A63@MANA1.netrtlsrv.com>
Message-ID: <CAOg=WDdPB+vGLzvpTDKJwUPtJXVHvt0D1G21jD_YH3RaKkAqGA@mail.gmail.com>

It's a problem of stripped volumes in 3.3.1.
It does not appear on 3.3.0 and it's solved in coming 3.4.

Best regards,
Samuel.

On 25 January 2013 14:41, <axel.weber at cbc.de> wrote:

>  Hi there,
> each time I copy (or dd or similar) a file to a striped replicated volume
> I get an error: the argument is not valid.
> An empty file is created.
> If I now run the copy, it works.
> This is in independed of the client platform.
> We are using version 3.3.1****
>
> ** **
>
> ** **
>
> Mit freundlichen Gr??en / Kind regards****
>
> ** **
>
> ** **
>
> Axel Weber****
>
> ** **
>
> ** **
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130201/32607cf7/attachment.html>

From yongtaofu at gmail.com  Fri Feb  1 18:15:45 2013
From: yongtaofu at gmail.com (yongtaofu)
Date: Sat, 2 Feb 2013 02:15:45 +0800
Subject: [Gluster-users] please please please I beg redhat fix file system
	crush issue for fluster
Message-ID: <C182E13D-BA30-433B-B8B7-7BBC4D44D42F@gmail.com>

It's mid night in china about 2:00 and I can't sleep. I have a glusterfs 3.3 cluster in production and there're several 
hundreds of clients on it. The io load is extremely high 24 hours. So the file system crush happens frequently in the cluster. About several times a year(I use xfs
)
 When one of the brick file system crush the whole volume collapse. Please don't say there're only few files can't be accessed. I swear to Beijing air the truth is the whole volume collapse and more files can't be accessed if don't handle it. Almost no file can be accessed. Some directory can't be accessed too which make millions of files missing and almost no fops can be performed. If you don't believe in me I have to swear to Beijing air again that I have see it happens several times. It's too expensive for us to restore this kind of failure! I admire redhat works and I hate myself that I don't have the ability to fix it. But I hope one day I can make contribute too.
I have raise this issue before and I see there're updates. But I beg you please raise it's priority! Thank you very much!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130202/e55fb737/attachment.html>

From johnmark at redhat.com  Fri Feb  1 18:20:38 2013
From: johnmark at redhat.com (John Mark Walker)
Date: Fri, 1 Feb 2013 13:20:38 -0500 (EST)
Subject: [Gluster-users] please please please I beg redhat fix file
 system	crush issue for fluster
Message-ID: <hr2poxuf5x43wqced4807f9p.1359742830991@email.android.com>

Hi, this list is for Gluster.org, which is a self-support community and only has volunteers. If you need immediate help from red hat, I recommend you call them.

Thanks,
JM


yongtaofu <yongtaofu at gmail.com> wrote:

It's mid night in china about 2:00 and I can't sleep. I have a glusterfs 3.3 cluster in production and there're several 
hundreds of clients on it. The io load is extremely high 24 hours. So the file system crush happens frequently in the cluster. About several times a year(I use xfs
)
 When one of the brick file system crush the whole volume collapse. Please don't say there're only few files can't be accessed. I swear to Beijing air the truth is the whole volume collapse and more files can't be accessed if don't handle it. Almost no file can be accessed. Some directory can't be accessed too which make millions of files missing and almost no fops can be performed. If you don't believe in me I have to swear to Beijing air again that I have see it happens several times. It's too expensive for us to restore this kind of failure! I admire redhat works and I hate myself that I don't have the ability to fix it. But I hope one day I can make contribute too.
I have raise this issue before and I see there're updates. But I beg you please raise it's priority! Thank you very much!


From mcolonno at stanford.edu  Fri Feb  1 20:46:13 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Fri, 1 Feb 2013 12:46:13 -0800
Subject: [Gluster-users] high CPU load on all bricks
Message-ID: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>

            Gluster gurus ~

 

            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
the volume. On each system contributing a brick, however, my CPU usage
(glusterfsd) is hovering around 20% (virtually zero memory usage
thankfully). These are brand new, fairly beefy servers so 20% CPU load is
quite a bit. The deployment is pretty plain with each brick mounting the
volume to itself via a glusterfs mount. I assume this type of CPU usage is
atypically high; is there anything I can do to investigate what's soaking up
CPU and minimize it? Total usable volume size is only about 22 TB (about 45
TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130201/404dacf2/attachment.html>

From mcolonno at stanford.edu  Fri Feb  1 20:53:24 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Fri, 1 Feb 2013 12:53:24 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
Message-ID: <019901ce00be$2d787ba0$886972e0$@stanford.edu>

            Forgot to mention: on a client system (not a brick) the
glusterfs process is consuming ~ 68% CPU continuously. This is a much less
powerful desktop system so the CPU load can't be compared 1:1 with the
systems comprising the bricks but still very high. So the issue seems to
exist with both glusterfsd and glusterfs processes. 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 12:46 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] high CPU load on all bricks

 

            Gluster gurus ~

 

            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
the volume. On each system contributing a brick, however, my CPU usage
(glusterfsd) is hovering around 20% (virtually zero memory usage
thankfully). These are brand new, fairly beefy servers so 20% CPU load is
quite a bit. The deployment is pretty plain with each brick mounting the
volume to itself via a glusterfs mount. I assume this type of CPU usage is
atypically high; is there anything I can do to investigate what's soaking up
CPU and minimize it? Total usable volume size is only about 22 TB (about 45
TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130201/a8848ca0/attachment.html>

From gluster at elyograg.org  Fri Feb  1 21:03:13 2013
From: gluster at elyograg.org (Shawn Heisey)
Date: Fri, 01 Feb 2013 14:03:13 -0700
Subject: [Gluster-users] New version of UFO - is there a new HOWTO?
In-Reply-To: <510AEEC4.9060501@elyograg.org>
References: <801026313.17486486.1359660978600.JavaMail.root@redhat.com>
	<510AEEC4.9060501@elyograg.org>
Message-ID: <510C2D91.1020703@elyograg.org>

On 1/31/2013 3:23 PM, Shawn Heisey wrote:> On 1/31/2013 12:36 PM, Kaleb 
Keithley wrote:
 >> Not sure if you saw this in #gluster on IRC.
 >>
 >> The other work-around for F18 is to delete
 >> /etc/swift/{account,container,object}-server.conf before starting UFO.
 >>
 >> With that my UFO set-up works as it did in F17.
 >
 > Still no joy.
 >
 > http://fpaste.org/SOYM/
 >
 > Same thing happens if I remove all the packages, get rid of /etc/swift,
 > and reinstall.  I can confirm that the file referenced in the last part
 > of the paste (/etc/swift/object.ring.gz) did not exist at any point.  A
 > previous installation of 3.3.1-2 on F17 has both .builder and .ring.gz
 > files.
 >
 > http://fpaste.org/jIm5/

I have also now tried it on the latest CentOS with the same result.  I 
must be doing something wrong, which is why I had hoped for a new HOWTO.

Thanks,
Shawn


From whit.gluster at transpect.com  Fri Feb  1 21:10:34 2013
From: whit.gluster at transpect.com (Whit Blauvelt)
Date: Fri, 1 Feb 2013 16:10:34 -0500
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <019901ce00be$2d787ba0$886972e0$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<019901ce00be$2d787ba0$886972e0$@stanford.edu>
Message-ID: <20130201211034.GA7300@black.transpect.com>

On Fri, Feb 01, 2013 at 12:53:24PM -0800, Michael Colonno wrote:
>             Forgot to mention: on a client system (not a brick) the glusterfs
> process is consuming ~ 68% CPU continuously. This is a much less powerful
> desktop system so the CPU load can?t be compared 1:1 with the systems
> comprising the bricks but still very high. So the issue seems to exist with
> both glusterfsd and glusterfs processes.

You said the host systems were also running the gluster client to have the
gluster mount locally available. Does the high load go away on them if you
unmount that? Some versions back (3.1.5), on some older systems (Ubuntu 8.04
with Gluster built from source), I had problems with the gluster client
running away on me. Those systems are happy using nfs mounts instead. And
the gluster servers, which aren't doing local mounting of gluster at all,
don't use much by way of CPU. Then again, they're not highly stressed by
use, so it's not much of a test.

That's a long winded way of asking, are you sure it's glusterfsd too, and
not just glusterfs sucking up the CPU cycles?

Whit

From lanning at lanning.cc  Fri Feb  1 21:48:26 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Fri, 01 Feb 2013 13:48:26 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
Message-ID: <510C382A.3060109@lanning.cc>

On 02/01/13 12:46, Michael Colonno wrote:
> Gluster gurus ~
>
> I?ve deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on CentOS
> 6.3 with tcp transport. I was able to build, start, mount, and use the
> volume. On each system contributing a brick, however, my CPU usage
> (glusterfsd) is hovering around 20% (virtually zero memory usage
> thankfully). These are brand new, fairly beefy servers so 20% CPU load
> is quite a bit. The deployment is pretty plain with each brick mounting
> the volume to itself via a glusterfs mount. I assume this type of CPU
> usage is atypically high; is there anything I can do to investigate
> what?s soaking up CPU and minimize it? Total usable volume size is only
> about 22 TB (about 45 TB total with 2x replicate).

Where are you reading the 20% from?

If it is from the per process line in top, then that 20% is of a single 
core/hyperthread.  It is not of the whole system.

If you have an otherwise quiescent server and the "Cpu(s)" summary line 
at the top says 20%us, then you can extrapolate to under 20% of the 
total system.

-- 
Mr. Flibble
King of the Potato People

From joe at julianfamily.org  Fri Feb  1 22:07:30 2013
From: joe at julianfamily.org (Joe Julian)
Date: Fri, 01 Feb 2013 14:07:30 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <019901ce00be$2d787ba0$886972e0$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<019901ce00be$2d787ba0$886972e0$@stanford.edu>
Message-ID: <33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>

Check the client log(s). 

Michael Colonno <mcolonno at stanford.edu> wrote:

>            Forgot to mention: on a client system (not a brick) the
>glusterfs process is consuming ~ 68% CPU continuously. This is a much
>less
>powerful desktop system so the CPU load can't be compared 1:1 with the
>systems comprising the bricks but still very high. So the issue seems
>to
>exist with both glusterfsd and glusterfs processes. 
>
> 
>
>            Thanks,
>
>            ~Mike C. 
>
> 
>
>From: gluster-users-bounces at gluster.org
>[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>Sent: Friday, February 01, 2013 12:46 PM
>To: gluster-users at gluster.org
>Subject: [Gluster-users] high CPU load on all bricks
>
> 
>
>            Gluster gurus ~
>
> 
>
>       I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
>CentOS 6.3 with tcp transport. I was able to build, start, mount, and
>use
>the volume. On each system contributing a brick, however, my CPU usage
>(glusterfsd) is hovering around 20% (virtually zero memory usage
>thankfully). These are brand new, fairly beefy servers so 20% CPU load
>is
>quite a bit. The deployment is pretty plain with each brick mounting
>the
>volume to itself via a glusterfs mount. I assume this type of CPU usage
>is
>atypically high; is there anything I can do to investigate what's
>soaking up
>CPU and minimize it? Total usable volume size is only about 22 TB
>(about 45
>TB total with 2x replicate). 
>
> 
>
>            Thanks,
>
>            ~Mike C. 
>
> 
>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>Gluster-users mailing list
>Gluster-users at gluster.org
>http://supercolony.gluster.org/mailman/listinfo/gluster-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130201/092986de/attachment.html>

From mcolonno at stanford.edu  Sat Feb  2 00:46:02 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Fri, 1 Feb 2013 16:46:02 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<019901ce00be$2d787ba0$886972e0$@stanford.edu>
	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>
Message-ID: <00d301ce00de$ad920250$08b606f0$@stanford.edu>

            Update: after a few hours the CPU usage seems to have dropped down to a small value. I did not change anything with respect to the configuration or unmount / stop anything as I wanted to see if this would persist for a long period of time. Both the client and the self-mounted bricks are now showing CPU < 1% (as reported by top). Prior to the larger CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is this kind a transient behavior ? by which I mean larger CPU loads after a lot of filesystem activity in short time ? typical? This is not a problem in my deployment; I just want to know what to expect in the future and to complete this thread for future users. If this is expected behavior we can wrap up this thread. If not then I?ll do more digging into the logs on the client and brick sides. 

 

            Thanks,

            ~Mike C. 

 

From: Joe Julian [mailto:joe at julianfamily.org] 
Sent: Friday, February 01, 2013 2:08 PM
To: Michael Colonno; gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

Check the client log(s). 

Michael Colonno <mcolonno at stanford.edu> wrote:

            Forgot to mention: on a client system (not a brick) the glusterfs process is consuming ~ 68% CPU continuously. This is a much less powerful desktop system so the CPU load can?t be compared 1:1 with the systems comprising the bricks but still very high. So the issue seems to exist with both glusterfsd and glusterfs processes. 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 12:46 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] high CPU load on all bricks

 

            Gluster gurus ~

 

            I?ve deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on CentOS 6.3 with tcp transport. I was able to build, start, mount, and use the volume. On each system contributing a brick, however, my CPU usage (glusterfsd) is hovering around 20% (virtually zero memory usage thankfully). These are brand new, fairly beefy servers so 20% CPU load is quite a bit. The deployment is pretty plain with each brick mounting the volume to itself via a glusterfs mount. I assume this type of CPU usage is atypically high; is there anything I can do to investigate what?s soaking up CPU and minimize it? Total usable volume size is only about 22 TB (about 45 TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 


  _____  


Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130201/0f86682b/attachment.html>

From mcolonno at stanford.edu  Sun Feb  3 19:22:38 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Sun, 3 Feb 2013 11:22:38 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <00d301ce00de$ad920250$08b606f0$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>	<019901ce00be$2d787ba0$886972e0$@stanford.edu>	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>
	<00d301ce00de$ad920250$08b606f0$@stanford.edu>
Message-ID: <015101ce0243$d464c000$7d2e4000$@stanford.edu>

            Having taken a lot more data it does seem the glusterfsd and glusterd processes (along with several ksoftirqd) spike up to near 100% on both client and brick servers during any file transport across the mount. Thankfully this is short-lived for the most part but I?m wondering if this is expected behavior or what others have experienced(?) I?m a little surprised such a large CPU load would be required to move small files and / or use an application within a Gluster mount point. 

 

            I wanted to test this against an NFS mount of the same Gluster volume. I managed to get rstatd installed and running but my attempts to mount the volume via NFS are met with: 

 

            mount.nfs: requested NFS version or transport protocol is not supported

 

            Relevant line in /etc/fstab:

 

            node1:/volume    /volume    nfs     defaults,_netdev,vers=3,mountproto=tcp        0 0      

 

It looks like CentOS 6.x has NFS version 4 built into everything. So a few questions:

 

-       Has anyone else noted significant performance differences between a glusterfs mount and NFS mount for volumes of 8+ bricks? 

-       Is there a straightforward way to make the newer versions of CentOS play nice with NFS version 3 + Gluster? 

-       Are there any general performance tuning guidelines I can follow to improve CPU performance? I found a few references to the cache settings but nothing solid. 

 

If the consensus is that NFS will not gain anything then I won?t waste the time setting it all up. 

 

Thanks,

~Mike C. 

 

 

From: gluster-users-bounces at gluster.org [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 4:46 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

            Update: after a few hours the CPU usage seems to have dropped down to a small value. I did not change anything with respect to the configuration or unmount / stop anything as I wanted to see if this would persist for a long period of time. Both the client and the self-mounted bricks are now showing CPU < 1% (as reported by top). Prior to the larger CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is this kind a transient behavior ? by which I mean larger CPU loads after a lot of filesystem activity in short time ? typical? This is not a problem in my deployment; I just want to know what to expect in the future and to complete this thread for future users. If this is expected behavior we can wrap up this thread. If not then I?ll do more digging into the logs on the client and brick sides. 

 

            Thanks,

            ~Mike C. 

 

From: Joe Julian [mailto:joe at julianfamily.org] 
Sent: Friday, February 01, 2013 2:08 PM
To: Michael Colonno; gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

Check the client log(s). 

Michael Colonno <mcolonno at stanford.edu> wrote:

            Forgot to mention: on a client system (not a brick) the glusterfs process is consuming ~ 68% CPU continuously. This is a much less powerful desktop system so the CPU load can?t be compared 1:1 with the systems comprising the bricks but still very high. So the issue seems to exist with both glusterfsd and glusterfs processes. 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 12:46 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] high CPU load on all bricks

 

            Gluster gurus ~

 

            I?ve deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on CentOS 6.3 with tcp transport. I was able to build, start, mount, and use the volume. On each system contributing a brick, however, my CPU usage (glusterfsd) is hovering around 20% (virtually zero memory usage thankfully). These are brand new, fairly beefy servers so 20% CPU load is quite a bit. The deployment is pretty plain with each brick mounting the volume to itself via a glusterfs mount. I assume this type of CPU usage is atypically high; is there anything I can do to investigate what?s soaking up CPU and minimize it? Total usable volume size is only about 22 TB (about 45 TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 


  _____  


Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130203/fd6f6622/attachment.html>

From joe at julianfamily.org  Sun Feb  3 19:46:32 2013
From: joe at julianfamily.org (Joe Julian)
Date: Sun, 03 Feb 2013 11:46:32 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <015101ce0243$d464c000$7d2e4000$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>	<019901ce00be$2d787ba0$886972e0$@stanford.edu>	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>
	<00d301ce00de$ad920250$08b606f0$@stanford.edu>
	<015101ce0243$d464c000$7d2e4000$@stanford.edu>
Message-ID: <510EBE98.9000807@julianfamily.org>

On 02/03/2013 11:22 AM, Michael Colonno wrote:
>
> Having taken a lot more data it does seem the glusterfsd and glusterd 
> processes (along with several ksoftirqd) spike up to near 100% on both 
> client and brick servers during any file transport across the mount. 
> Thankfully this is short-lived for the most part but I'm wondering if 
> this is expected behavior or what others have experienced(?) I'm a 
> little surprised such a large CPU load would be required to move small 
> files and / or use an application within a Gluster mount point.
>

If you're getting ksoftirqd spikes, that sounds like a hardware issue to 
me. I never see huge spikes like that on my servers nor clients.

> I wanted to test this against an NFS mount of the same Gluster volume. 
> I managed to get rstatd installed and running but my attempts to mount 
> the volume via NFS are met with:
>
> mount.nfs: requested NFS version or transport protocol is not supported
>
> Relevant line in /etc/fstab:
>
> node1:/volume    /volume    nfs 
> defaults,_netdev,vers=3,mountproto=tcp        0 0
>
> It looks like CentOS 6.x has NFS version 4 built into everything. So a 
> few questions:
>
> -Has anyone else noted significant performance differences between a 
> glusterfs mount and NFS mount for volumes of 8+ bricks?
>
> -Is there a straightforward way to make the newer versions of CentOS 
> play nice with NFS version 3 + Gluster?
>
> -Are there any general performance tuning guidelines I can follow to 
> improve CPU performance? I found a few references to the cache 
> settings but nothing solid.
>
> If the consensus is that NFS will not gain anything then I won't waste 
> the time setting it all up.
>

NFS gains you the use of FSCache to cache directories and file stats 
making directory listings faster, but it adds overhead decreasing the 
overall throughput (from all the reports I've seen).

I would suspect that you have the kernel nfs server running on your 
servers. Make sure it's disabled.

> Thanks,
>
> ~Mike C.
>
> *From:*gluster-users-bounces at gluster.org 
> [mailto:gluster-users-bounces at gluster.org] *On Behalf Of *Michael Colonno
> *Sent:* Friday, February 01, 2013 4:46 PM
> *To:* gluster-users at gluster.org
> *Subject:* Re: [Gluster-users] high CPU load on all bricks
>
> Update: after a few hours the CPU usage seems to have dropped down to 
> a small value. I did not change anything with respect to the 
> configuration or unmount / stop anything as I wanted to see if this 
> would persist for a long period of time. Both the client and the 
> self-mounted bricks are now showing CPU < 1% (as reported by top). 
> Prior to the larger CPU loads I installed a bunch of software into the 
> volume (~ 5 GB total). Is this kind a transient behavior -- by which I 
> mean larger CPU loads after a lot of filesystem activity in short time 
> -- typical? This is not a problem in my deployment; I just want to 
> know what to expect in the future and to complete this thread for 
> future users. If this is expected behavior we can wrap up this thread. 
> If not then I'll do more digging into the logs on the client and brick 
> sides.
>
> Thanks,
>
> ~Mike C.
>
> *From:*Joe Julian [mailto:joe at julianfamily.org]
> *Sent:* Friday, February 01, 2013 2:08 PM
> *To:* Michael Colonno; gluster-users at gluster.org 
> <mailto:gluster-users at gluster.org>
> *Subject:* Re: [Gluster-users] high CPU load on all bricks
>
> Check the client log(s).
>
> Michael Colonno <mcolonno at stanford.edu <mailto:mcolonno at stanford.edu>> 
> wrote:
>
>             Forgot to mention: on a client system (not a brick) the 
> glusterfs process is consuming ~ 68% CPU continuously. This is a much 
> less powerful desktop system so the CPU load can't be compared 1:1 
> with the systems comprising the bricks but still very high. So the 
> issue seems to exist with both glusterfsd and glusterfs processes.
>
>             Thanks,
>
>             ~Mike C.
>
> *From:* gluster-users-bounces at gluster.org 
> <mailto:gluster-users-bounces at gluster.org> 
> [mailto:gluster-users-bounces at gluster.org] *On Behalf Of *Michael Colonno
> *Sent:* Friday, February 01, 2013 12:46 PM
> *To:* gluster-users at gluster.org <mailto:gluster-users at gluster.org>
> *Subject:* [Gluster-users] high CPU load on all bricks
>
>             Gluster gurus ~
>
>             I've deployed and 8-brick (2x replicate) Gluster 3.3.1 
> volume on CentOS 6.3 with tcp transport. I was able to build, start, 
> mount, and use the volume. On each system contributing a brick, 
> however, my CPU usage (glusterfsd) is hovering around 20% (virtually 
> zero memory usage thankfully). These are brand new, fairly beefy 
> servers so 20% CPU load is quite a bit. The deployment is pretty plain 
> with each brick mounting the volume to itself via a glusterfs mount. I 
> assume this type of CPU usage is atypically high; is there anything I 
> can do to investigate what's soaking up CPU and minimize it? Total 
> usable volume size is only about 22 TB (about 45 TB total with 2x 
> replicate).
>
>             Thanks,
>
>             ~Mike C.
>
> ------------------------------------------------------------------------
>
> Gluster-users mailing list
> Gluster-users at gluster.org  <mailto:Gluster-users at gluster.org>
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130203/91bbe858/attachment-0001.html>

From andrei at arhont.com  Mon Feb  4 14:34:57 2013
From: andrei at arhont.com (Andrei Mikhailovsky)
Date: Mon, 4 Feb 2013 14:34:57 +0000 (GMT)
Subject: [Gluster-users] adding a brick causes mountpoints to hang
In-Reply-To: <9511116.752.1359987993855.JavaMail.andrei@finka>
Message-ID: <32734416.777.1359988495177.JavaMail.andrei@finka>

Hello guys, 

I am having a bit of an issue with adding a new server to the glusterfs. I was wondering if anyone could help me. I am running glusterfs 3.3.0 over rdma on ubuntu 12.04 server. 

Basically, I had a single glusterfs server that was used by two host servers for running a bunch of vms. The host servers used nfs to mount the glusterfs mount point. I've added a new server with the hope to replicate data. The server was added fine and the clients could mount the mountpoint which had both servers. 

Following that I've ran "ls -laR /mountpoint" on the client side to kick off the self heal process so that all the data is copied across to the second server. Following a few seconds delay I started seeing file server contents from the ls command. This lasted about 10 seconds after which point the mountpoint froze on both clients. However, I can see data being copied to the new storage server while the mountpoints were still hanging. I've waited about two hours, but the mountpoints were still frozen. 

Does anyone know what the problem could be? 

After about three hours I've had to stop the glusterfs service on the newly introduced server as I had to turn on the infrastructure. After the service stopped, the mountpoints came back and started working again. 

I was wondering if it's possible to add new bricks without the client mountpoint hangs? How can I find out what the problem is? 

Many thanks 

Andrei 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/4cd5661d/attachment.html>

From guillermo.marco at sistemasgenomicos.com  Mon Feb  4 16:24:11 2013
From: guillermo.marco at sistemasgenomicos.com (Guillermo Marco)
Date: Mon, 04 Feb 2013 17:24:11 +0100
Subject: [Gluster-users] Editing file with VI
Message-ID: <510FE0AB.6010607@sistemasgenomicos.com>

Hello,

Sometimes when i'm editing a file with located in Gluster with VI i get 
this message:

W12: Warning: File "test.sh" has changed and the buffer was changed
in Vim as well
See ":help W12" for more info.
[O]K, (L)oad File:

I've no idea what can be causing that and i'm scared.

Thank you.

Best regards,
Guillermo.


From purpleidea at gmail.com  Mon Feb  4 16:33:19 2013
From: purpleidea at gmail.com (James)
Date: Mon, 04 Feb 2013 11:33:19 -0500
Subject: [Gluster-users] Editing file with VI
In-Reply-To: <510FE0AB.6010607@sistemasgenomicos.com>
References: <510FE0AB.6010607@sistemasgenomicos.com>
Message-ID: <1359995599.6085.20.camel@freed.purpleidea.com>

On Mon, 2013-02-04 at 17:24 +0100, Guillermo Marco wrote:
> Hello,
> 
> Sometimes when i'm editing a file with located in Gluster with VI i get 
> this message:
> 
> W12: Warning: File "test.sh" has changed and the buffer was changed
> in Vim as well
> See ":help W12" for more info.
> [O]K, (L)oad File:
Sounds like the file is being edited by someone else while it's open by
you.

> 
> I've no idea what can be causing that and i'm scared.
> 
> Thank you.
> 
> Best regards,
> Guillermo.
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: This is a digitally signed message part
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/b932134b/attachment.sig>

From guillermo.marco at sistemasgenomicos.com  Mon Feb  4 16:54:12 2013
From: guillermo.marco at sistemasgenomicos.com (Guillermo Marco)
Date: Mon, 04 Feb 2013 17:54:12 +0100
Subject: [Gluster-users] Fwd: Re:  Editing file with VI
In-Reply-To: <1359996271.6085.22.camel@freed.purpleidea.com>
References: <1359996271.6085.22.camel@freed.purpleidea.com>
Message-ID: <510FE7B4.8050401@sistemasgenomicos.com>


You have to cc the list or nobody else will see this.

On Mon, 2013-02-04 at 17:42 +0100, Guillermo Marco wrote:
> Hello,
>
> I'm the only user on the system at this moment so it's not possible.
> Does Gluster modifies any part of the file indexing or something?
> On 02/04/2013 05:33 PM, James wrote:
> > On Mon, 2013-02-04 at 17:24 +0100, Guillermo Marco wrote:
> >> Hello,
> >>
> >> Sometimes when i'm editing a file with located in Gluster with VI i get
> >> this message:
> >>
> >> W12: Warning: File "test.sh" has changed and the buffer was changed
> >> in Vim as well
> >> See ":help W12" for more info.
> >> [O]K, (L)oad File:
> > Sounds like the file is being edited by someone else while it's open by
> > you.
> >
> >> I've no idea what can be causing that and i'm scared.
> >>
> >> Thank you.
> >>
> >> Best regards,
> >> Guillermo.
> >>
> >> _______________________________________________
> >> Gluster-users mailing list
> >> Gluster-users at gluster.org
> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>




-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/65f795f1/attachment.sig>

From joe at julianfamily.org  Mon Feb  4 16:57:55 2013
From: joe at julianfamily.org (Joe Julian)
Date: Mon, 04 Feb 2013 08:57:55 -0800
Subject: [Gluster-users] Fwd: Re:  Editing file with VI
In-Reply-To: <510FE7B4.8050401@sistemasgenomicos.com>
References: <1359996271.6085.22.camel@freed.purpleidea.com>
	<510FE7B4.8050401@sistemasgenomicos.com>
Message-ID: <525cbc7e-43f2-4254-9bf9-0fb0610206d2@email.android.com>

Are you editing files directly on the brick? 


Guillermo Marco <guillermo.marco at sistemasgenomicos.com> wrote:

>
>You have to cc the list or nobody else will see this.
>
>On Mon, 2013-02-04 at 17:42 +0100, Guillermo Marco wrote:
>> Hello,
>>
>> I'm the only user on the system at this moment so it's not possible.
>> Does Gluster modifies any part of the file indexing or something?
>> On 02/04/2013 05:33 PM, James wrote:
>> > On Mon, 2013-02-04 at 17:24 +0100, Guillermo Marco wrote:
>> >> Hello,
>> >>
>> >> Sometimes when i'm editing a file with located in Gluster with VI
>i get
>> >> this message:
>> >>
>> >> W12: Warning: File "test.sh" has changed and the buffer was
>changed
>> >> in Vim as well
>> >> See ":help W12" for more info.
>> >> [O]K, (L)oad File:
>> > Sounds like the file is being edited by someone else while it's
>open by
>> > you.
>> >
>> >> I've no idea what can be causing that and i'm scared.
>> >>
>> >> Thank you.
>> >>
>> >> Best regards,
>> >> Guillermo.
>> >>
>> >> _______________________________________________
>> >> Gluster-users mailing list
>> >> Gluster-users at gluster.org
>> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>
>
>
>
>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>Gluster-users mailing list
>Gluster-users at gluster.org
>http://supercolony.gluster.org/mailman/listinfo/gluster-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/743e1d64/attachment.html>

From guillermo.marco at sistemasgenomicos.com  Mon Feb  4 17:11:17 2013
From: guillermo.marco at sistemasgenomicos.com (Guillermo Marco)
Date: Mon, 04 Feb 2013 18:11:17 +0100
Subject: [Gluster-users] Fwd: Re:  Fwd: Re:  Editing file with VI
In-Reply-To: <510FEB49.3010206@sistemasgenomicos.com>
References: <510FEB49.3010206@sistemasgenomicos.com>
Message-ID: <510FEBB5.6050609@sistemasgenomicos.com>



Hello,

Yes i've my gluster system on /mnt/gluster/

If i'm editing a file with vi like this: $vi /mnt/gluster/myfile.sh

Sometimes i get the error mentioned below.


On 02/04/2013 05:57 PM, Joe Julian wrote:
> Are you editing files directly on the brick?
>
>
> Guillermo Marco <guillermo.marco at sistemasgenomicos.com> wrote:
>
>     You have to cc the list or nobody else will see this.
>
>     On Mon, 2013-02-04 at 17:42 +0100, Guillermo Marco wrote:
>
>         Hello, I'm the only user on the system at this moment so it's
>         not possible. Does Gluster modifies any part of the file
>         indexing or something? On 02/04/2013 05:33 PM, James wrote:
>
>             On Mon, 2013-02-04 at 17:24 +0100, Guillermo Marco wrote:
>
>                 Hello, Sometimes when i'm editing a file with located
>                 in Gluster with VI i get this message: W12: Warning:
>                 File "test.sh <http://test.sh>" has changed and the
>                 buffer was changed in Vim as well See ":help W12" for
>                 more info. [O]K, (L)oad File: 
>
>             Sounds like the file is being edited by someone else while
>             it's open by you.
>
>                 I've no idea what can be causing that and i'm scared.
>                 Thank you. Best regards, Guillermo.
>                 ------------------------------------------------------------------------
>                 Gluster-users mailing list Gluster-users at gluster.org
>                 http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
>     ------------------------------------------------------------------------
>
>     Gluster-users mailing list
>     Gluster-users at gluster.org
>     http://supercolony.gluster.org/mailman/listinfo/gluster-users
>



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/8747fa1c/attachment-0001.html>

From michael.kushnir at nih.gov  Mon Feb  4 17:14:23 2013
From: michael.kushnir at nih.gov (Kushnir, Michael (NIH/NLM/LHC) [C])
Date: Mon, 4 Feb 2013 17:14:23 +0000
Subject: [Gluster-users] Does fuse mount report IO Wait?
Message-ID: <2B177685F5DDAB479F03287189FD8C530441B843@MLBXV09.nih.gov>

Hi All,

I have a couple small compute clusters set up as clients to GlusterFS volumes mounted over fuse. My GlusterFS servers are separate and independent of these compute clusters.

Are the IO Wait figures that I see in top on the GlusterFS clients accurate and inclusive of the GlusterFS, or should I be looking somewhere else to check for client side IO bottlenecks relating to GlusterFS fuse mounts?

Thanks,
Michael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/3b0fd4df/attachment.html>

From daemons at kanuka.com.au  Tue Feb  5 01:28:46 2013
From: daemons at kanuka.com.au (Daniel Mons)
Date: Tue, 5 Feb 2013 11:28:46 +1000
Subject: [Gluster-users] MacOSX Finder performance woes
In-Reply-To: <CA+iB1e4ccG8tVpnWtgDtt-+krX_x2rebRQFqFL2kFdaRpZ9AsQ@mail.gmail.com>
References: <CAORgGr86U-p_j1OtutzQsVgkMSp5A3F3=_p0B-08tUoAXnjTeQ@mail.gmail.com>
	<CA+iB1e4ccG8tVpnWtgDtt-+krX_x2rebRQFqFL2kFdaRpZ9AsQ@mail.gmail.com>
Message-ID: <CAORgGr8fkh8pExywr5QujBbYFGYpAj3pVc7b=myCSZeF2yheuQ@mail.gmail.com>

Hi Adam,

Sadly the disabling of writing out .DS_Store and other resource-fork type
files does nothing to assist.  The problem is exactly as you describe it -
the negative lookup via Finder on directory reads, which happens regardless
whether or not the client has been told to disabled writing of these files.

As it stands, performance over Netatalk is fine, with .AppleDouble database
files dealing with the resource forks.  The downside is that I can't run
Netatalk on every node in the cluster like I can with Samba or NFS, so it
forces all Mac traffic through one node, killing off any elegant attempt to
distribute bandwidth.  Still, it's a workaround for now until negative
lookup caching can be implemented, or Finder's default behavior is fixed.
(And asking Apple to fix Finder seems to be a dead end, given the grief I
see people having with Finder and NFS shares).

For now, in a 6 node cluster, I have MacOSX clients using AFP to Netatalk
on the first node only, and round-robin DNS distributing Windows/Samba
clients and Legacy/NFS clients across the other five nodes.  Modern Linux
clients happily use FUSE+GlusterFS, and that configuration is getting me
through production for now.

-Dan



On 20 December 2012 14:00, Adam Tygart <mozes at k-state.edu> wrote:

> Dan,
>
> Does something like this help for SAMBA?
> http://www.e-rave.nl/disable-creation-of-ds_store-files-on-samba-shares
>
> Negative lookups are a particularly expensive operation on distributed
> filesystems, GlusterFS in particular. It has to check through each
> brick to see that the file *really* doesn't exist, increasing the
> lookup overhead immensely. If samba doesn't pass these queries to
> GlusterFS, it might help a lot.
>
> --
> Adam Tygart
> Beocat Sysadmin
>
> On Wed, Dec 19, 2012 at 8:29 PM, Daniel Mons <daemons at kanuka.com.au>
> wrote:
> > I'm rolling out 4 lots of 6-node GlusterFS setups for my employer.  Each
> > node is ~33TB of RAID6 backed storage (16x 3TB SATA disks in RAID6 with a
> > hot spare hanging off an LSI controller, with 2x SSDs configured for
> > caching), and Gluster is configured in distribute-replicate.  Each
> cluster
> > is 200TB of raw space, 100TB usable after replication.  When complete,
> there
> > will be 4 of these clusters.
> >
> > Nodes are configured as XFS with 512byte inodes, running a fully patched
> > CentOS6 and Gluster 3.3.1.  Each node has a 6 core Xeon processor (with
> HT
> > for 12 threads) with 32GB of RAM.  Each node runs 2x 10Gbps Ethernet over
> > fiber in a bonded configuration (single IP address per node) for a full
> > 20Gbits per node.
> >
> > GlusterFS FUSE performance under Linux is great (clients run a mix of
> Ubuntu
> > 12.04 LTS for workstations and CentOS6 for servers).  Samba performance
> back
> > to Windows 7 clients is great.  NFS performance via both Gluster's
> userspace
> > setup as well as CentOS6's native NFS4 kernel server are great to most
> other
> > systems where we can't get the Gluster FUSE client loaded (large
> > industry-specific Linux boxes that are provided by vendors as a "black
> box"
> > solution, and only allow limited access via NFS or SMB/CIFS).  All
> testing
> > so far under those conditions proves orders of magnitude faster
> throughput
> > than our existing single NAS solutions.
> >
> > MacOSX Finder performance is a problem, however.  There's a huge bug in
> > MacOSX itself that prevents using NFS at all (discussions on other
> mailing
> > lists suggest it occurred somewhere around 10.6, and continues through
> into
> > 10.7 and 10.8).
> >
> > Mounting via SMB under OSX is more stable than NFS, however in folders
> with
> > a large amount of files, Finder goes looking for a corresponding Apple
> > Resource Fork file (for every "filename.ext", it looks for a
> > "._filename.ext").  Running tcpdump and wireshark on the Gluster nodes
> shows
> > that the resulting "FILE_NOT_FOUND" error back to the client takes a very
> > long time.  Configuring a single node as a pure NAS with the same
> software
> > (but no Gluster implementation) is lightening fast.   As soon as
> GlusterFS
> > comes in to play, reporting of each "FILE_NOT_FOUND" slows down the
> process
> > dramatically, causing a directory with ~1000 images in it to take well
> over
> > 5 minutes to display the contents in MacOSX finder.
> >
> > This problem is resolved somewhat by switching to AFP (via Netatalk
> loaded
> > on the GlusterFS nodes), but it has it's own problems unique to that
> > protocol, and I'd rather stick to GlusterFS-FUSE, NFS or SMB in that
> order
> > of preference.
> >
> > It's worth noting that through the terminal, these problems don't exist.
> > Mounting via SMB, browsing to the volume in terminal and running "ls" or
> > "find" style commands retrieve file listings at a similar speed to Linux
> and
> > Windows.  The problem is limited to clients using Finder to browse
> > directories, and again particularly ones with a large number of files
> that
> > don't have matching Apple Resource Fork files.  (Of note, creating empty
> > files of the matching "._filename.ext" format solves the performance
> > problem, but litters our filestores with millions of empty files, which
> we
> > don't want).
> >
> > I understand the problem is not strictly Gluster's issue.  Finder is
> looking
> > for a heck of a lot of files that don't exist (which is a pretty silly
> > design), and it tends to occur only with Samba re-exporting GlusterFS
> > volumes that we can see.  And likewise Apple's NFS bug that has now been
> in
> > existence across three releases of their OS is pretty horrible.   But
> > hopefully I can at least describe the problem and prompt some testing by
> > others.
> >
> > I haven't had a chance to test a MacOSX FUSE client due to time
> constraints,
> > but that would at least answer the question if the problem is Gluster's
> lag
> > in reporting of files not found, or Samba's.
> >
> > -Dan
> >
> >
> > _______________________________________________
> > Gluster-users mailing list
> > Gluster-users at gluster.org
> > http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130205/d62446a1/attachment.html>

From glusterzhxue at 163.com  Tue Feb  5 02:53:28 2013
From: glusterzhxue at 163.com (glusterzhxue)
Date: Tue, 5 Feb 2013 10:53:28 +0800
Subject: [Gluster-users] Does Gluster not support mul-thread to the same
	file?
Message-ID: <2013020510532749544812@163.com>


I used a mul-thread program to write the same file in gluster. We just gained about 30MB/s write performance by gluster native client, whereas, using the same program, it's 100MB/s writing performance if mount gluster volume by NFS.

Could anyone tell me why? and how to improve write performance for mul-thread writing to the same file mouted by gluster native client. Thanks. 

Regards,
Zhenghua
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130205/8be8d50b/attachment.html>

From anand.avati at gmail.com  Tue Feb  5 03:29:14 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Mon, 4 Feb 2013 19:29:14 -0800
Subject: [Gluster-users] Does Gluster not support mul-thread to the same
	file?
In-Reply-To: <2013020510532749544812@163.com>
References: <2013020510532749544812@163.com>
Message-ID: <CAFboF2zzXgJgm72F04ZfpvAs_1HN_mMqrxP29ny=iPQ4AqLydQ@mail.gmail.com>

On Mon, Feb 4, 2013 at 6:53 PM, glusterzhxue <glusterzhxue at 163.com> wrote:

> **
>
> I used a mul-thread program to write the same file in gluster. We just
> gained about 30MB/s write performance by gluster native client, whereas,
> using the same program, it's 100MB/s writing performance if mount gluster
> volume by NFS.
>
> Could anyone tell me why? and how to improve write performance for
> mul-thread writing to the same file mouted by gluster native client.
> Thanks.
>
>
Have you tested this to be not a behavior of the NFS client? How does
multithreaded writes behave with NFS client and Linux kernel NFS server?

Avati
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130204/5a9928de/attachment-0001.html>

From lhawthor at redhat.com  Tue Feb  5 14:49:31 2013
From: lhawthor at redhat.com (Leslie Hawthorn)
Date: Tue, 05 Feb 2013 15:49:31 +0100
Subject: [Gluster-users] Gluster Community Summit: Join us March 7-8,
	2013 in Bangalore
Message-ID: <51111BFB.9020807@redhat.com>

Hello everyone,

The Gluster Community Team is pleased to announce the first ever Gluster 
Community Summit, which will take place at Red Hat's Bangalore Campus on 
7-8 March 2013. We hope that you will be able to join us for the summit 
and in crafting our agenda.

You can find a preliminary set of topics for the meeting on the Gluster 
wiki. [0] We look forward to recommendations for further items to 
address during the summit; please add your thoughts to the wiki page or 
feel free to comment in this thread if you'd prefer.

Last but not least, there is no cost to attend the summit, but 
registration is required. We only have space for 35 individuals to 
attend due to constraints on conference room space, so please take a 
moment to register now. [1]

[0] - 
http://www.gluster.org/community/documentation/index.php/Planning/Community_Summit

[1] - http://glusterdevsummit2013.eventbrite.com/

With best regards,
John Mark Walker and Leslie Hawthorn

-- 
Leslie Hawthorn
Community Action and Impact
Open Source and Standards @ Red Hat

identi.ca/lh
twitter.com/lhawthorn


From jhorne at skopos.us  Wed Feb  6 01:30:50 2013
From: jhorne at skopos.us (Jonathan Horne)
Date: Wed, 6 Feb 2013 01:30:50 +0000
Subject: [Gluster-users] write performance
Message-ID: <9BE6F493F83A594DA60C45E6A09DC5AC01744227@AUSP01DAG0201.collaborationhost.net>

Hello,

I have a 2 node glusterfs, with 2 clients connecting.  brick0 is shared out, and when the clients try to write to it, say my example I've been running is that I wget a .iso file from my server, and it writes at about 39MB/s.  the same 2 client servers when I download the same .iso file to its local storage (which is actually on the same SAN raid group as where both gluster file systems live), and the clients can write to their own file systems at 110MB/s.  (i.e.? so its not the SAN).

Is this the type of performance I should expect from gluster, or is there some tuning I can do to increase the read writes?

Today when we tested our intended configuration for our new production servers, the 2 clients store their /var/www/html as as single folder that is on the glusterfs.  But when you log in and try to access the web pages, they login and open from page to page unacceptably slow.

If there is some tuning I can do to squeeze some better performance out of this, id love some advice or tips.

Thanks,
jonathan

________________________________
This is a PRIVATE message. If you are not the intended recipient, please delete without copying and kindly advise us by e-mail of the mistake in delivery. NOTE: Regardless of content, this e-mail shall not operate to bind SKOPOS to any order or other contract unless pursuant to explicit written agreement or government initiative expressly permitting the use of e-mail for such purpose.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130206/3344c7cb/attachment.html>

From yongtaofu at gmail.com  Wed Feb  6 01:49:55 2013
From: yongtaofu at gmail.com (=?GB2312?B?t/vTwMzO?=)
Date: Wed, 6 Feb 2013 09:49:55 +0800
Subject: [Gluster-users] volume status report brick port NA while online
 status is Y after restore brick
Message-ID: <CADFMGuLh6g0fVBK1v9iqB0W6FTJejV8J5byV3cW-MG9nPB-KMw@mail.gmail.com>

Dear gluster experts,

Recently I have restored one brick of my volume from hardware failure
and everything works fine except that the volume status command report
the resotred brick port as NA while the brick online statis is Y.
What's wrong?

Thank you.

-- 
???

From schroete at iup.physik.uni-bremen.de  Wed Feb  6 13:04:36 2013
From: schroete at iup.physik.uni-bremen.de (=?ISO-8859-15?Q?Heiko_Schr=F6ter?=)
Date: Wed, 06 Feb 2013 14:04:36 +0100
Subject: [Gluster-users] Raid removed
Message-ID: <511254E4.70208@iup.physik.uni-bremen.de>

Hello,

because of a hardware failure a brick had to be removed from the gluster 
(3.2.5) pool.
Fortunatly the hardware could be repaired and the raid array with the 
data is operational again.


Is it possible to "re-insert" this brick into the gluster pool without 
copying all data (15TB) ?

Thanks.
Regards
Heiko


From lhawthor at redhat.com  Wed Feb  6 15:52:40 2013
From: lhawthor at redhat.com (Leslie Hawthorn)
Date: Wed, 6 Feb 2013 10:52:40 -0500 (EST)
Subject: [Gluster-users] Upcoming Workshop: CERN, 26 February
In-Reply-To: <50FF2296.10509@redhat.com>
Message-ID: <692024893.280647.1360165960608.JavaMail.root@redhat.com>

Hello everyone,

We have published a preliminary agenda [0] for the upcoming Gluster Workshop at CERN in *Geneva,* Switzerland. We'll continue updating the agenda on the Gluster wiki as more news becomes available.

http://www.gluster.org/community/documentation/index.php/Planning/CERN_Workshop

Best,
LH

----- Original Message -----
From: "Leslie Hawthorn" <lhawthor at redhat.com>
To: gluster-users at gluster.org
Sent: Wednesday, January 23, 2013 12:36:54 AM
Subject: Upcoming Workshop: CERN, 26 February

Hello everyone,

The Gluster Community Team will be hosting a workshop for the employees 
of CERN on Tuesday, 26 February 2013. We also have a limited number of 
places we can offer to the wider community who would like to attend the 
workshop.

If you would be interested in joining us and will be in and around 
Zurich, Switzerland on 26 February, please contact me off-list for an 
invitation to the workshop.

Agenda details are forthcoming; please stay tuned.

Cheers,
LH

-- 
Leslie Hawthorn
Community Action and Impact
Open Source and Standards @ Red Hat

identi.ca/lh
twitter.com/lhawthorn


From michael.kushnir at nih.gov  Wed Feb  6 16:58:14 2013
From: michael.kushnir at nih.gov (Kushnir, Michael (NIH/NLM/LHC) [C])
Date: Wed, 6 Feb 2013 16:58:14 +0000
Subject: [Gluster-users] Does fuse mount report IO Wait?
In-Reply-To: <2B177685F5DDAB479F03287189FD8C530441B843@MLBXV09.nih.gov>
References: <2B177685F5DDAB479F03287189FD8C530441B843@MLBXV09.nih.gov>
Message-ID: <2B177685F5DDAB479F03287189FD8C53044430A5@MLBXV09.nih.gov>

Anyone?

Thanks,
Michael

From: Kushnir, Michael (NIH/NLM/LHC) [C]
Sent: Monday, February 04, 2013 12:14 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] Does fuse mount report IO Wait?

Hi All,

I have a couple small compute clusters set up as clients to GlusterFS volumes mounted over fuse. My GlusterFS servers are separate and independent of these compute clusters.

Are the IO Wait figures that I see in top on the GlusterFS clients accurate and inclusive of the GlusterFS, or should I be looking somewhere else to check for client side IO bottlenecks relating to GlusterFS fuse mounts?

Thanks,
Michael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130206/f15d81ce/attachment.html>

From anand.avati at gmail.com  Wed Feb  6 20:09:02 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Wed, 6 Feb 2013 12:09:02 -0800
Subject: [Gluster-users] Does fuse mount report IO Wait?
In-Reply-To: <2B177685F5DDAB479F03287189FD8C530441B843@MLBXV09.nih.gov>
References: <2B177685F5DDAB479F03287189FD8C530441B843@MLBXV09.nih.gov>
Message-ID: <CAFboF2x7Ka4gZHdv8jcOfG49to82K6xrmVuVE6w6xJ-s=T1=Xg@mail.gmail.com>

On Mon, Feb 4, 2013 at 9:14 AM, Kushnir, Michael (NIH/NLM/LHC) [C] <
michael.kushnir at nih.gov> wrote:

>  Hi All, ****
>
> ** **
>
> I have a couple small compute clusters set up as clients to GlusterFS
> volumes mounted over fuse. My GlusterFS servers are separate and
> independent of these compute clusters. ****
>
> ** **
>
> Are the IO Wait figures that I see in top on the GlusterFS clients
> accurate and inclusive of the GlusterFS, or should I be looking somewhere
> else to check for client side IO bottlenecks relating to GlusterFS fuse
> mounts?****
>
>
> Thanks,
> Michael ****
>

IIRC it should include client side IO delays. It would be best to ask this
question on fuse-devel@ list.

Avati
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130206/398e668a/attachment.html>

From bcipriano at zerovfx.com  Wed Feb  6 21:13:51 2013
From: bcipriano at zerovfx.com (Brian Cipriano)
Date: Wed, 06 Feb 2013 16:13:51 -0500
Subject: [Gluster-users] errors during rebalance, EC2
Message-ID: <5112C78F.60300@zerovfx.com>

Hi all -

Having some issues doing a rebalance on our gluster, hoping to get some 
input.

We're running gluster 3.3.0 on Amazon EC2 nodes. When we try to do a 
rebalance, we see a very high error rate. Lots of these:

[2013-02-06 02:53:17.032693] W [client3_1-fops.c:474:client3_1_stat_cbk] 
0-uswest2-client-0: remote operation failed: No such file or directory

[2013-02-06 02:53:17.361387] W 
[client3_1-fops.c:258:client3_1_mknod_cbk] 0-uswest2-client-4: remote 
operation failed: File exists. Path: /path/to/file 
(00000000-0000-0000-0000-000000000000)

I.e., various errors indicating a failure to copy files. When these 
errors occur, they seem to result in corrupted files.

Has anyone else had this problem? Any suggestions?

Our bricks in this case are AWS EBS volumes, i.e., they are virtual 
drives that are networked, but appear to the system as locally attached 
drives. I wonder if this has something to do with it - some slight lag 
is causing gluster or fuse to think there's a file error, when really it 
just needs to wait a bit longer?

Thanks for your help,

- brian

From sgowda at redhat.com  Thu Feb  7 07:04:53 2013
From: sgowda at redhat.com (Shishir Gowda)
Date: Thu, 7 Feb 2013 02:04:53 -0500 (EST)
Subject: [Gluster-users] errors during rebalance, EC2
In-Reply-To: <5112C78F.60300@zerovfx.com>
Message-ID: <1088188587.22488713.1360220693379.JavaMail.root@redhat.com>

Hi Brian,

Can you please provide the volume info output.
Additionally, a stat/ls -l output of any one of the files directly from the backend(wherever they exist).
Logs would help too(rebalance and client). 

what is the corruption you are seeing?

With regards,
Shishir
----- Original Message -----
From: "Brian Cipriano" <bcipriano at zerovfx.com>
To: gluster-users at gluster.org
Sent: Thursday, February 7, 2013 2:43:51 AM
Subject: [Gluster-users] errors during rebalance, EC2

Hi all -

Having some issues doing a rebalance on our gluster, hoping to get some 
input.

We're running gluster 3.3.0 on Amazon EC2 nodes. When we try to do a 
rebalance, we see a very high error rate. Lots of these:

[2013-02-06 02:53:17.032693] W [client3_1-fops.c:474:client3_1_stat_cbk] 
0-uswest2-client-0: remote operation failed: No such file or directory

[2013-02-06 02:53:17.361387] W 
[client3_1-fops.c:258:client3_1_mknod_cbk] 0-uswest2-client-4: remote 
operation failed: File exists. Path: /path/to/file 
(00000000-0000-0000-0000-000000000000)

I.e., various errors indicating a failure to copy files. When these 
errors occur, they seem to result in corrupted files.

Has anyone else had this problem? Any suggestions?

Our bricks in this case are AWS EBS volumes, i.e., they are virtual 
drives that are networked, but appear to the system as locally attached 
drives. I wonder if this has something to do with it - some slight lag 
is causing gluster or fuse to think there's a file error, when really it 
just needs to wait a bit longer?

Thanks for your help,

- brian
_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

From radecki.rafal at gmail.com  Thu Feb  7 13:57:58 2013
From: radecki.rafal at gmail.com (=?ISO-8859-2?Q?Rafa=B3_Radecki?=)
Date: Thu, 7 Feb 2013 14:57:58 +0100
Subject: [Gluster-users] Gluster - data migration.
Message-ID: <CAHd9_iTJfnyvSo30VqbWW3o-FtNsLj3eagiqO1JQy+x-FFKGQw@mail.gmail.com>

Hi All.

I have two servers ("master" and "slave") with a replicated gluster
volume. Recently I've had a problem with slave and gluster does not
work on it now.
So I would like to:
- stop and remove current volume on master (on slave it is not accessible);
- stop gluster software on master (already stopped on slave);
- remove gluster software on master and slave (previous administrator
used own built rpms, I would like to use
http://download.gluster.org/pub/gluster/glusterfs/3.3/3.3.1/EPEL.repo/);
- clean old information:
setfattr -x trusted.glusterfs.volume-id /gluster
setfattr -x trusted.gfid /gluster
rm -rf /gluster/.glusterfs
- rsync data from master to slave;
- start gluster and create a volume with data in rsynced /gluster directory.
Are there any pitfalls I should know about?

Best regards,
Rafal Radecki.

From radecki.rafal at gmail.com  Thu Feb  7 13:58:22 2013
From: radecki.rafal at gmail.com (=?ISO-8859-2?Q?Rafa=B3_Radecki?=)
Date: Thu, 7 Feb 2013 14:58:22 +0100
Subject: [Gluster-users] Gluster 3.3 - good to use?
Message-ID: <CAHd9_iQMVEwEaFaMSzBAHy03hxaA_8cKA6yTLYHpOcBjWe_pSw@mail.gmail.com>

Hi All.

I want to use mentioned version of glusterfs in a simple master/slave
setup (production servers). Two nodes have a replicated volume and the
both mount it using glusterfs. What are your experiences with gluster
3.3? Is it stable? Have you had any serious problems? I do not have
much experience with gluster and would like to avoid problems if
possible.

Best regards,
Rafal Radecki.

From robinr at miamioh.edu  Thu Feb  7 14:35:03 2013
From: robinr at miamioh.edu (R, Robin)
Date: Thu, 7 Feb 2013 09:35:03 -0500
Subject: [Gluster-users] Gluster 3.3 - good to use?
In-Reply-To: <CAHd9_iQMVEwEaFaMSzBAHy03hxaA_8cKA6yTLYHpOcBjWe_pSw@mail.gmail.com>
References: <CAHd9_iQMVEwEaFaMSzBAHy03hxaA_8cKA6yTLYHpOcBjWe_pSw@mail.gmail.com>
Message-ID: <CAG35g9-MCW=-V6P3Qu04qfj5a0L8HGEoAT4CDVpM5fKLg4QqYA@mail.gmail.com>

Hi,

I've used Gluster-3.3.0 (with replica 2 on all volumes) with good success.

When updating to Gluster-3.3.1, I hit a bug on the NFS which caused the NFS
daemon to die.

I filed that NFS bug on
https://bugzilla.redhat.com/show_bug.cgi?id=902857

Another user also commented that he hit the same NFS bug on 3.3.1 in the
bugzilla report.

I'm not certain what I did to trigger that NFS bug (I could not replicate
it on test systems -- my test systems do not have as much data though).

I downgraded to 3.3.0 which has been running fine for at least half a year
now.

Robin

On Thu, Feb 7, 2013 at 8:58 AM, Rafa? Radecki <radecki.rafal at gmail.com>wrote:

> Hi All.
>
> I want to use mentioned version of glusterfs in a simple master/slave
> setup (production servers). Two nodes have a replicated volume and the
> both mount it using glusterfs. What are your experiences with gluster
> 3.3? Is it stable? Have you had any serious problems? I do not have
> much experience with gluster and would like to avoid problems if
> possible.
>
> Best regards,
> Rafal Radecki.
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130207/bb37e58a/attachment.html>

From service at vhosting-it.com  Thu Feb  7 23:33:16 2013
From: service at vhosting-it.com (VHosting Solution)
Date: Fri, 8 Feb 2013 00:33:16 +0100
Subject: [Gluster-users] gluster-information
Message-ID: <6803BBE344F2492D9B11EDBE6A6DBAC8@VHostingTOSH>

Hello at all, I'm trying the gluster fs replication for my cluster in HA but I do not have understood an thing.

For autoreplicate the file accross all nodes, what is the directory that I must use for work?

This is my setup

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root at node1 ~]# df -H
Filesystem             Size   Used  Avail Use% Mounted on
rootfs                  21G   850M    19G   5% /
/dev/root               21G   850M    19G   5% /
none                   1.1G   316k   1.1G   1% /dev
tmpfs                  1.1G      0   1.1G   0% /dev/shm
/dev/sda2              479G   298M   479G   1% /export/brick1
localhost:/gv0         479G   298M   479G   1% /mnt

[root at node2 ~]# df -H
Filesystem             Size   Used  Avail Use% Mounted on
rootfs                  21G   849M    19G   5% /
/dev/root               21G   849M    19G   5% /
none                   1.1G   308k   1.1G   1% /dev
tmpfs                  1.1G      0   1.1G   0% /dev/shm
/dev/sda2              479G   297M   479G   1% /export/brick1
localhost:/gv0         479G   298M   479G   1% /mnt

[root at node1 ~]# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 9967e4bb-48bd-43e0-ae25-d985df935ea3
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: node2.xxx.com:/export/brick1
Brick2: node1.xxx.com:/export/brick1

[root at node2 ~]# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 9967e4bb-48bd-43e0-ae25-d985df935ea3
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: node2.xxx.com:/export/brick1
Brick2: node1.xxx.com:/export/brick1

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

I must work on /mnt mount or into /export/brick1? If I work on /export/brick1 I receive "I/O error"

Is correct my setup?

Thanks you so much.

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130208/7c69dd44/attachment-0001.html>

From service at vhosting-it.com  Fri Feb  8 01:06:33 2013
From: service at vhosting-it.com (VHosting Solution)
Date: Fri, 8 Feb 2013 02:06:33 +0100
Subject: [Gluster-users] gluster-information
Message-ID: <47BA16815BEE45F4B57FE478C04BEA42@VHostingTOSH>

Hello at all, I'm trying the gluster fs replication for my cluster in HA but I do not have understood an thing.

For autoreplicate the file accross all nodes, what is the directory that I must use for work?

This is my setup

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root at node1 ~]# df -H
Filesystem             Size   Used  Avail Use% Mounted on
rootfs                  21G   850M    19G   5% /
/dev/root               21G   850M    19G   5% /
none                   1.1G   316k   1.1G   1% /dev
tmpfs                  1.1G      0   1.1G   0% /dev/shm
/dev/sda2              479G   298M   479G   1% /export/brick1
localhost:/gv0         479G   298M   479G   1% /mnt

[root at node2 ~]# df -H
Filesystem             Size   Used  Avail Use% Mounted on
rootfs                  21G   849M    19G   5% /
/dev/root               21G   849M    19G   5% /
none                   1.1G   308k   1.1G   1% /dev
tmpfs                  1.1G      0   1.1G   0% /dev/shm
/dev/sda2              479G   297M   479G   1% /export/brick1
localhost:/gv0         479G   298M   479G   1% /mnt

[root at node1 ~]# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 9967e4bb-48bd-43e0-ae25-d985df935ea3
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: node2.xxx.com:/export/brick1
Brick2: node1.xxx.com:/export/brick1

[root at node2 ~]# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 9967e4bb-48bd-43e0-ae25-d985df935ea3
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: node2.xxx.com:/export/brick1
Brick2: node1.xxx.com:/export/brick1

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

I must work on /mnt mount or into /export/brick1? If I work on /export/brick1 I receive "I/O error"

Is correct my setup?

Thanks you so much.

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130208/b3d62699/attachment-0001.html>

From johnmark at redhat.com  Fri Feb  8 01:14:08 2013
From: johnmark at redhat.com (John Mark Walker)
Date: Thu, 7 Feb 2013 20:14:08 -0500 (EST)
Subject: [Gluster-users] gluster-information
Message-ID: <634xrxkwefxoe6riifvxyld0.1360286044272@email.android.com>

Yes, everything must go through the mount point. Everything will automatically replicate to both bricks.

VHosting Solution <service at vhosting-it.com> wrote:

Hello at all, I'm trying the gluster fs replication for my cluster in HA but I do not have understood an thing.

For autoreplicate the file accross all nodes, what is the directory that I must use for work?

This is my setup

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

[root at node1 ~]# df -H
Filesystem             Size   Used  Avail Use% Mounted on
rootfs                  21G   850M    19G   5% /
/dev/root               21G   850M    19G   5% /
none                   1.1G   316k   1.1G   1% /dev
tmpfs                  1.1G      0   1.1G   0% /dev/shm
/dev/sda2              479G   298M   479G   1% /export/brick1
localhost:/gv0         479G   298M   479G   1% /mnt

[root at node2 ~]# df -H
Filesystem             Size   Used  Avail Use% Mounted on
rootfs                  21G   849M    19G   5% /
/dev/root               21G   849M    19G   5% /
none                   1.1G   308k   1.1G   1% /dev
tmpfs                  1.1G      0   1.1G   0% /dev/shm
/dev/sda2              479G   297M   479G   1% /export/brick1
localhost:/gv0         479G   298M   479G   1% /mnt

[root at node1 ~]# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 9967e4bb-48bd-43e0-ae25-d985df935ea3
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: node2.xxx.com:/export/brick1
Brick2: node1.xxx.com:/export/brick1

[root at node2 ~]# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 9967e4bb-48bd-43e0-ae25-d985df935ea3
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: node2.xxx.com:/export/brick1
Brick2: node1.xxx.com:/export/brick1

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

I must work on /mnt mount or into /export/brick1? If I work on /export/brick1 I receive "I/O error"

Is correct my setup?

Thanks you so much.

Regards

From B.Candler at pobox.com  Fri Feb  8 08:23:26 2013
From: B.Candler at pobox.com (Brian Candler)
Date: Fri, 8 Feb 2013 08:23:26 +0000
Subject: [Gluster-users] gluster-information
In-Reply-To: <6803BBE344F2492D9B11EDBE6A6DBAC8@VHostingTOSH>
References: <6803BBE344F2492D9B11EDBE6A6DBAC8@VHostingTOSH>
Message-ID: <20130208082326.GB4294@nsrc.org>

On Fri, Feb 08, 2013 at 12:33:16AM +0100, VHosting Solution wrote:
>    For autoreplicate the file accross all nodes, what is the directory
>    that I must use for work?

You must mount the volume on a client, and work with it there.

From nux at li.nux.ro  Fri Feb  8 17:39:16 2013
From: nux at li.nux.ro (Nux!)
Date: Fri, 08 Feb 2013 17:39:16 +0000
Subject: [Gluster-users] Quota command failed (with 3.4qa8)
Message-ID: <dc1cb3380e69f68d73c4d72582666b55@li.nux.ro>

Hello,

I've upgraded my glusterfs servers and clients to 3.4qa8. Now I'm 
trying to set a quota on a volume, but I get this:
###
gluster volume quota blah enable

Quota command failed
###


Logs say nothing. Where do I look for more info?




-- 
Sent from the Delta quadrant using Borg technology!

Nux!
www.nux.ro

From sking at kingrst.com  Fri Feb  8 22:14:56 2013
From: sking at kingrst.com (Steven King)
Date: Fri, 08 Feb 2013 17:14:56 -0500
Subject: [Gluster-users] GlusterFS OOM Issue
Message-ID: <511578E0.5020605@kingrst.com>

Hello,

I am running GlusterFS version 3.2.7-2~bpo60+1 on Debian 6.0.6. Today, I 
have experienced a a glusterfs process cause the server to invoke 
oom_killer.

How exactly would I go about investigating this and coming up with a fix?

-- 
Steve King

Network/Linux Engineer - AdSafe Media
Cisco Certified Network Professional
CompTIA Linux+ Certified Professional
CompTIA A+ Certified Professional


From ling at slac.stanford.edu  Fri Feb  8 22:36:16 2013
From: ling at slac.stanford.edu (Ling Ho)
Date: Fri, 08 Feb 2013 14:36:16 -0800
Subject: [Gluster-users] Write failure on distributed volume with free
 space available
In-Reply-To: <5106FDC1.4030206@redhat.com>
References: <aa0e20731907aa67ea7a0573cb7a7aad@li.nux.ro>
	<d5f2f291208462c285f3acbc7b88ed45@li.nux.ro>	<5106DC2B.30800@redhat.com>
	<5106F4A2.5080803@ekt.gr> <5106F95E.8040601@slac.stanford.edu>
	<5106FDC1.4030206@redhat.com>
Message-ID: <51157DE0.3070606@slac.stanford.edu>

Last night I ran into problem where some of the bricks in my volume 
completed filled up to 100%, while others were still below 90%. The 
bricks are 25TB each, so 10% would be about 2.5TB. The files we were 
writing were not more than 100GB each so even a number of concurrent 
file writes were not capable of filling up 10% in a very short amount of 
time. We have 40 bricks in the volume.

Looks like the default min-free-disk setting (which I did not set) is 
not observed.

After trying to set it to 10%, and also 500GB on a different try, I 
tried creating 1000 new empty files on a client (after remounting the 
volume), and it still writes to the bricks with less than 10% or less 
than 500GB free.

Is this a feature available in later released only? I am using Stable 
3.3.1 .

Thanks,
...
ling

On 01/28/2013 02:37 PM, Jeff Darcy wrote:
> On 01/28/2013 05:19 PM, Ling Ho wrote:
>> How "full" does it has to be before new files start getting written into
>> the other bricks?
> By default, min-free-disk is set to 10% and min-free-inodes is set to 5%.
>
>> In my recent experience, I added a new brick to an existing volume while
>> one of the existing 4 bricks was close to full. And yet I constantly get
>> out of space error when trying to write new files. Full rebalancing was
>> also such as slow process that it cannot keep up with new files I need
>> to write to the volume.
> Yes, rebalancing is slow.  I have work in progress to make it less slow
> by doing the fix-layout part minimally instead of for every single
> directory in the entire volume, and to skip the migrate-data part
> entirely in favor of just letting new data get placed onto new bricks.
> I don't know when those will make it into a release, but if you're
> interested you can read about the work in progress here.
>
> http://hekafs.org/index.php/2012/03/spreading-the-load/
> http://hekafs.org/index.php/2012/05/the-quest-for-balance/
> http://review.gluster.org/#change,3573
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From bfoster at redhat.com  Sat Feb  9 19:26:18 2013
From: bfoster at redhat.com (Brian Foster)
Date: Sat, 09 Feb 2013 14:26:18 -0500
Subject: [Gluster-users] GlusterFS OOM Issue
In-Reply-To: <511578E0.5020605@kingrst.com>
References: <511578E0.5020605@kingrst.com>
Message-ID: <5116A2DA.7060908@redhat.com>

On 02/08/2013 05:14 PM, Steven King wrote:
> Hello,
> 
> I am running GlusterFS version 3.2.7-2~bpo60+1 on Debian 6.0.6. Today, I
> have experienced a a glusterfs process cause the server to invoke
> oom_killer.
> 
> How exactly would I go about investigating this and coming up with a fix?
> 

The OOM killer output to syslog and details on your hardware might be
useful to include.

Following that, you could monitor the address space (VIRT) and set size
(RES/RSS) of the relevant processes with top on your server. For
example, is there a sudden increase in set size or does it constantly,
gradually increase?

Brian

From mcolonno at stanford.edu  Sat Feb  9 20:02:08 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Sat, 9 Feb 2013 12:02:08 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <510EBE98.9000807@julianfamily.org>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>	<019901ce00be$2d787ba0$886972e0$@stanford.edu>	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>	<00d301ce00de$ad920250$08b606f0$@stanford.edu>	<015101ce0243$d464c000$7d2e4000$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
Message-ID: <00b801ce0700$57d93450$078b9cf0$@stanford.edu>

            Follow up: still seeing extremely high CPU loads on all bricks
during any file activity. e.g. from top:

 

            630 root      20   0  319m  85m 2676 R 134.8  0.3   1023:21
glusterfs

26486 root    20   0 1921m  72m 2124 S 41.5   0.2 418:27.29 glusterfsd

 

The 134.8% CPU is curious reporting (peaks up to 200% or so) but these
readings are typical of any of the eight brick systems in the volume when
either files are being read / written or an application stored in the volume
is in use on any of the systems in the brick. Is this kind of load expected
in an eight-brick 2x replicated volume? When I grabbed the readings above no
files were being accesses but an application file was in use. Is there any
way I can get more detailed performance metrics while in use? 

 

            Thanks,

            ~Mike C. 

 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
Sent: Sunday, February 03, 2013 11:47 AM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

On 02/03/2013 11:22 AM, Michael Colonno wrote:



            Having taken a lot more data it does seem the glusterfsd and
glusterd processes (along with several ksoftirqd) spike up to near 100% on
both client and brick servers during any file transport across the mount.
Thankfully this is short-lived for the most part but I'm wondering if this
is expected behavior or what others have experienced(?) I'm a little
surprised such a large CPU load would be required to move small files and /
or use an application within a Gluster mount point. 


If you're getting ksoftirqd spikes, that sounds like a hardware issue to me.
I never see huge spikes like that on my servers nor clients.




 

            I wanted to test this against an NFS mount of the same Gluster
volume. I managed to get rstatd installed and running but my attempts to
mount the volume via NFS are met with: 

 

            mount.nfs: requested NFS version or transport protocol is not
supported

 

            Relevant line in /etc/fstab:

 

            node1:/volume    /volume    nfs
defaults,_netdev,vers=3,mountproto=tcp        0 0      

 

It looks like CentOS 6.x has NFS version 4 built into everything. So a few
questions:

 

-       Has anyone else noted significant performance differences between a
glusterfs mount and NFS mount for volumes of 8+ bricks? 

-       Is there a straightforward way to make the newer versions of CentOS
play nice with NFS version 3 + Gluster? 

-       Are there any general performance tuning guidelines I can follow to
improve CPU performance? I found a few references to the cache settings but
nothing solid. 

 

If the consensus is that NFS will not gain anything then I won't waste the
time setting it all up. 


NFS gains you the use of FSCache to cache directories and file stats making
directory listings faster, but it adds overhead decreasing the overall
throughput (from all the reports I've seen).

I would suspect that you have the kernel nfs server running on your servers.
Make sure it's disabled.




 

Thanks,

~Mike C. 

 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 4:46 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

            Update: after a few hours the CPU usage seems to have dropped
down to a small value. I did not change anything with respect to the
configuration or unmount / stop anything as I wanted to see if this would
persist for a long period of time. Both the client and the self-mounted
bricks are now showing CPU < 1% (as reported by top). Prior to the larger
CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is
this kind a transient behavior - by which I mean larger CPU loads after a
lot of filesystem activity in short time - typical? This is not a problem in
my deployment; I just want to know what to expect in the future and to
complete this thread for future users. If this is expected behavior we can
wrap up this thread. If not then I'll do more digging into the logs on the
client and brick sides. 

 

            Thanks,

            ~Mike C. 

 

From: Joe Julian [mailto:joe at julianfamily.org] 
Sent: Friday, February 01, 2013 2:08 PM
To: Michael Colonno; gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

Check the client log(s). 

Michael Colonno <mcolonno at stanford.edu> wrote:

            Forgot to mention: on a client system (not a brick) the
glusterfs process is consuming ~ 68% CPU continuously. This is a much less
powerful desktop system so the CPU load can't be compared 1:1 with the
systems comprising the bricks but still very high. So the issue seems to
exist with both glusterfsd and glusterfs processes. 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 12:46 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] high CPU load on all bricks

 

            Gluster gurus ~

 

            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
the volume. On each system contributing a brick, however, my CPU usage
(glusterfsd) is hovering around 20% (virtually zero memory usage
thankfully). These are brand new, fairly beefy servers so 20% CPU load is
quite a bit. The deployment is pretty plain with each brick mounting the
volume to itself via a glusterfs mount. I assume this type of CPU usage is
atypically high; is there anything I can do to investigate what's soaking up
CPU and minimize it? Total usable volume size is only about 22 TB (about 45
TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 


  _____  

 
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users






_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130209/cde7d6c9/attachment-0001.html>

From vshastry at redhat.com  Mon Feb 11 06:15:09 2013
From: vshastry at redhat.com (Varun Shastry)
Date: Mon, 11 Feb 2013 11:45:09 +0530
Subject: [Gluster-users] Quota command failed (with 3.4qa8)
In-Reply-To: <dc1cb3380e69f68d73c4d72582666b55@li.nux.ro>
References: <dc1cb3380e69f68d73c4d72582666b55@li.nux.ro>
Message-ID: <51188C6D.3060305@redhat.com>

Hi Nux,

Can you ensure whether its not related to either of the below,
(I doubt the below cases, but just ensuring its not the known case)
https://bugzilla.redhat.com/show_bug.cgi?id=909798
https://bugzilla.redhat.com/show_bug.cgi?id=819973

If not as above case, it would be great to look into glusterd logs. And
can you check does the file in backend contains extended attributes
related to quota (trusted.glusterfs.quota.*).

Thanks
Varun Shastry

On Friday 08 February 2013 11:09 PM, Nux! wrote:
> Hello,
>
> I've upgraded my glusterfs servers and clients to 3.4qa8. Now I'm
> trying to set a quota on a volume, but I get this:
> ###
> gluster volume quota blah enable
>
> Quota command failed
> ###
>
>
> Logs say nothing. Where do I look for more info?
>
>
>
>


From nux at li.nux.ro  Mon Feb 11 08:28:06 2013
From: nux at li.nux.ro (Nux!)
Date: Mon, 11 Feb 2013 08:28:06 +0000
Subject: [Gluster-users] Quota command failed (with 3.4qa8)
In-Reply-To: <51188C6D.3060305@redhat.com>
References: <dc1cb3380e69f68d73c4d72582666b55@li.nux.ro>
	<51188C6D.3060305@redhat.com>
Message-ID: <fd42778c814682d2def25ab1b3f5af47@li.nux.ro>

On 11.02.2013 06:15, Varun Shastry wrote:
> Hi Nux,
> 
> Can you ensure whether its not related to either of the below,
> (I doubt the below cases, but just ensuring its not the known case)
> https://bugzilla.redhat.com/show_bug.cgi?id=909798
> https://bugzilla.redhat.com/show_bug.cgi?id=819973

Hello,

Thanks, but it's not that. Logs revealed nothing at the time; 
unfortunately I can no longer test as I reverted back to 3.3..

-- 
Sent from the Delta quadrant using Borg technology!

Nux!
www.nux.ro

From johnmark at johnmark.org  Mon Feb 11 17:18:38 2013
From: johnmark at johnmark.org (John Mark Walker)
Date: Mon, 11 Feb 2013 12:18:38 -0500
Subject: [Gluster-users] GlusterFS 3.4 Alpha Release
Message-ID: <CAMUFnfaq73NqYxvoGsQR0hAba8omPbjpaDsu6=puitrH0Mcegw@mail.gmail.com>

Here's a write-up on the newly-released GlusterFS 3.4 alpha:

http://www.gluster.org/2013/02/new-release-glusterfs-3-4alpha/


Note that community builds are showing up on download.gluster.org:

http://download.gluster.org/pub/gluster/glusterfs/qa-releases/3.4.0alpha/


And the Git repository has now been tagged for 3.4:

https://github.com/gluster/glusterfs/tree/release-3.4


In order to make this release the Best. Release. Evah, we need all hands on
deck putting this release through its paces and testing it out. Please take
a moment to see if this release "works for you". If not, file a bug and
report here what your experience is.

https://bugzilla.redhat.com/enter_bug.cgi?product=GlusterFS


And if it does work for you, we want to know that as well.

Thanks,
JM
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130211/160c1288/attachment.html>

From whit.gluster at transpect.com  Mon Feb 11 17:39:59 2013
From: whit.gluster at transpect.com (Whit Blauvelt)
Date: Mon, 11 Feb 2013 12:39:59 -0500
Subject: [Gluster-users] GlusterFS 3.4 Alpha Release
In-Reply-To: <CAMUFnfaq73NqYxvoGsQR0hAba8omPbjpaDsu6=puitrH0Mcegw@mail.gmail.com>
References: <CAMUFnfaq73NqYxvoGsQR0hAba8omPbjpaDsu6=puitrH0Mcegw@mail.gmail.com>
Message-ID: <20130211173959.GA17686@black.transpect.com>

On Mon, Feb 11, 2013 at 12:18:38PM -0500, John Mark Walker wrote:
> Here's a write-up on the newly-released GlusterFS 3.4 alpha:
> 
> http://www.gluster.org/2013/02/new-release-glusterfs-3-4alpha/

If we want to test the QEMU stuff, is there a more thorough/current writeup
than Rao's blog post at
http://raobharata.wordpress.com/2012/10/29/qemu-glusterfs-native-integration/? 

Thanks,
Whit

From johnmark at redhat.com  Mon Feb 11 17:44:32 2013
From: johnmark at redhat.com (John Mark Walker)
Date: Mon, 11 Feb 2013 12:44:32 -0500 (EST)
Subject: [Gluster-users] GlusterFS 3.4 Alpha Release
In-Reply-To: <20130211173959.GA17686@black.transpect.com>
Message-ID: <1784670898.780971.1360604672724.JavaMail.root@redhat.com>

That is the most thorough writeup thus far. There are also some recorded demos on youtube:

http://www.youtube.com/watch?v=JG3kF_djclg

-JM


----- Original Message -----
> On Mon, Feb 11, 2013 at 12:18:38PM -0500, John Mark Walker wrote:
> > Here's a write-up on the newly-released GlusterFS 3.4 alpha:
> > 
> > http://www.gluster.org/2013/02/new-release-glusterfs-3-4alpha/
> 
> If we want to test the QEMU stuff, is there a more thorough/current
> writeup
> than Rao's blog post at
> http://raobharata.wordpress.com/2012/10/29/qemu-glusterfs-native-integration/?
> 
> Thanks,
> Whit
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 

From bfoster at redhat.com  Mon Feb 11 20:59:53 2013
From: bfoster at redhat.com (Brian Foster)
Date: Mon, 11 Feb 2013 15:59:53 -0500
Subject: [Gluster-users] GlusterFS OOM Issue
In-Reply-To: <511932BE.7020203@kingrst.com>
References: <511578E0.5020605@kingrst.com> <5116A2DA.7060908@redhat.com>
	<511932BE.7020203@kingrst.com>
Message-ID: <51195BC9.8090003@redhat.com>

On 02/11/2013 01:04 PM, Steven King wrote:
> Thanks Brian,
> 
> Our system is a Supermicro motherboard-X8 Series with a 4 drive hardware
> RAID 10. The CPU is an Intel Quad Core i7 X3440 @ 2.53GHz.
> 
> At the time the system had 4GB RAM and about 3GB swap. We have since
> upgraded the RAM to 16GB and swap is the same.
> 
> Current memory usage:
> PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
> 1488 root 20 0 8768m 8.5g 2032 R 2 53.9 181:15.04 glusterfs
> 

So your client is using 8.5GB of RAM..? I don't know your workload and I
don't have client footprint data on hand, but that seems seriously
excessive. Copying a couple 1GB files on a simple single brick client
has my client at ~55MB. Can you describe your workload?

To confirm one thing from your first post: you see a glusterfsd process
killed, but a client process is using a ton of memory. Are you using
this host as a server and client? Can you provide 'gluster volume info'
output?

> The process over time but quickly consumes the available memory on the
> system and then more slowly begins to eat up swap.
> 

Could you monitor that RES value quoted above for a period of time after
you mount the filesystem? This appears to be a fairly severe memory
leak. Note that 'free' might show memory being used rather quickly due
to page cache, but this memory is typically easily reclaimed by the
system for application use. IOW, the behavior of interest is the growth
of that glusterfs process over time.

> I've tried forcing the kernel to GC and reclaim cache, however the cache
> is only about a GB
> 

I'd suspect the cache starts out large if you are dealing with largish
files (Linux uses all available memory for page cache), particularly if
this host is a server and client, and then shrinks over time as the
glusterfs process grows.

I notice from your previous mail you are running an older version of
gluster (3.2.7). Perhaps somebody with more historical context can chime
in on whether there are known leaks with that version. The best solution
might be to upgrade to something more recent and see whether that leak
has already been addressed (I couldn't say off hand whether you can
connect a newer client or would have to upgrade the entire cluster).

Another approach might be to try and narrow down the offending
translator, provided it can be disabled. Could you post the gluster .vol
file for this particular volume?

Brian

> Current output of free -m before a cache reclaim:
> root at ifx05:~# free -m
> total used free shared buffers cached
> Mem: 16079 15865 213 0 374 1224
> -/+ buffers/cache: 14267 1812
> Swap: 3814 1 3813
> 
> Here is the output from OOM killer:
> ?
> Feb 8 08:05:36 ifx05 kernel: [679946.164642] glusterfsd invoked
> oom-killer: gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0
> Feb 8 08:05:36 ifx05 kernel: [679946.270816] glusterfsd cpuset=/
> mems_allowed=0
> Feb 8 08:05:36 ifx05 kernel: [679946.325069] Pid: 2070, comm: glusterfsd
> Not tainted 3.2.0-0.bpo.3-amd64 #1
> Feb 8 08:05:36 ifx05 kernel: [679946.408416] Call Trace:
> Feb 8 08:05:36 ifx05 kernel: [679946.438667] [<ffffffff810bf159>] ?
> dump_header+0x76/0x1a7
> Feb 8 08:05:36 ifx05 kernel: [679946.505292] [<ffffffff81173f34>] ?
> security_real_capable_noaudit+0x34/0x59
> Feb 8 08:05:36 ifx05 kernel: [679946.589592] [<ffffffff810bf088>] ?
> oom_unkillable_task+0x5f/0x92
> Feb 8 08:05:36 ifx05 kernel: [679946.663604] [<ffffffff810bf5af>] ?
> oom_kill_process+0x52/0x28d
> Feb 8 08:05:36 ifx05 kernel: [679946.735431] [<ffffffff810bfabb>] ?
> out_of_memory+0x2d1/0x337
> Feb 8 08:05:36 ifx05 kernel: [679946.805178] [<ffffffff810c3cbd>] ?
> __alloc_pages_nodemask+0x5d8/0x731
> Feb 8 08:05:48 ifx05 kernel: [679946.884294] [<ffffffff810ef944>] ?
> alloc_pages_current+0xa7/0xc9
> Feb 8 08:05:48 ifx05 kernel: [679946.958201] [<ffffffff810be8e1>] ?
> filemap_fault+0x26d/0x35c
> Feb 8 08:05:48 ifx05 kernel: [679947.027962] [<ffffffff810dad70>] ?
> __do_fault+0xc6/0x438
> Feb 8 08:05:48 ifx05 kernel: [679947.093631] [<ffffffff810dbf09>] ?
> handle_pte_fault+0x352/0x965
> Feb 8 08:05:48 ifx05 kernel: [679947.166503] [<ffffffff81120595>] ?
> getxattr+0xee/0x119
> Feb 8 08:05:48 ifx05 kernel: [679947.230005] [<ffffffff81120595>] ?
> getxattr+0xee/0x119
> Feb 8 08:05:48 ifx05 kernel: [679947.293519] [<ffffffff81368e2e>] ?
> do_page_fault+0x327/0x34c
> Feb 8 08:05:48 ifx05 kernel: [679947.363275] [<ffffffff81111e9e>] ?
> user_path_at_empty+0x55/0x7d
> Feb 8 08:05:48 ifx05 kernel: [679947.436234] [<ffffffff81109949>] ?
> sys_newlstat+0x24/0x2d
> Feb 8 08:05:48 ifx05 kernel: [679947.502876] [<ffffffff81117bb9>] ?
> dput+0x29/0xf2
> Feb 8 08:05:50 ifx05 kernel: [679947.561177] [<ffffffff81366235>] ?
> page_fault+0x25/0x30
> Feb 8 08:05:50 ifx05 kernel: [679947.625726] Mem-Info:
> Feb 8 08:05:50 ifx05 kernel: [679947.653900] Node 0 DMA per-cpu:
> Feb 8 08:05:50 ifx05 kernel: [679947.692559] CPU 0: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679947.750881] CPU 1: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679947.809187] CPU 2: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679947.867501] CPU 3: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679947.925813] CPU 4: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679947.984128] CPU 5: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.042445] CPU 6: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.100757] CPU 7: hi: 0, btch: 1 usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.159068] Node 0 DMA32 per-cpu:
> Feb 8 08:05:50 ifx05 kernel: [679948.199814] CPU 0: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.258130] CPU 1: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.316447] CPU 2: hi: 186, btch: 31
> usd: 30
> Feb 8 08:05:50 ifx05 kernel: [679948.374756] CPU 3: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.433069] CPU 4: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.491381] CPU 5: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.549695] CPU 6: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.608011] CPU 7: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.666323] Node 0 Normal per-cpu:
> Feb 8 08:05:50 ifx05 kernel: [679948.708109] CPU 0: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.766425] CPU 1: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.824739] CPU 2: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.883052] CPU 3: hi: 186, btch: 31
> usd: 59
> Feb 8 08:05:50 ifx05 kernel: [679948.941365] CPU 4: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679948.999678] CPU 5: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679949.057996] CPU 6: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679949.116306] CPU 7: hi: 186, btch: 31
> usd: 0
> Feb 8 08:05:50 ifx05 kernel: [679949.174626] active_anon:702601
> inactive_anon:260864 isolated_anon:55
> Feb 8 08:05:50 ifx05 kernel: [679949.174628] active_file:630
> inactive_file:833 isolated_file:86
> Feb 8 08:05:50 ifx05 kernel: [679949.174629] unevictable:0 dirty:0
> writeback:0 unstable:0
> Feb 8 08:05:50 ifx05 kernel: [679949.174630] free:21828
> slab_reclaimable:5760 slab_unreclaimable:5863
> Feb 8 08:05:50 ifx05 kernel: [679949.174631] mapped:378 shmem:163
> pagetables:5053 bounce:0
> Feb 8 08:05:50 ifx05 kernel: [679949.533783] Node 0 DMA free:15904kB
> min:256kB low:320kB high:384kB active_anon:0kB inactive_anon:0kB
> active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB
> isolated(file):0kB present:15680kB mlocked:0kB dirty:0kB writeback:0kB
> mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB
> kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB
> writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes
> Feb 8 08:05:50 ifx05 kernel: [679949.974901] lowmem_reserve[]: 0 2991
> 4001 4001
> Feb 8 08:05:50 ifx05 kernel: [679950.029570] Node 0 DMA32 free:54652kB
> min:50332kB low:62912kB high:75496kB active_anon:2356292kB
> inactive_anon:589348kB active_file:1656kB inactive_file:2088kB
> unevictable:0kB isolated(anon):128kB isolated(file):0kB
> present:3063584kB mlocked:0kB dirty:0kB writeback:0kB mapped:1136kB
> shmem:648kB slab_reclaimable:14680kB slab_unreclaimable:5804kB
> kernel_stack:776kB pagetables:11180kB unstable:0kB bounce:0kB
> writeback_tmp:0kB pages_scanned:0 all_unreclaimable? no
> Feb 8 08:05:50 ifx05 kernel: [679950.518329] lowmem_reserve[]: 0 0 1010
> 1010
> Feb 8 08:05:50 ifx05 kernel: [679950.569883] Node 0 Normal free:16616kB
> min:16992kB low:21240kB high:25488kB active_anon:453668kB
> inactive_anon:454080kB active_file:676kB inactive_file:392kB
> unevictable:0kB isolated(anon):220kB isolated(file):128kB
> present:1034240kB mlocked:0kB dirty:0kB writeback:0kB mapped:508kB
> shmem:4kB slab_reclaimable:8360kB slab_unreclaimable:17648kB
> kernel_stack:1632kB pagetables:9032kB unstable:0kB bounce:0kB
> writeback_tmp:0kB pages_scanned:3 all_unreclaimable? no
> Feb 8 08:05:50 ifx05 kernel: [679951.055531] lowmem_reserve[]: 0 0 0 0
> Feb 8 08:05:50 ifx05 kernel: [679951.100836] Node 0 DMA: 0*4kB 0*8kB
> 0*16kB 1*32kB 2*64kB 1*128kB 1*256kB 0*512kB 1*1024kB 1*2048kB 3*4096kB
> = 15904kB
> Feb 8 08:05:50 ifx05 kernel: [679951.230148] Node 0 DMA32: 12135*4kB
> 0*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 1*2048kB
> 1*4096kB = 54684kB
> Feb 8 08:05:50 ifx05 kernel: [679951.365691] Node 0 Normal: 843*4kB
> 502*8kB 173*16kB 55*32kB 9*64kB 1*128kB 0*256kB 0*512kB 0*1024kB
> 0*2048kB 1*4096kB = 16716kB
> Feb 8 08:05:50 ifx05 kernel: [679951.505398] 94663 total pagecache pages
> Feb 8 08:05:50 ifx05 kernel: [679951.552277] 92078 pages in swap cache
> Feb 8 08:05:50 ifx05 kernel: [679951.597096] Swap cache stats: add
> 6081494, delete 5990029, find 1251231/1858053
> Feb 8 08:05:50 ifx05 kernel: [679951.685541] Free swap = 0kB
> Feb 8 08:05:50 ifx05 kernel: [679951.720980] Total swap = 3906556kB
> Feb 8 08:05:50 ifx05 kernel: [679951.775776] 1048560 pages RAM
> Feb 8 08:05:50 ifx05 kernel: [679951.812253] 35003 pages reserved
> Feb 8 08:05:50 ifx05 kernel: [679951.851847] 10245 pages shared
> Feb 8 08:05:50 ifx05 kernel: [679951.889380] 982341 pages non-shared
> Feb 8 08:05:50 ifx05 kernel: [679951.932102] [ pid ] uid tgid total_vm
> rss cpu oom_adj oom_score_adj name
> Feb 8 08:05:50 ifx05 kernel: [679952.021615] [ 329] 0 329 4263 1 0 -17
> -1000 udevd
> Feb 8 08:05:50 ifx05 kernel: [679952.021619] [ 826] 1 826 2036 21 1 0 0
> portmap
> Feb 8 08:05:50 ifx05 kernel: [679952.021622] [ 906] 102 906 3608 2 1 0 0
> rpc.statd
> Feb 8 08:05:50 ifx05 kernel: [679952.021625] [ 1051] 0 1051 6768 0 7 0 0
> rpc.idmapd
> Feb 8 08:05:50 ifx05 kernel: [679952.021628] [ 1212] 0 1212 992 1 2 0 0
> acpid
> Feb 8 08:05:50 ifx05 kernel: [679952.021631] [ 1221] 0 1221 4691 1 2 0 0
> atd
> Feb 8 08:05:50 ifx05 kernel: [679952.021634] [ 1253] 0 1253 5619 20 3 0
> 0 cron
> Feb 8 08:05:50 ifx05 kernel: [679952.021637] [ 1438] 0 1438 12307 29 3
> -17 -1000 sshd
> Feb 8 08:05:50 ifx05 kernel: [679952.021640] [ 1450] 0 1450 9304 29 3 0
> 0 master
> Feb 8 08:05:50 ifx05 kernel: [679952.021642] [ 1457] 107 1457 9860 37 2
> 0 0 qmgr
> Feb 8 08:05:50 ifx05 kernel: [679952.021645] [ 1491] 0 1491 54480 636 3
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021648] [ 1495] 0 1495 70879 1081 6
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021651] [ 1499] 0 1499 37250 25 4 0
> 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021653] [ 1503] 0 1503 70880 934 4
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021656] [ 1507] 0 1507 111360 1936
> 5 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021659] [ 1511] 0 1511 87515 1429 4
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021661] [ 1515] 0 1515 70617 489 0
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021664] [ 1519] 0 1519 70901 956 7
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021667] [ 1523] 0 1523 88380 2041 2
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021670] [ 1527] 0 1527 54818 1421 7
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021673] [ 1531] 0 1531 37250 27 4 0
> 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021675] [ 1535] 0 1535 54224 315 1
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021678] [ 1539] 0 1539 37250 0 4 0
> 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021681] [ 1543] 0 1543 72001 1561 2
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021684] [ 1547] 0 1547 87954 2595 5
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021687] [ 1551] 0 1551 37209 11 4 0
> 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021689] [ 1559] 0 1559 71410 1301 5
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021692] [ 1563] 0 1563 121841 3270
> 2 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021694] [ 1567] 0 1567 70925 1217 5
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021697] [ 1571] 0 1571 54241 1169 2
> 0 0 glusterfsd
> Feb 8 08:05:50 ifx05 kernel: [679952.021699] [ 1635] 105 1635 9597 32 3
> 0 0 ntpd
> Feb 8 08:05:50 ifx05 kernel: [679952.021702] [ 1670] 0 1670 1495 1 0 0 0
> getty
> Feb 8 08:05:50 ifx05 kernel: [679952.021705] [ 2659] 107 2659 10454 47 2
> 0 0 tlsmgr
> Feb 8 08:05:50 ifx05 kernel: [679952.021708] [ 6691] 0 6691 47094 44 1 0
> 0 glusterd
> Feb 8 08:05:50 ifx05 kernel: [679952.021711] [ 7449] 0 7449 10818 5 1 0
> 0 syslog-ng
> Feb 8 08:05:50 ifx05 kernel: [679952.021713] [ 7450] 0 7450 12452 157 1
> 0 0 syslog-ng
> Feb 8 08:05:50 ifx05 kernel: [679952.021716] [ 9475] 108 9475 12591 8 5
> 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021719] [ 9476] 108 9476 12591 241
> 7 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021722] [ 9477] 108 9477 12591 24 3
> 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021725] [ 9478] 108 9478 12591 24 4
> 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021728] [ 9479] 108 9479 12591 24 0
> 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021731] [ 9480] 108 9480 12591 24 4
> 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021734] [ 9481] 108 9481 12591 24 1
> 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021737] [ 9482] 108 9482 12591 112
> 4 0 0 zabbix_agentd
> Feb 8 08:05:50 ifx05 kernel: [679952.021740] [ 9600] 106 9600 11810 300
> 0 0 0 snmpd
> Feb 8 08:05:50 ifx05 kernel: [679952.021743] [10556] 0 10556 57500 82 4
> 0 0 glusterfs
> Feb 8 08:05:50 ifx05 kernel: [679952.021746] [31815] 0 31815 27485 245 4
> 0 0 ruby
> Feb 8 08:05:50 ifx05 kernel: [679952.021749] [ 2842] 0 2842 4262 1 3 -17
> -1000 udevd
> Feb 8 08:05:50 ifx05 kernel: [679952.021754] [30513] 0 30513 1724628
> 814692 5 0 0 glusterfs
> Feb 8 08:05:50 ifx05 kernel: [679952.021757] [19809] 107 19809 9820 89 2
> 0 0 pickup
> Feb 8 08:05:50 ifx05 kernel: [679952.021759] [20590] 0 20590 8214 61 2 0
> 0 cron
> Feb 8 08:05:50 ifx05 kernel: [679952.021762] [20591] 0 20591 1001 24 2 0
> 0 sh
> Feb 8 08:05:50 ifx05 kernel: [679952.021765] [20592] 0 20592 40966 18786
> 3 0 0 puppet
> Feb 8 08:05:50 ifx05 kernel: [679952.021767] [20861] 0 20861 41134 19137
> 5 0 0 puppet
> Feb 8 08:05:50 ifx05 kernel: [679952.021770] Out of memory: Kill process
> 30513 (glusterfs) score 816 or sacrifice child
> Feb 8 08:05:50 ifx05 kernel: [679952.021772] Killed process 30513
> (glusterfs) total-vm:6898512kB, anon-rss:3257908kB, file-rss:860kB
> 
> On 2/9/13 2:26 PM, Brian Foster wrote:
>> On 02/08/2013 05:14 PM, Steven King wrote:
>>> Hello,
>>>
>>> I am running GlusterFS version 3.2.7-2~bpo60+1 on Debian 6.0.6. Today, I
>>> have experienced a a glusterfs process cause the server to invoke
>>> oom_killer.
>>>
>>> How exactly would I go about investigating this and coming up with a
>>> fix?
>>>
>> The OOM killer output to syslog and details on your hardware might be
>> useful to include.
>>
>> Following that, you could monitor the address space (VIRT) and set size
>> (RES/RSS) of the relevant processes with top on your server. For
>> example, is there a sudden increase in set size or does it constantly,
>> gradually increase?
>>
>> Brian
> 


From jayunit100 at gmail.com  Tue Feb 12 14:20:50 2013
From: jayunit100 at gmail.com (jayunit100 at gmail.com)
Date: Tue, 12 Feb 2013 09:20:50 -0500
Subject: [Gluster-users] FUSE and consistent/sync writes
Message-ID: <38061DC3-4746-415D-9CCC-9C0C8639956B@gmail.com>

Hi guys:

The gluster Fuse mount strategy is convenient, but doesn't gaurantee consistency for writes--- but the mount "sync" seems not to work.  how can we force fuse mounted gluster to do synchronous writes for certain operations?

In general: what are the knobs to turn / buttons to push to exchange consistency vs availability?

Jay Vyas 
http://jayunit100.blogspot.com

From bcipriano at zerovfx.com  Tue Feb 12 15:08:32 2013
From: bcipriano at zerovfx.com (Brian Cipriano)
Date: Tue, 12 Feb 2013 10:08:32 -0500
Subject: [Gluster-users] upgrading 3.3.0 to 3.3.1 on Ubuntu
Message-ID: <511A5AF0.7080202@zerovfx.com>

Hi all -

Has anyone had any success upgrading from 3.3.0 to 3.3.1 on Ubuntu?

It appears the PPA installs its files in a different location than the 
3.3.0 .deb, and I haven't been able to do the upgrade without wiping out 
my existing volume data.

Thanks,

- brian

From bharata.rao at gmail.com  Tue Feb 12 15:09:27 2013
From: bharata.rao at gmail.com (Bharata B Rao)
Date: Tue, 12 Feb 2013 20:39:27 +0530
Subject: [Gluster-users] GlusterFS 3.4 Alpha Release
In-Reply-To: <20130211173959.GA17686@black.transpect.com>
References: <CAMUFnfaq73NqYxvoGsQR0hAba8omPbjpaDsu6=puitrH0Mcegw@mail.gmail.com>
	<20130211173959.GA17686@black.transpect.com>
Message-ID: <CAGZKiBo9cUZwPGELSuoCO2=aZW90K8bujguB2vg3gn49WC8KBQ@mail.gmail.com>

On Mon, Feb 11, 2013 at 11:09 PM, Whit Blauvelt
<whit.gluster at transpect.com>wrote:

> On Mon, Feb 11, 2013 at 12:18:38PM -0500, John Mark Walker wrote:
> > Here's a write-up on the newly-released GlusterFS 3.4 alpha:
> >
> > http://www.gluster.org/2013/02/new-release-glusterfs-3-4alpha/
>
> If we want to test the QEMU stuff, is there a more thorough/current writeup
> than Rao's blog post at
>
> http://raobharata.wordpress.com/2012/10/29/qemu-glusterfs-native-integration/
> ?
>

I keep that blog post up-to-date and it reflects what is currently
available with GlusterFS-3.4 alpha and QEMU starting from QEMU-1.3.

Regards,
Bharata.
-- 
http://raobharata.wordpress.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130212/dfdede4f/attachment.html>

From johnmark at redhat.com  Tue Feb 12 15:30:30 2013
From: johnmark at redhat.com (John Mark Walker)
Date: Tue, 12 Feb 2013 10:30:30 -0500 (EST)
Subject: [Gluster-users] [Gluster-devel]  GlusterFS 3.4 Alpha Release
In-Reply-To: <CAGZKiBo9cUZwPGELSuoCO2=aZW90K8bujguB2vg3gn49WC8KBQ@mail.gmail.com>
Message-ID: <131987815.1455322.1360683030088.JavaMail.root@redhat.com>

----- Original Message -----

> On Mon, Feb 11, 2013 at 11:09 PM, Whit Blauvelt <
> whit.gluster at transpect.com > wrote:

> > On Mon, Feb 11, 2013 at 12:18:38PM -0500, John Mark Walker wrote:
> 
> > > Here's a write-up on the newly-released GlusterFS 3.4 alpha:
> 
> > >
> 
> > > http://www.gluster.org/2013/02/new-release-glusterfs-3-4alpha/
> 

> > If we want to test the QEMU stuff, is there a more thorough/current
> > writeup
> 
> > than Rao's blog post at
> 
> > http://raobharata.wordpress.com/2012/10/29/qemu-glusterfs-native-integration/
> > ?
> 

> I keep that blog post up-to-date and it reflects what is currently
> available with GlusterFS-3.4 alpha and QEMU starting from QEMU-1.3.

Excellent - good to know! We should also try and come up with some basic docs on how to use the new features. 

-JM 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130212/ce20b1e2/attachment.html>

From samu60 at gmail.com  Tue Feb 12 18:19:43 2013
From: samu60 at gmail.com (samuel)
Date: Tue, 12 Feb 2013 19:19:43 +0100
Subject: [Gluster-users] server3_1-fops.c:1240 (Cannot allocate memory)
Message-ID: <CAOg=WDd-sr68X7mcBLdVVYG+xrWVtTJL-SuaCr0JOB=ENUp=Gg@mail.gmail.com>

Hi folks,

We're using in an old environment gluster 3.2.2 and we've just started
seeing the following error in /usr/local/var/log/glusterfs/bricks/gfs-log:

[2013-02-12 19:15:05.214430] I [server3_1-fops.c:1240:server_writev_cbk]
0-cloud-server: 2400444243: WRITEV 7 (37067) ==> -1 (Cannot allocate memory)
[2013-02-12 19:15:19.463087] I [server3_1-fops.c:1240:server_writev_cbk]
0-cloud-server: 2249406582: WRITEV 15 (52493) ==> -1 (Cannot allocate
memory)

in a distributed replicated 2-node environment.

Both nodes have enough disk space (1.3T) but looks like used memory is
quite high:

node1:
             total       used       free     shared    buffers     cached
Mem:       4047680    4012700      34980          0        276    3728652
-/+ buffers/cache:     283772    3763908
Swap:      3906244      19916    3886328

node2:
             total       used       free     shared    buffers     cached
Mem:       4047680    4012488      35192          0       1088    3713512
-/+ buffers/cache:     297888    3749792
Swap:      3905532      25244    3880288

could it be a memory issue?

Best regards,
Samuel.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130212/3e850c0e/attachment.html>

From soumplis at ekt.gr  Wed Feb 13 12:22:15 2013
From: soumplis at ekt.gr (Alexandros Soumplis)
Date: Wed, 13 Feb 2013 14:22:15 +0200
Subject: [Gluster-users] Import data from brick
Message-ID: <511B8577.8060305@ekt.gr>

Hello,

Is it possible to "import" data already existent in a gluster brick?

a.


From B.Candler at pobox.com  Wed Feb 13 12:27:46 2013
From: B.Candler at pobox.com (Brian Candler)
Date: Wed, 13 Feb 2013 12:27:46 +0000
Subject: [Gluster-users] server3_1-fops.c:1240 (Cannot allocate memory)
In-Reply-To: <CAOg=WDd-sr68X7mcBLdVVYG+xrWVtTJL-SuaCr0JOB=ENUp=Gg@mail.gmail.com>
References: <CAOg=WDd-sr68X7mcBLdVVYG+xrWVtTJL-SuaCr0JOB=ENUp=Gg@mail.gmail.com>
Message-ID: <20130213122746.GA7267@nsrc.org>

On Tue, Feb 12, 2013 at 07:19:43PM +0100, samuel wrote:
>    Both nodes have enough disk space (1.3T) but looks like used memory is
>    quite high:

Not true.

>    node1:
>                 total       used       free     shared    buffers    cached
>    Mem:       4047680    4012700      34980          0        276    3728652
>    -/+ buffers/cache:     283772    3763908

It shows:

* total memory is 4GB

* any spare memory is used to cache disk blocks. In this case it's 3.7GB.

* the second line shows you the memory usage after disk buffers and cache
  are ignored. It shows you are only using 283M and have 3.7GB free.

Ditto for the second box.

From bcipriano at zerovfx.com  Wed Feb 13 15:14:30 2013
From: bcipriano at zerovfx.com (Brian Cipriano)
Date: Wed, 13 Feb 2013 10:14:30 -0500
Subject: [Gluster-users] Import data from brick
In-Reply-To: <511B8577.8060305@ekt.gr>
References: <511B8577.8060305@ekt.gr>
Message-ID: <511BADD6.5010907@zerovfx.com>

Hi -

I was just looking at this yesterday. I've done some basic testing and 
this seems to be possible.

I think you're saying, you'd like to add a brick that was already 
previously part of a gluster? Without losing the data that was on it, 
and importing that data into the new gluster?

All you should have to do is remove the .glusterfs folder on that brick. 
Doc on that here:

http://joejulian.name/blog/glusterfs-path-or-a-prefix-of-it-is-already-part-of-a-volume/

Then running add-brick as normal. It will take a few minutes depending 
on how much data is on the brick, but gluster should see that data and 
rebuild its metadata accordingly.

Again, I've only done BASIC testing, don't take my word for it.

- brian

On 2/13/13 7:22 AM, Alexandros Soumplis wrote:
> Hello,
>
> Is it possible to "import" data already existent in a gluster brick?
>
> a.
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From sking at kingrst.com  Mon Feb 11 18:04:46 2013
From: sking at kingrst.com (Steven King)
Date: Mon, 11 Feb 2013 13:04:46 -0500
Subject: [Gluster-users] GlusterFS OOM Issue
In-Reply-To: <5116A2DA.7060908@redhat.com>
References: <511578E0.5020605@kingrst.com> <5116A2DA.7060908@redhat.com>
Message-ID: <511932BE.7020203@kingrst.com>

Thanks Brian,

Our system is a Supermicro motherboard-X8 Series with a 4 drive hardware 
RAID 10. The CPU is an Intel Quad Core i7 X3440 @ 2.53GHz.

At the time the system had 4GB RAM and about 3GB swap. We have since 
upgraded the RAM to 16GB and swap is the same.

Current memory usage:
PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
1488 root 20 0 8768m 8.5g 2032 R 2 53.9 181:15.04 glusterfs

The process over time but quickly consumes the available memory on the 
system and then more slowly begins to eat up swap.

I've tried forcing the kernel to GC and reclaim cache, however the cache 
is only about a GB

Current output of free -m before a cache reclaim:
root at ifx05:~# free -m
total used free shared buffers cached
Mem: 16079 15865 213 0 374 1224
-/+ buffers/cache: 14267 1812
Swap: 3814 1 3813

Here is the output from OOM killer:
?
Feb 8 08:05:36 ifx05 kernel: [679946.164642] glusterfsd invoked 
oom-killer: gfp_mask=0x201da, order=0, oom_adj=0, oom_score_adj=0
Feb 8 08:05:36 ifx05 kernel: [679946.270816] glusterfsd cpuset=/ 
mems_allowed=0
Feb 8 08:05:36 ifx05 kernel: [679946.325069] Pid: 2070, comm: glusterfsd 
Not tainted 3.2.0-0.bpo.3-amd64 #1
Feb 8 08:05:36 ifx05 kernel: [679946.408416] Call Trace:
Feb 8 08:05:36 ifx05 kernel: [679946.438667] [<ffffffff810bf159>] ? 
dump_header+0x76/0x1a7
Feb 8 08:05:36 ifx05 kernel: [679946.505292] [<ffffffff81173f34>] ? 
security_real_capable_noaudit+0x34/0x59
Feb 8 08:05:36 ifx05 kernel: [679946.589592] [<ffffffff810bf088>] ? 
oom_unkillable_task+0x5f/0x92
Feb 8 08:05:36 ifx05 kernel: [679946.663604] [<ffffffff810bf5af>] ? 
oom_kill_process+0x52/0x28d
Feb 8 08:05:36 ifx05 kernel: [679946.735431] [<ffffffff810bfabb>] ? 
out_of_memory+0x2d1/0x337
Feb 8 08:05:36 ifx05 kernel: [679946.805178] [<ffffffff810c3cbd>] ? 
__alloc_pages_nodemask+0x5d8/0x731
Feb 8 08:05:48 ifx05 kernel: [679946.884294] [<ffffffff810ef944>] ? 
alloc_pages_current+0xa7/0xc9
Feb 8 08:05:48 ifx05 kernel: [679946.958201] [<ffffffff810be8e1>] ? 
filemap_fault+0x26d/0x35c
Feb 8 08:05:48 ifx05 kernel: [679947.027962] [<ffffffff810dad70>] ? 
__do_fault+0xc6/0x438
Feb 8 08:05:48 ifx05 kernel: [679947.093631] [<ffffffff810dbf09>] ? 
handle_pte_fault+0x352/0x965
Feb 8 08:05:48 ifx05 kernel: [679947.166503] [<ffffffff81120595>] ? 
getxattr+0xee/0x119
Feb 8 08:05:48 ifx05 kernel: [679947.230005] [<ffffffff81120595>] ? 
getxattr+0xee/0x119
Feb 8 08:05:48 ifx05 kernel: [679947.293519] [<ffffffff81368e2e>] ? 
do_page_fault+0x327/0x34c
Feb 8 08:05:48 ifx05 kernel: [679947.363275] [<ffffffff81111e9e>] ? 
user_path_at_empty+0x55/0x7d
Feb 8 08:05:48 ifx05 kernel: [679947.436234] [<ffffffff81109949>] ? 
sys_newlstat+0x24/0x2d
Feb 8 08:05:48 ifx05 kernel: [679947.502876] [<ffffffff81117bb9>] ? 
dput+0x29/0xf2
Feb 8 08:05:50 ifx05 kernel: [679947.561177] [<ffffffff81366235>] ? 
page_fault+0x25/0x30
Feb 8 08:05:50 ifx05 kernel: [679947.625726] Mem-Info:
Feb 8 08:05:50 ifx05 kernel: [679947.653900] Node 0 DMA per-cpu:
Feb 8 08:05:50 ifx05 kernel: [679947.692559] CPU 0: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679947.750881] CPU 1: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679947.809187] CPU 2: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679947.867501] CPU 3: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679947.925813] CPU 4: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679947.984128] CPU 5: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.042445] CPU 6: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.100757] CPU 7: hi: 0, btch: 1 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.159068] Node 0 DMA32 per-cpu:
Feb 8 08:05:50 ifx05 kernel: [679948.199814] CPU 0: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.258130] CPU 1: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.316447] CPU 2: hi: 186, btch: 31 
usd: 30
Feb 8 08:05:50 ifx05 kernel: [679948.374756] CPU 3: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.433069] CPU 4: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.491381] CPU 5: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.549695] CPU 6: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.608011] CPU 7: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.666323] Node 0 Normal per-cpu:
Feb 8 08:05:50 ifx05 kernel: [679948.708109] CPU 0: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.766425] CPU 1: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.824739] CPU 2: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.883052] CPU 3: hi: 186, btch: 31 
usd: 59
Feb 8 08:05:50 ifx05 kernel: [679948.941365] CPU 4: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679948.999678] CPU 5: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679949.057996] CPU 6: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679949.116306] CPU 7: hi: 186, btch: 31 usd: 0
Feb 8 08:05:50 ifx05 kernel: [679949.174626] active_anon:702601 
inactive_anon:260864 isolated_anon:55
Feb 8 08:05:50 ifx05 kernel: [679949.174628] active_file:630 
inactive_file:833 isolated_file:86
Feb 8 08:05:50 ifx05 kernel: [679949.174629] unevictable:0 dirty:0 
writeback:0 unstable:0
Feb 8 08:05:50 ifx05 kernel: [679949.174630] free:21828 
slab_reclaimable:5760 slab_unreclaimable:5863
Feb 8 08:05:50 ifx05 kernel: [679949.174631] mapped:378 shmem:163 
pagetables:5053 bounce:0
Feb 8 08:05:50 ifx05 kernel: [679949.533783] Node 0 DMA free:15904kB 
min:256kB low:320kB high:384kB active_anon:0kB inactive_anon:0kB 
active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB 
isolated(file):0kB present:15680kB mlocked:0kB dirty:0kB writeback:0kB 
mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB 
kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB 
writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes
Feb 8 08:05:50 ifx05 kernel: [679949.974901] lowmem_reserve[]: 0 2991 
4001 4001
Feb 8 08:05:50 ifx05 kernel: [679950.029570] Node 0 DMA32 free:54652kB 
min:50332kB low:62912kB high:75496kB active_anon:2356292kB 
inactive_anon:589348kB active_file:1656kB inactive_file:2088kB 
unevictable:0kB isolated(anon):128kB isolated(file):0kB 
present:3063584kB mlocked:0kB dirty:0kB writeback:0kB mapped:1136kB 
shmem:648kB slab_reclaimable:14680kB slab_unreclaimable:5804kB 
kernel_stack:776kB pagetables:11180kB unstable:0kB bounce:0kB 
writeback_tmp:0kB pages_scanned:0 all_unreclaimable? no
Feb 8 08:05:50 ifx05 kernel: [679950.518329] lowmem_reserve[]: 0 0 1010 1010
Feb 8 08:05:50 ifx05 kernel: [679950.569883] Node 0 Normal free:16616kB 
min:16992kB low:21240kB high:25488kB active_anon:453668kB 
inactive_anon:454080kB active_file:676kB inactive_file:392kB 
unevictable:0kB isolated(anon):220kB isolated(file):128kB 
present:1034240kB mlocked:0kB dirty:0kB writeback:0kB mapped:508kB 
shmem:4kB slab_reclaimable:8360kB slab_unreclaimable:17648kB 
kernel_stack:1632kB pagetables:9032kB unstable:0kB bounce:0kB 
writeback_tmp:0kB pages_scanned:3 all_unreclaimable? no
Feb 8 08:05:50 ifx05 kernel: [679951.055531] lowmem_reserve[]: 0 0 0 0
Feb 8 08:05:50 ifx05 kernel: [679951.100836] Node 0 DMA: 0*4kB 0*8kB 
0*16kB 1*32kB 2*64kB 1*128kB 1*256kB 0*512kB 1*1024kB 1*2048kB 3*4096kB 
= 15904kB
Feb 8 08:05:50 ifx05 kernel: [679951.230148] Node 0 DMA32: 12135*4kB 
0*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 1*2048kB 
1*4096kB = 54684kB
Feb 8 08:05:50 ifx05 kernel: [679951.365691] Node 0 Normal: 843*4kB 
502*8kB 173*16kB 55*32kB 9*64kB 1*128kB 0*256kB 0*512kB 0*1024kB 
0*2048kB 1*4096kB = 16716kB
Feb 8 08:05:50 ifx05 kernel: [679951.505398] 94663 total pagecache pages
Feb 8 08:05:50 ifx05 kernel: [679951.552277] 92078 pages in swap cache
Feb 8 08:05:50 ifx05 kernel: [679951.597096] Swap cache stats: add 
6081494, delete 5990029, find 1251231/1858053
Feb 8 08:05:50 ifx05 kernel: [679951.685541] Free swap = 0kB
Feb 8 08:05:50 ifx05 kernel: [679951.720980] Total swap = 3906556kB
Feb 8 08:05:50 ifx05 kernel: [679951.775776] 1048560 pages RAM
Feb 8 08:05:50 ifx05 kernel: [679951.812253] 35003 pages reserved
Feb 8 08:05:50 ifx05 kernel: [679951.851847] 10245 pages shared
Feb 8 08:05:50 ifx05 kernel: [679951.889380] 982341 pages non-shared
Feb 8 08:05:50 ifx05 kernel: [679951.932102] [ pid ] uid tgid total_vm 
rss cpu oom_adj oom_score_adj name
Feb 8 08:05:50 ifx05 kernel: [679952.021615] [ 329] 0 329 4263 1 0 -17 
-1000 udevd
Feb 8 08:05:50 ifx05 kernel: [679952.021619] [ 826] 1 826 2036 21 1 0 0 
portmap
Feb 8 08:05:50 ifx05 kernel: [679952.021622] [ 906] 102 906 3608 2 1 0 0 
rpc.statd
Feb 8 08:05:50 ifx05 kernel: [679952.021625] [ 1051] 0 1051 6768 0 7 0 0 
rpc.idmapd
Feb 8 08:05:50 ifx05 kernel: [679952.021628] [ 1212] 0 1212 992 1 2 0 0 
acpid
Feb 8 08:05:50 ifx05 kernel: [679952.021631] [ 1221] 0 1221 4691 1 2 0 0 atd
Feb 8 08:05:50 ifx05 kernel: [679952.021634] [ 1253] 0 1253 5619 20 3 0 
0 cron
Feb 8 08:05:50 ifx05 kernel: [679952.021637] [ 1438] 0 1438 12307 29 3 
-17 -1000 sshd
Feb 8 08:05:50 ifx05 kernel: [679952.021640] [ 1450] 0 1450 9304 29 3 0 
0 master
Feb 8 08:05:50 ifx05 kernel: [679952.021642] [ 1457] 107 1457 9860 37 2 
0 0 qmgr
Feb 8 08:05:50 ifx05 kernel: [679952.021645] [ 1491] 0 1491 54480 636 3 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021648] [ 1495] 0 1495 70879 1081 6 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021651] [ 1499] 0 1499 37250 25 4 0 
0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021653] [ 1503] 0 1503 70880 934 4 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021656] [ 1507] 0 1507 111360 1936 
5 0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021659] [ 1511] 0 1511 87515 1429 4 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021661] [ 1515] 0 1515 70617 489 0 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021664] [ 1519] 0 1519 70901 956 7 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021667] [ 1523] 0 1523 88380 2041 2 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021670] [ 1527] 0 1527 54818 1421 7 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021673] [ 1531] 0 1531 37250 27 4 0 
0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021675] [ 1535] 0 1535 54224 315 1 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021678] [ 1539] 0 1539 37250 0 4 0 
0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021681] [ 1543] 0 1543 72001 1561 2 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021684] [ 1547] 0 1547 87954 2595 5 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021687] [ 1551] 0 1551 37209 11 4 0 
0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021689] [ 1559] 0 1559 71410 1301 5 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021692] [ 1563] 0 1563 121841 3270 
2 0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021694] [ 1567] 0 1567 70925 1217 5 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021697] [ 1571] 0 1571 54241 1169 2 
0 0 glusterfsd
Feb 8 08:05:50 ifx05 kernel: [679952.021699] [ 1635] 105 1635 9597 32 3 
0 0 ntpd
Feb 8 08:05:50 ifx05 kernel: [679952.021702] [ 1670] 0 1670 1495 1 0 0 0 
getty
Feb 8 08:05:50 ifx05 kernel: [679952.021705] [ 2659] 107 2659 10454 47 2 
0 0 tlsmgr
Feb 8 08:05:50 ifx05 kernel: [679952.021708] [ 6691] 0 6691 47094 44 1 0 
0 glusterd
Feb 8 08:05:50 ifx05 kernel: [679952.021711] [ 7449] 0 7449 10818 5 1 0 
0 syslog-ng
Feb 8 08:05:50 ifx05 kernel: [679952.021713] [ 7450] 0 7450 12452 157 1 
0 0 syslog-ng
Feb 8 08:05:50 ifx05 kernel: [679952.021716] [ 9475] 108 9475 12591 8 5 
0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021719] [ 9476] 108 9476 12591 241 
7 0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021722] [ 9477] 108 9477 12591 24 3 
0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021725] [ 9478] 108 9478 12591 24 4 
0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021728] [ 9479] 108 9479 12591 24 0 
0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021731] [ 9480] 108 9480 12591 24 4 
0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021734] [ 9481] 108 9481 12591 24 1 
0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021737] [ 9482] 108 9482 12591 112 
4 0 0 zabbix_agentd
Feb 8 08:05:50 ifx05 kernel: [679952.021740] [ 9600] 106 9600 11810 300 
0 0 0 snmpd
Feb 8 08:05:50 ifx05 kernel: [679952.021743] [10556] 0 10556 57500 82 4 
0 0 glusterfs
Feb 8 08:05:50 ifx05 kernel: [679952.021746] [31815] 0 31815 27485 245 4 
0 0 ruby
Feb 8 08:05:50 ifx05 kernel: [679952.021749] [ 2842] 0 2842 4262 1 3 -17 
-1000 udevd
Feb 8 08:05:50 ifx05 kernel: [679952.021754] [30513] 0 30513 1724628 
814692 5 0 0 glusterfs
Feb 8 08:05:50 ifx05 kernel: [679952.021757] [19809] 107 19809 9820 89 2 
0 0 pickup
Feb 8 08:05:50 ifx05 kernel: [679952.021759] [20590] 0 20590 8214 61 2 0 
0 cron
Feb 8 08:05:50 ifx05 kernel: [679952.021762] [20591] 0 20591 1001 24 2 0 
0 sh
Feb 8 08:05:50 ifx05 kernel: [679952.021765] [20592] 0 20592 40966 18786 
3 0 0 puppet
Feb 8 08:05:50 ifx05 kernel: [679952.021767] [20861] 0 20861 41134 19137 
5 0 0 puppet
Feb 8 08:05:50 ifx05 kernel: [679952.021770] Out of memory: Kill process 
30513 (glusterfs) score 816 or sacrifice child
Feb 8 08:05:50 ifx05 kernel: [679952.021772] Killed process 30513 
(glusterfs) total-vm:6898512kB, anon-rss:3257908kB, file-rss:860kB

On 2/9/13 2:26 PM, Brian Foster wrote:
> On 02/08/2013 05:14 PM, Steven King wrote:
>> Hello,
>>
>> I am running GlusterFS version 3.2.7-2~bpo60+1 on Debian 6.0.6. Today, I
>> have experienced a a glusterfs process cause the server to invoke
>> oom_killer.
>>
>> How exactly would I go about investigating this and coming up with a fix?
>>
> The OOM killer output to syslog and details on your hardware might be
> useful to include.
>
> Following that, you could monitor the address space (VIRT) and set size
> (RES/RSS) of the relevant processes with top on your server. For
> example, is there a sudden increase in set size or does it constantly,
> gradually increase?
>
> Brian

-- 
Steve King

Network/Linux Engineer - AdSafe Media
Cisco Certified Network Professional
CompTIA Linux+ Certified Professional
CompTIA A+ Certified Professional


From soumplis at ekt.gr  Wed Feb 13 16:01:54 2013
From: soumplis at ekt.gr (Alexandros Soumplis)
Date: Wed, 13 Feb 2013 18:01:54 +0200
Subject: [Gluster-users] Import data from brick
In-Reply-To: <511BADD6.5010907@zerovfx.com>
References: <511B8577.8060305@ekt.gr> <511BADD6.5010907@zerovfx.com>
Message-ID: <511BB8F2.6020407@ekt.gr>

Hi,

Just tried that with the additional step (after removing the .glusterfs 
dir) of clearing the extended attributes of the existing files. Then 
issued the add-brick command but while the data are in the bricks, they 
have no extended attributes "attached" to them, thus they are 
unavailable to the gluster clients.

I suppose that somehow I should instruct gluster to run through the 
brick folder and files and add appropriate ext attrs.

a.




On 13/02/2013 05:14 ??, Brian Cipriano wrote:
> Hi -
>
> I was just looking at this yesterday. I've done some basic testing and 
> this seems to be possible.
>
> I think you're saying, you'd like to add a brick that was already 
> previously part of a gluster? Without losing the data that was on it, 
> and importing that data into the new gluster?
>
> All you should have to do is remove the .glusterfs folder on that 
> brick. Doc on that here:
>
> http://joejulian.name/blog/glusterfs-path-or-a-prefix-of-it-is-already-part-of-a-volume/ 
>
>
> Then running add-brick as normal. It will take a few minutes depending 
> on how much data is on the brick, but gluster should see that data and 
> rebuild its metadata accordingly.
>
> Again, I've only done BASIC testing, don't take my word for it.
>
> - brian
>
> On 2/13/13 7:22 AM, Alexandros Soumplis wrote:
>> Hello,
>>
>> Is it possible to "import" data already existent in a gluster brick?
>>
>> a.
>>
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From soumplis at ekt.gr  Wed Feb 13 16:05:24 2013
From: soumplis at ekt.gr (Alexandros Soumplis)
Date: Wed, 13 Feb 2013 18:05:24 +0200
Subject: [Gluster-users] Import data from brick
In-Reply-To: <511BB8F2.6020407@ekt.gr>
References: <511B8577.8060305@ekt.gr> <511BADD6.5010907@zerovfx.com>
	<511BB8F2.6020407@ekt.gr>
Message-ID: <511BB9C4.2070104@ekt.gr>

I do not know if it is related (probably is) but I now get in the brick 
logs the following messages

[2013-02-13 16:03:50.861313] W [server-resolve.c:516:server_resolve] 
0-BACULAVOL-server: no resolution type for (null) (LOOKUP)
[2013-02-13 16:03:50.861328] I [server-rpc-fops.c:160:server_lookup_cbk] 
0-BACULAVOL-server: 37: LOOKUP (null) 
(00000000-0000-0000-0000-000000000000) ==> (Invalid argument)



On 13/02/2013 06:01 ??, Alexandros Soumplis wrote:
> Hi,
>
> Just tried that with the additional step (after removing the 
> .glusterfs dir) of clearing the extended attributes of the existing 
> files. Then issued the add-brick command but while the data are in the 
> bricks, they have no extended attributes "attached" to them, thus they 
> are unavailable to the gluster clients.
>
> I suppose that somehow I should instruct gluster to run through the 
> brick folder and files and add appropriate ext attrs.
>
> a.
>
>
>
>
> On 13/02/2013 05:14 ??, Brian Cipriano wrote:
>> Hi -
>>
>> I was just looking at this yesterday. I've done some basic testing 
>> and this seems to be possible.
>>
>> I think you're saying, you'd like to add a brick that was already 
>> previously part of a gluster? Without losing the data that was on it, 
>> and importing that data into the new gluster?
>>
>> All you should have to do is remove the .glusterfs folder on that 
>> brick. Doc on that here:
>>
>> http://joejulian.name/blog/glusterfs-path-or-a-prefix-of-it-is-already-part-of-a-volume/ 
>>
>>
>> Then running add-brick as normal. It will take a few minutes 
>> depending on how much data is on the brick, but gluster should see 
>> that data and rebuild its metadata accordingly.
>>
>> Again, I've only done BASIC testing, don't take my word for it.
>>
>> - brian
>>
>> On 2/13/13 7:22 AM, Alexandros Soumplis wrote:
>>> Hello,
>>>
>>> Is it possible to "import" data already existent in a gluster brick?
>>>
>>> a.
>>>
>>> _______________________________________________
>>> Gluster-users mailing list
>>> Gluster-users at gluster.org
>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>


-- 
Alexandros Soumplis
Systems Engineer (M.Sc, RHCE, VCP, SCSA)

National Documentation Center / N.H.R.F.
Information Systems Department
48, Vas. Constantinou Av.
116 35 Athens, Greece
phone: +30 210 7273984  fax: +30 210 7252223
email: soumplis at ekt.gr
http://www.ekt.gr


From tompos at martos.bme.hu  Wed Feb 13 18:42:07 2013
From: tompos at martos.bme.hu (Papp Tamas)
Date: Wed, 13 Feb 2013 19:42:07 +0100
Subject: [Gluster-users] i7 vs. Xeon
Message-ID: <511BDE7F.2020008@martos.bme.hu>

hi All,

Is there a comparison in this subject?
Is it worth to spend money to Xeon rather than say RAM?

10x
tamas

From service at vhosting-it.com  Wed Feb 13 18:53:13 2013
From: service at vhosting-it.com (VHosting Solution)
Date: Wed, 13 Feb 2013 19:53:13 +0100
Subject: [Gluster-users] i7 vs. Xeon
In-Reply-To: <511BDE7F.2020008@martos.bme.hu>
References: <511BDE7F.2020008@martos.bme.hu>
Message-ID: <4048961B69EA43D1A490031536592A3C@VHostingTOSH>

For better stabilty xeon is better.

--------------------------------------------------
From: "Papp Tamas" <tompos at martos.bme.hu>
Sent: Wednesday, February 13, 2013 7:42 PM
To: <gluster-users at gluster.org>
Subject: [Gluster-users] i7 vs. Xeon

> hi All,
> 
> Is there a comparison in this subject?
> Is it worth to spend money to Xeon rather than say RAM?
> 
> 10x
> tamas
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users 

From tompos at martos.bme.hu  Wed Feb 13 18:59:03 2013
From: tompos at martos.bme.hu (Papp Tamas)
Date: Wed, 13 Feb 2013 19:59:03 +0100
Subject: [Gluster-users] i7 vs. Xeon
In-Reply-To: <4048961B69EA43D1A490031536592A3C@VHostingTOSH>
References: <511BDE7F.2020008@martos.bme.hu>
	<4048961B69EA43D1A490031536592A3C@VHostingTOSH>
Message-ID: <511BE277.4060808@martos.bme.hu>

On 02/13/2013 07:53 PM, VHosting Solution wrote:
>
> For better stabilty xeon is better.

Well.
Is i7 not stable enough? What do you mean on 'stability'?

tamas


From service at vhosting-it.com  Wed Feb 13 19:07:04 2013
From: service at vhosting-it.com (VHosting Solution)
Date: Wed, 13 Feb 2013 20:07:04 +0100
Subject: [Gluster-users] i7 vs. Xeon
In-Reply-To: <511BE277.4060808@martos.bme.hu>
References: <511BDE7F.2020008@martos.bme.hu>
	<4048961B69EA43D1A490031536592A3C@VHostingTOSH>
	<511BE277.4060808@martos.bme.hu>
Message-ID: <DAD5A77E4B3649638A9A5F100C58C288@VHostingTOSH>

Usually with xen is used ram ddr-ecc so, for server is better xeon with 
ddr-ecc and not i7

Bye

--------------------------------------------------
From: "Papp Tamas" <tompos at martos.bme.hu>
Sent: Wednesday, February 13, 2013 7:59 PM
To: "VHosting Solution" <service at vhosting-it.com>
Cc: <gluster-users at gluster.org>
Subject: Re: [Gluster-users] i7 vs. Xeon

> On 02/13/2013 07:53 PM, VHosting Solution wrote:
>>
>> For better stabilty xeon is better.
>
> Well.
> Is i7 not stable enough? What do you mean on 'stability'?
>
> tamas
> 

From tompos at martos.bme.hu  Wed Feb 13 19:10:31 2013
From: tompos at martos.bme.hu (Papp Tamas)
Date: Wed, 13 Feb 2013 20:10:31 +0100
Subject: [Gluster-users] i7 vs. Xeon
In-Reply-To: <DAD5A77E4B3649638A9A5F100C58C288@VHostingTOSH>
References: <511BDE7F.2020008@martos.bme.hu>
	<4048961B69EA43D1A490031536592A3C@VHostingTOSH>
	<511BE277.4060808@martos.bme.hu>
	<DAD5A77E4B3649638A9A5F100C58C288@VHostingTOSH>
Message-ID: <511BE527.2010608@martos.bme.hu>

On 02/13/2013 08:07 PM, VHosting Solution wrote:
>
> Usually with xen is used ram ddr-ecc so, for server is better xeon with ddr-ecc and not i7

Yes, that is a gluster independent reason. Of course my question is related to the gluster on i7 vs. 
Xeon:)


Thanks,
tamas

From michael.kushnir at nih.gov  Thu Feb 14 03:38:16 2013
From: michael.kushnir at nih.gov (Kushnir, Michael (NIH/NLM/LHC) [C])
Date: Thu, 14 Feb 2013 03:38:16 +0000
Subject: [Gluster-users] Gluster - data migration.
In-Reply-To: <CAHd9_iTJfnyvSo30VqbWW3o-FtNsLj3eagiqO1JQy+x-FFKGQw@mail.gmail.com>
References: <CAHd9_iTJfnyvSo30VqbWW3o-FtNsLj3eagiqO1JQy+x-FFKGQw@mail.gmail.com>
Message-ID: <2B177685F5DDAB479F03287189FD8C530963E0D4@MLBXV10.nih.gov>

Hi Rafal,

I just ran into a similar situation. 

You can leave slave bricks empty and create a volume. As long as, in each replicated set of bricks, each brick with data is on the left side of the pair, gluster will replicate your data. 

I.e. 

gluster volume create my-volume replica 2 transport tcp master:/brick1 slave:/brick1 master:/brick2 slave:/brick2

In my case, that replicated the data from the server with data to the server without data. 


Cheers!

Michael


________________________________________
From: Rafa? Radecki [radecki.rafal at gmail.com]
Sent: Thursday, February 07, 2013 8:57 AM
To: gluster-users at gluster.org
Subject: [Gluster-users] Gluster - data migration.

Hi All.

I have two servers ("master" and "slave") with a replicated gluster
volume. Recently I've had a problem with slave and gluster does not
work on it now.
So I would like to:
- stop and remove current volume on master (on slave it is not accessible);
- stop gluster software on master (already stopped on slave);
- remove gluster software on master and slave (previous administrator
used own built rpms, I would like to use
http://download.gluster.org/pub/gluster/glusterfs/3.3/3.3.1/EPEL.repo/);
- clean old information:
setfattr -x trusted.glusterfs.volume-id /gluster
setfattr -x trusted.gfid /gluster
rm -rf /gluster/.glusterfs
- rsync data from master to slave;
- start gluster and create a volume with data in rsynced /gluster directory.
Are there any pitfalls I should know about?

Best regards,
Rafal Radecki.
_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

From mcolonno at stanford.edu  Thu Feb 14 06:35:12 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Wed, 13 Feb 2013 22:35:12 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <510EBE98.9000807@julianfamily.org>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>	<019901ce00be$2d787ba0$886972e0$@stanford.edu>	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>	<00d301ce00de$ad920250$08b606f0$@stanford.edu>	<015101ce0243$d464c000$7d2e4000$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
Message-ID: <003901ce0a7d$71fc6840$55f538c0$@stanford.edu>

            More data: I got the Infiniband network (QDR) working well and
switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since
it doesn't seem to be supported yet for 3.x). The filesystem was slightly
faster but still well short of what I would expect by a wide margin. Via an
informal test (timing the movement of a large file) I'm getting several MB/s
- well short of even a standard Gb network copy. With the faster network the
CPU load on the brick systems increased dramatically: now I'm seeing
200%-250% usage by glusterfsd and glusterfs. 

            This leads me to believe that gluster is really not enjoying my
eight-brick, 2x replication volume with each brick system also being a
client. I tried a rebalance but no measurable effect. Any suggestions for
improving the performance? Having each brick be a client of itself seemed
the most logical choice to remove interdependencies but now I'm doubting the
setup.

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
Sent: Sunday, February 03, 2013 11:47 AM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

On 02/03/2013 11:22 AM, Michael Colonno wrote:



            Having taken a lot more data it does seem the glusterfsd and
glusterd processes (along with several ksoftirqd) spike up to near 100% on
both client and brick servers during any file transport across the mount.
Thankfully this is short-lived for the most part but I'm wondering if this
is expected behavior or what others have experienced(?) I'm a little
surprised such a large CPU load would be required to move small files and /
or use an application within a Gluster mount point. 


If you're getting ksoftirqd spikes, that sounds like a hardware issue to me.
I never see huge spikes like that on my servers nor clients.




 

            I wanted to test this against an NFS mount of the same Gluster
volume. I managed to get rstatd installed and running but my attempts to
mount the volume via NFS are met with: 

 

            mount.nfs: requested NFS version or transport protocol is not
supported

 

            Relevant line in /etc/fstab:

 

            node1:/volume    /volume    nfs
defaults,_netdev,vers=3,mountproto=tcp        0 0      

 

It looks like CentOS 6.x has NFS version 4 built into everything. So a few
questions:

 

-       Has anyone else noted significant performance differences between a
glusterfs mount and NFS mount for volumes of 8+ bricks? 

-       Is there a straightforward way to make the newer versions of CentOS
play nice with NFS version 3 + Gluster? 

-       Are there any general performance tuning guidelines I can follow to
improve CPU performance? I found a few references to the cache settings but
nothing solid. 

 

If the consensus is that NFS will not gain anything then I won't waste the
time setting it all up. 


NFS gains you the use of FSCache to cache directories and file stats making
directory listings faster, but it adds overhead decreasing the overall
throughput (from all the reports I've seen).

I would suspect that you have the kernel nfs server running on your servers.
Make sure it's disabled.




 

Thanks,

~Mike C. 

 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 4:46 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

            Update: after a few hours the CPU usage seems to have dropped
down to a small value. I did not change anything with respect to the
configuration or unmount / stop anything as I wanted to see if this would
persist for a long period of time. Both the client and the self-mounted
bricks are now showing CPU < 1% (as reported by top). Prior to the larger
CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is
this kind a transient behavior - by which I mean larger CPU loads after a
lot of filesystem activity in short time - typical? This is not a problem in
my deployment; I just want to know what to expect in the future and to
complete this thread for future users. If this is expected behavior we can
wrap up this thread. If not then I'll do more digging into the logs on the
client and brick sides. 

 

            Thanks,

            ~Mike C. 

 

From: Joe Julian [mailto:joe at julianfamily.org] 
Sent: Friday, February 01, 2013 2:08 PM
To: Michael Colonno; gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

Check the client log(s). 

Michael Colonno <mcolonno at stanford.edu> wrote:

            Forgot to mention: on a client system (not a brick) the
glusterfs process is consuming ~ 68% CPU continuously. This is a much less
powerful desktop system so the CPU load can't be compared 1:1 with the
systems comprising the bricks but still very high. So the issue seems to
exist with both glusterfsd and glusterfs processes. 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 12:46 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] high CPU load on all bricks

 

            Gluster gurus ~

 

            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
the volume. On each system contributing a brick, however, my CPU usage
(glusterfsd) is hovering around 20% (virtually zero memory usage
thankfully). These are brand new, fairly beefy servers so 20% CPU load is
quite a bit. The deployment is pretty plain with each brick mounting the
volume to itself via a glusterfs mount. I assume this type of CPU usage is
atypically high; is there anything I can do to investigate what's soaking up
CPU and minimize it? Total usable volume size is only about 22 TB (about 45
TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 


  _____  

 
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users






_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130213/2f203ee2/attachment-0001.html>

From Scott.Hazelhurst at wits.ac.za  Thu Feb 14 06:39:54 2013
From: Scott.Hazelhurst at wits.ac.za (Scott Hazelhurst)
Date: Thu, 14 Feb 2013 06:39:54 +0000
Subject: [Gluster-users] Gluster 3.3.1 consistency problems listing directory
Message-ID: <B9102D95F5488847A4BAF1D871C34B9A1500D03F@Elpis.ds.WITS.AC.ZA>


Dear all,

We ran into severe problems with our recently installed gluster system, described below

Volume Name: GA01
Type: Distributed-Replicate
Volume ID: ba885c47-3b44-4e7b-b8f9-a4cc0959b7a3
Status: Started
Number of Bricks: 3 x 2 = 6
Transport-type: tcp
Bricks:
Brick1: c3
Brick2: c2
Brick3: n3
Brick4: n5
Brick5: n9
Brick6: n13

While I was on leave for 10 days, the machine n3 went down. All seemed fine (noone knew) but after  I came back and restarted the machine, the system went into a very bad state. A typical scenario would be creating a new directory, adding files to it and then doing an ls. Any files that were physically started on the n3/n5 pair would not appear (though if you explicitly listed it eg. ls newdir/evec2gp.py it would appear and you could use it). I tried running heal several times, shutting down all cluster daemons in the system, remounting bricks, remounting the gluster system, etc. I could restart the n3 machine but I can't restart other machines because it's in production. This happens reproducibly, not just with files that were created while n3 was down but even now.

We are running gluster 3.3.1. The underlying bricks are formatted as ext3 (not ext4).  The n machines run SL6.3 and the c machines run Ubuntu 12.04 (there are constraints that require this). Could the use of different OS cause a problem?

At this point, I prepared to copy all files off the gluster volume and completely reformat the underlying file systems and recreate the gluster volumes from scratch. However, I am concerned since we need to rely on the system.

Any suggestions

Thanks

Scott


<html><p><font face = "verdana" size = "0.8" color = "navy">This communication is intended for the addressee only. It is confidential. If you have received this communication in error, please notify us immediately and destroy the original message. You may not copy or disseminate this communication without the permission of the University. Only authorized signatories are competent to enter into agreements on behalf of the University and recipients are thus advised that the content of this message may not be legally binding on the University and may contain the personal views and opinions of the author, which are not necessarily the views and opinions of The University of the Witwatersrand, Johannesburg. All agreements between the University and outsiders are subject to South African Law unless the University agrees in writing to the contrary.</font></p></html>


From johnmark at johnmark.org  Thu Feb 14 15:36:15 2013
From: johnmark at johnmark.org (John Mark Walker)
Date: Thu, 14 Feb 2013 10:36:15 -0500
Subject: [Gluster-users] HA with CTDB and NFS or CIFS
Message-ID: <CAMUFnfabAzS=-zX5-4x1zaGoW80EMnFGU7vBRGRXkOWZsKPdgA@mail.gmail.com>

If you're interested in setting up high availability with NFS or SMB, you
may be interested in this webinar:

https://vts.inxpo.com/scripts/Server.nxp

Just remember, when you hear "Red Hat Storage" think "GlusterFS" :)

-JM
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130214/5b01e3e4/attachment.html>

From joe at julianfamily.org  Thu Feb 14 17:25:16 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 14 Feb 2013 09:25:16 -0800
Subject: [Gluster-users] server3_1-fops.c:1240 (Cannot allocate memory)
In-Reply-To: <CAOg=WDd-sr68X7mcBLdVVYG+xrWVtTJL-SuaCr0JOB=ENUp=Gg@mail.gmail.com>
References: <CAOg=WDd-sr68X7mcBLdVVYG+xrWVtTJL-SuaCr0JOB=ENUp=Gg@mail.gmail.com>
Message-ID: <511D1DFC.8050201@julianfamily.org>

Note that those are Info notices (" I "), not Errors (" E ").

3.2.2 has known critical bugs. You should upgrade (at least within the 
3.2 branch) asap.

On 02/12/2013 10:19 AM, samuel wrote:
> Hi folks,
>
> We're using in an old environment gluster 3.2.2 and we've just started 
> seeing the following error in /usr/local/var/log/glusterfs/bricks/gfs-log:
>
> [2013-02-12 19:15:05.214430] I 
> [server3_1-fops.c:1240:server_writev_cbk] 0-cloud-server: 2400444243: 
> WRITEV 7 (37067) ==> -1 (Cannot allocate memory)
> [2013-02-12 19:15:19.463087] I 
> [server3_1-fops.c:1240:server_writev_cbk] 0-cloud-server: 2249406582: 
> WRITEV 15 (52493) ==> -1 (Cannot allocate memory)
>
> in a distributed replicated 2-node environment.
>
> Both nodes have enough disk space (1.3T) but looks like used memory is 
> quite high:
>
> node1:
>              total       used       free     shared    buffers cached
> Mem:       4047680    4012700      34980          0        276 3728652
> -/+ buffers/cache:     283772    3763908
> Swap:      3906244      19916    3886328
>
> node2:
>              total       used       free     shared    buffers cached
> Mem:       4047680    4012488      35192          0       1088 3713512
> -/+ buffers/cache:     297888    3749792
> Swap:      3905532      25244    3880288
>
> could it be a memory issue?
>
> Best regards,
> Samuel.
>
>
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130214/325a317c/attachment.html>

From driver at megahappy.net  Thu Feb 14 17:52:09 2013
From: driver at megahappy.net (Bryan Whitehead)
Date: Thu, 14 Feb 2013 09:52:09 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<019901ce00be$2d787ba0$886972e0$@stanford.edu>
	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>
	<00d301ce00de$ad920250$08b606f0$@stanford.edu>
	<015101ce0243$d464c000$7d2e4000$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
Message-ID: <CAA3XVTxP1Z0GQG4Wb8hL+YRK89PVzpnmCDcNWN91Tm61051qpg@mail.gmail.com>

is transport tcp or tcp,rdma? I'm using transport=tcp for IPoIB and get
pretty fantastic speeds. I noticed when I used tcp,rdma as my transport I
had problems.

Are you mounting via fuse or nfs? I don't have any experience using the nfs
but fuse works really well.

Additionally, how are you using the volume? many small files or big large
files? I'm hosting qcow2 files that are between 4 and 250GB.


On Wed, Feb 13, 2013 at 10:35 PM, Michael Colonno <mcolonno at stanford.edu>wrote:

>             More data: I got the Infiniband network (QDR) working well and
> switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since
> it doesn?t seem to be supported yet for 3.x). The filesystem was slightly
> faster but still well short of what I would expect by a wide margin. Via an
> informal test (timing the movement of a large file) I?m getting several
> MB/s ? well short of even a standard Gb network copy. With the faster
> network the CPU load on the brick systems increased dramatically: now I?m
> seeing 200%-250% usage by glusterfsd and glusterfs. ****
>
>             This leads me to believe that gluster is really not enjoying
> my eight-brick, 2x replication volume with each brick system also being a
> client. I tried a rebalance but no measurable effect. Any suggestions for
> improving the performance? Having each brick be a client of itself seemed
> the most logical choice to remove interdependencies but now I?m doubting
> the setup?****
>
> ** **
>
>             Thanks,****
>
>             ~Mike C. ****
>
> ** **
>
> *From:* gluster-users-bounces at gluster.org [mailto:
> gluster-users-bounces at gluster.org] *On Behalf Of *Joe Julian
>
> *Sent:* Sunday, February 03, 2013 11:47 AM
> *To:* gluster-users at gluster.org
> *Subject:* Re: [Gluster-users] high CPU load on all bricks****
>
> ** **
>
> On 02/03/2013 11:22 AM, Michael Colonno wrote:
>
> ****
>
>             Having taken a lot more data it does seem the glusterfsd and
> glusterd processes (along with several ksoftirqd) spike up to near 100% on
> both client and brick servers during any file transport across the mount.
> Thankfully this is short-lived for the most part but I?m wondering if this
> is expected behavior or what others have experienced(?) I?m a little
> surprised such a large CPU load would be required to move small files and /
> or use an application within a Gluster mount point. ****
>
>
> If you're getting ksoftirqd spikes, that sounds like a hardware issue to
> me. I never see huge spikes like that on my servers nor clients.
>
>
> ****
>
>  ****
>
>             I wanted to test this against an NFS mount of the same Gluster
> volume. I managed to get rstatd installed and running but my attempts to
> mount the volume via NFS are met with: ****
>
>  ****
>
>             mount.nfs: requested NFS version or transport protocol is not
> supported****
>
>  ****
>
>             Relevant line in /etc/fstab:****
>
>  ****
>
>             node1:/volume    /volume    nfs
> defaults,_netdev,vers=3,mountproto=tcp        0 0      ****
>
>  ****
>
> It looks like CentOS 6.x has NFS version 4 built into everything. So a few
> questions:****
>
>  ****
>
> **-       **Has anyone else noted significant performance differences
> between a glusterfs mount and NFS mount for volumes of 8+ bricks? ****
>
> **-       **Is there a straightforward way to make the newer versions of
> CentOS play nice with NFS version 3 + Gluster? ****
>
> **-       **Are there any general performance tuning guidelines I can
> follow to improve CPU performance? I found a few references to the cache
> settings but nothing solid. ****
>
>  ****
>
> If the consensus is that NFS will not gain anything then I won?t waste the
> time setting it all up. ****
>
>
> NFS gains you the use of FSCache to cache directories and file stats
> making directory listings faster, but it adds overhead decreasing the
> overall throughput (from all the reports I've seen).
>
> I would suspect that you have the kernel nfs server running on your
> servers. Make sure it's disabled.
>
>
> ****
>
>  ****
>
> Thanks,****
>
> ~Mike C. ****
>
>  ****
>
>  ****
>
> *From:* gluster-users-bounces at gluster.org [
> mailto:gluster-users-bounces at gluster.org<gluster-users-bounces at gluster.org>]
> *On Behalf Of *Michael Colonno
> *Sent:* Friday, February 01, 2013 4:46 PM
> *To:* gluster-users at gluster.org
> *Subject:* Re: [Gluster-users] high CPU load on all bricks****
>
>  ****
>
>             Update: after a few hours the CPU usage seems to have dropped
> down to a small value. I did not change anything with respect to the
> configuration or unmount / stop anything as I wanted to see if this would
> persist for a long period of time. Both the client and the self-mounted
> bricks are now showing CPU < 1% (as reported by top). Prior to the larger
> CPU loads I installed a bunch of software into the volume (~ 5 GB total).
> Is this kind a transient behavior ? by which I mean larger CPU loads after
> a lot of filesystem activity in short time ? typical? This is not a problem
> in my deployment; I just want to know what to expect in the future and to
> complete this thread for future users. If this is expected behavior we can
> wrap up this thread. If not then I?ll do more digging into the logs on the
> client and brick sides. ****
>
>  ****
>
>             Thanks,****
>
>             ~Mike C. ****
>
>  ****
>
> *From:* Joe Julian [mailto:joe at julianfamily.org <joe at julianfamily.org>]
> *Sent:* Friday, February 01, 2013 2:08 PM
> *To:* Michael Colonno; gluster-users at gluster.org
> *Subject:* Re: [Gluster-users] high CPU load on all bricks****
>
>  ****
>
> Check the client log(s). ****
>
> Michael Colonno <mcolonno at stanford.edu> wrote:****
>
>             Forgot to mention: on a client system (not a brick) the
> glusterfs process is consuming ~ 68% CPU continuously. This is a much less
> powerful desktop system so the CPU load can?t be compared 1:1 with the
> systems comprising the bricks but still very high. So the issue seems to
> exist with both glusterfsd and glusterfs processes. ****
>
>  ****
>
>             Thanks,****
>
>             ~Mike C. ****
>
>  ****
>
> *From:* gluster-users-bounces at gluster.org [
> mailto:gluster-users-bounces at gluster.org<gluster-users-bounces at gluster.org>]
> *On Behalf Of *Michael Colonno
> *Sent:* Friday, February 01, 2013 12:46 PM
> *To:* gluster-users at gluster.org
> *Subject:* [Gluster-users] high CPU load on all bricks****
>
>  ****
>
>             Gluster gurus ~****
>
>  ****
>
>             I?ve deployed and 8-brick (2x replicate) Gluster 3.3.1 volume
> on CentOS 6.3 with tcp transport. I was able to build, start, mount, and
> use the volume. On each system contributing a brick, however, my CPU usage
> (glusterfsd) is hovering around 20% (virtually zero memory usage
> thankfully). These are brand new, fairly beefy servers so 20% CPU load is
> quite a bit. The deployment is pretty plain with each brick mounting the
> volume to itself via a glusterfs mount. I assume this type of CPU usage is
> atypically high; is there anything I can do to investigate what?s soaking
> up CPU and minimize it? Total usable volume size is only about 22 TB (about
> 45 TB total with 2x replicate). ****
>
>  ****
>
>             Thanks,****
>
>             ~Mike C. ****
>
>  ****
>
> ------------------------------
>
> ** **
>
> Gluster-users mailing list****
>
> Gluster-users at gluster.org****
>
> http://supercolony.gluster.org/mailman/listinfo/gluster-users****
>
>
>
>
> ****
>
> _______________________________________________****
>
> Gluster-users mailing list****
>
> Gluster-users at gluster.org****
>
> http://supercolony.gluster.org/mailman/listinfo/gluster-users****
>
> ** **
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130214/d9fb3ea6/attachment-0001.html>

From harry.mangalam at uci.edu  Thu Feb 14 18:28:15 2013
From: harry.mangalam at uci.edu (harry mangalam)
Date: Thu, 14 Feb 2013 10:28:15 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
Message-ID: <85058672.lsbGZuViaP@stunted>

While I don't understand your 'each brick system also being a client' setup - 
you mean that each gluster brick is a native gluster client as well?  And that 
is where much of your gluster access is coming from?  That seems .. suboptimal 
if that's the setup.  Is there a reason for that setup?

We have a distributed-only glusterfs feeding a medium cluster over a similar 
same setup QDR IPoIB with 4 servers with 2 bricks each.  On a fairly busy 
system (~80MB/s background), I can get about 100-300MB/s writes to the gluster 
fs on a large 1.7GB file.  (With tiny writes, the perf decreases 
dramatically).

Here is my config: (if anyone spies something that I should change to increase 
my perf, please feel free to point out my mistake)

gluster:
Volume Name: gl
Type: Distribute
Volume ID: 21f480f7-fc5a-4fd8-a084-3964634a9332
Status: Started
Number of Bricks: 8
Transport-type: tcp,rdma
Bricks:
Brick1: bs2:/raid1
Brick2: bs2:/raid2
Brick3: bs3:/raid1
Brick4: bs3:/raid2
Brick5: bs4:/raid1
Brick6: bs4:/raid2
Brick7: bs1:/raid1
Brick8: bs1:/raid2
Options Reconfigured:
performance.write-behind-window-size: 1024MB
performance.flush-behind: on
performance.cache-size: 268435456
nfs.disable: on
performance.io-cache: on
performance.quick-read: on
performance.io-thread-count: 64
auth.allow: 10.2.*.*,10.1.*.*

my RAID6s (via 3ware 9750s) are mounted with the following options

/dev/sdc /raid1 xfs rw,noatime,sunit=512,swidth=8192,allocsize=32m 0 0
/dev/sdd /raid2 xfs rw,noatime,sunit=512,swidth=7680,allocsize=32m 0 0
(and should probably be using 'nobarrier,inode64' as well. - testing this now)

There are some good refs on prepping XFS fs for max perf here:
<http://www.mythtv.org/wiki/Optimizing_Performance#XFS-Specific_Tips>
The script at:
<http://www.mythtv.org/wiki/Optimizing_Performance#Further_Information>
can help to setup the sunit/swidth options.
<http://www.mysqlperformanceblog.com/2011/12/16/setting-up-xfs-the-simple-
edition/>
Your ib interfaces should be using large mtus (65536)

hjm

On Wednesday, February 13, 2013 10:35:12 PM Michael Colonno wrote:
>             More data: I got the Infiniband network (QDR) working well and
> switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since
> it doesn't seem to be supported yet for 3.x). The filesystem was slightly
> faster but still well short of what I would expect by a wide margin. Via an
> informal test (timing the movement of a large file) I'm getting several MB/s
> - well short of even a standard Gb network copy. With the faster network
> the CPU load on the brick systems increased dramatically: now I'm seeing
> 200%-250% usage by glusterfsd and glusterfs.
> 
>             This leads me to believe that gluster is really not enjoying my
> eight-brick, 2x replication volume with each brick system also being a
> client. I tried a rebalance but no measurable effect. Any suggestions for
> improving the performance? Having each brick be a client of itself seemed
> the most logical choice to remove interdependencies but now I'm doubting the
> setup.
> 
> 
> 
>             Thanks,
> 
>             ~Mike C.
> 
> 
> 
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
> Sent: Sunday, February 03, 2013 11:47 AM
> To: gluster-users at gluster.org
> Subject: Re: [Gluster-users] high CPU load on all bricks
> 
> 
> 
> On 02/03/2013 11:22 AM, Michael Colonno wrote:
> 
> 
> 
>             Having taken a lot more data it does seem the glusterfsd and
> glusterd processes (along with several ksoftirqd) spike up to near 100% on
> both client and brick servers during any file transport across the mount.
> Thankfully this is short-lived for the most part but I'm wondering if this
> is expected behavior or what others have experienced(?) I'm a little
> surprised such a large CPU load would be required to move small files and /
> or use an application within a Gluster mount point.
> 
> 
> If you're getting ksoftirqd spikes, that sounds like a hardware issue to me.
> I never see huge spikes like that on my servers nor clients.
> 
> 
> 
> 
> 
> 
>             I wanted to test this against an NFS mount of the same Gluster
> volume. I managed to get rstatd installed and running but my attempts to
> mount the volume via NFS are met with:
> 
> 
> 
>             mount.nfs: requested NFS version or transport protocol is not
> supported
> 
> 
> 
>             Relevant line in /etc/fstab:
> 
> 
> 
>             node1:/volume    /volume    nfs
> defaults,_netdev,vers=3,mountproto=tcp        0 0
> 
> 
> 
> It looks like CentOS 6.x has NFS version 4 built into everything. So a few
> questions:
> 
> 
> 
> -       Has anyone else noted significant performance differences between a
> glusterfs mount and NFS mount for volumes of 8+ bricks?
> 
> -       Is there a straightforward way to make the newer versions of CentOS
> play nice with NFS version 3 + Gluster?
> 
> -       Are there any general performance tuning guidelines I can follow to
> improve CPU performance? I found a few references to the cache settings but
> nothing solid.
> 
> 
> 
> If the consensus is that NFS will not gain anything then I won't waste the
> time setting it all up.
> 
> 
> NFS gains you the use of FSCache to cache directories and file stats making
> directory listings faster, but it adds overhead decreasing the overall
> throughput (from all the reports I've seen).
> 
> I would suspect that you have the kernel nfs server running on your servers.
> Make sure it's disabled.
> 
> 
> 
> 
> 
> 
> Thanks,
> 
> ~Mike C.
> 
> 
> 
> 
> 
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> Sent: Friday, February 01, 2013 4:46 PM
> To: gluster-users at gluster.org
> Subject: Re: [Gluster-users] high CPU load on all bricks
> 
> 
> 
>             Update: after a few hours the CPU usage seems to have dropped
> down to a small value. I did not change anything with respect to the
> configuration or unmount / stop anything as I wanted to see if this would
> persist for a long period of time. Both the client and the self-mounted
> bricks are now showing CPU < 1% (as reported by top). Prior to the larger
> CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is
> this kind a transient behavior - by which I mean larger CPU loads after a
> lot of filesystem activity in short time - typical? This is not a problem
> in my deployment; I just want to know what to expect in the future and to
> complete this thread for future users. If this is expected behavior we can
> wrap up this thread. If not then I'll do more digging into the logs on the
> client and brick sides.
> 
> 
> 
>             Thanks,
> 
>             ~Mike C.
> 
> 
> 
> From: Joe Julian [mailto:joe at julianfamily.org]
> Sent: Friday, February 01, 2013 2:08 PM
> To: Michael Colonno; gluster-users at gluster.org
> Subject: Re: [Gluster-users] high CPU load on all bricks
> 
> 
> 
> Check the client log(s).
> 
> Michael Colonno <mcolonno at stanford.edu> wrote:
> 
>             Forgot to mention: on a client system (not a brick) the
> glusterfs process is consuming ~ 68% CPU continuously. This is a much less
> powerful desktop system so the CPU load can't be compared 1:1 with the
> systems comprising the bricks but still very high. So the issue seems to
> exist with both glusterfsd and glusterfs processes.
> 
> 
> 
>             Thanks,
> 
>             ~Mike C.
> 
> 
> 
> From: gluster-users-bounces at gluster.org
> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> Sent: Friday, February 01, 2013 12:46 PM
> To: gluster-users at gluster.org
> Subject: [Gluster-users] high CPU load on all bricks
> 
> 
> 
>             Gluster gurus ~
> 
> 
> 
>             I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
> CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
> the volume. On each system contributing a brick, however, my CPU usage
> (glusterfsd) is hovering around 20% (virtually zero memory usage
> thankfully). These are brand new, fairly beefy servers so 20% CPU load is
> quite a bit. The deployment is pretty plain with each brick mounting the
> volume to itself via a glusterfs mount. I assume this type of CPU usage is
> atypically high; is there anything I can do to investigate what's soaking up
> CPU and minimize it? Total usable volume size is only about 22 TB (about 45
> TB total with 2x replicate).
> 
> 
> 
>             Thanks,
> 
>             ~Mike C.
> 
> 
> 
> 
>   _____
> 
> 
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
> 
> 
> 
> 
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

---
Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
[m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
415 South Circle View Dr, Irvine, CA, 92697 [shipping]
MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
---
"Something must be done. [X] is something. Therefore, we must do it."
Bruce Schneier, on American response to just about anything.

From mcolonno at stanford.edu  Thu Feb 14 18:13:42 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 14 Feb 2013 10:13:42 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <CAA3XVTxP1Z0GQG4Wb8hL+YRK89PVzpnmCDcNWN91Tm61051qpg@mail.gmail.com>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<019901ce00be$2d787ba0$886972e0$@stanford.edu>
	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>
	<00d301ce00de$ad920250$08b606f0$@stanford.edu>
	<015101ce0243$d464c000$7d2e4000$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
	<CAA3XVTxP1Z0GQG4Wb8hL+YRK89PVzpnmCDcNWN91Tm61051qpg@mail.gmail.com>
Message-ID: <9FC1AF2E-0292-41C5-8DAF-9DA4D30F15B1@stanford.edu>

Tcp transport and file sizes are nominal (up to a few GB typically). Using glusterfs mount (no NFS). There's nothing unusual about the deployment except the eight-brick setup server-client setup mentioned below. Is there anything I can do to identify the bottleneck(s) and / or tune performance? I'm going to try to build the rpms myself though I doubt that will change anything vs. the pre-built ones.

Thanks,
Mike C.

On Feb 14, 2013, at 9:52 AM, Bryan Whitehead <driver at megahappy.net> wrote:

> is transport tcp or tcp,rdma? I'm using transport=tcp for IPoIB and get pretty fantastic speeds. I noticed when I used tcp,rdma as my transport I had problems.
> 
> Are you mounting via fuse or nfs? I don't have any experience using the nfs but fuse works really well.
> 
> Additionally, how are you using the volume? many small files or big large files? I'm hosting qcow2 files that are between 4 and 250GB.
> 
> 
> On Wed, Feb 13, 2013 at 10:35 PM, Michael Colonno <mcolonno at stanford.edu> wrote:
>>             More data: I got the Infiniband network (QDR) working well and switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since it doesn?t seem to be supported yet for 3.x). The filesystem was slightly faster but still well short of what I would expect by a wide margin. Via an informal test (timing the movement of a large file) I?m getting several MB/s ? well short of even a standard Gb network copy. With the faster network the CPU load on the brick systems increased dramatically: now I?m seeing 200%-250% usage by glusterfsd and glusterfs.
>> 
>>             This leads me to believe that gluster is really not enjoying my eight-brick, 2x replication volume with each brick system also being a client. I tried a rebalance but no measurable effect. Any suggestions for improving the performance? Having each brick be a client of itself seemed the most logical choice to remove interdependencies but now I?m doubting the setup?
>> 
>>  
>> 
>>             Thanks,
>> 
>>             ~Mike C.
>> 
>>  
>> 
>> From: gluster-users-bounces at gluster.org [mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
>> 
>> 
>> Sent: Sunday, February 03, 2013 11:47 AM
>> To: gluster-users at gluster.org
>> Subject: Re: [Gluster-users] high CPU load on all bricks
>>  
>> 
>> On 02/03/2013 11:22 AM, Michael Colonno wrote:
>> 
>> 
>>             Having taken a lot more data it does seem the glusterfsd and glusterd processes (along with several ksoftirqd) spike up to near 100% on both client and brick servers during any file transport across the mount. Thankfully this is short-lived for the most part but I?m wondering if this is expected behavior or what others have experienced(?) I?m a little surprised such a large CPU load would be required to move small files and / or use an application within a Gluster mount point.
>> 
>> 
>> If you're getting ksoftirqd spikes, that sounds like a hardware issue to me. I never see huge spikes like that on my servers nor clients.
>> 
>> 
>> 
>>  
>> 
>>             I wanted to test this against an NFS mount of the same Gluster volume. I managed to get rstatd installed and running but my attempts to mount the volume via NFS are met with:
>> 
>>  
>> 
>>             mount.nfs: requested NFS version or transport protocol is not supported
>> 
>>  
>> 
>>             Relevant line in /etc/fstab:
>> 
>>  
>> 
>>             node1:/volume    /volume    nfs     defaults,_netdev,vers=3,mountproto=tcp        0 0     
>> 
>>  
>> 
>> It looks like CentOS 6.x has NFS version 4 built into everything. So a few questions:
>> 
>>  
>> 
>> -       Has anyone else noted significant performance differences between a glusterfs mount and NFS mount for volumes of 8+ bricks?
>> 
>> -       Is there a straightforward way to make the newer versions of CentOS play nice with NFS version 3 + Gluster? 
>> 
>> -       Are there any general performance tuning guidelines I can follow to improve CPU performance? I found a few references to the cache settings but nothing solid.
>> 
>>  
>> 
>> If the consensus is that NFS will not gain anything then I won?t waste the time setting it all up.
>> 
>> 
>> NFS gains you the use of FSCache to cache directories and file stats making directory listings faster, but it adds overhead decreasing the overall throughput (from all the reports I've seen).
>> 
>> I would suspect that you have the kernel nfs server running on your servers. Make sure it's disabled.
>> 
>> 
>> 
>>  
>> 
>> Thanks,
>> 
>> ~Mike C.
>> 
>>  
>> 
>>  
>> 
>> From: gluster-users-bounces at gluster.org [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>> Sent: Friday, February 01, 2013 4:46 PM
>> To: gluster-users at gluster.org
>> Subject: Re: [Gluster-users] high CPU load on all bricks
>> 
>>  
>> 
>>             Update: after a few hours the CPU usage seems to have dropped down to a small value. I did not change anything with respect to the configuration or unmount / stop anything as I wanted to see if this would persist for a long period of time. Both the client and the self-mounted bricks are now showing CPU < 1% (as reported by top). Prior to the larger CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is this kind a transient behavior ? by which I mean larger CPU loads after a lot of filesystem activity in short time ? typical? This is not a problem in my deployment; I just want to know what to expect in the future and to complete this thread for future users. If this is expected behavior we can wrap up this thread. If not then I?ll do more digging into the logs on the client and brick sides.
>> 
>>  
>> 
>>             Thanks,
>> 
>>             ~Mike C.
>> 
>>  
>> 
>> From: Joe Julian [mailto:joe at julianfamily.org] 
>> Sent: Friday, February 01, 2013 2:08 PM
>> To: Michael Colonno; gluster-users at gluster.org
>> Subject: Re: [Gluster-users] high CPU load on all bricks
>> 
>>  
>> 
>> Check the client log(s).
>> 
>> Michael Colonno <mcolonno at stanford.edu> wrote:
>> 
>>             Forgot to mention: on a client system (not a brick) the glusterfs process is consuming ~ 68% CPU continuously. This is a much less powerful desktop system so the CPU load can?t be compared 1:1 with the systems comprising the bricks but still very high. So the issue seems to exist with both glusterfsd and glusterfs processes.
>> 
>>  
>> 
>>             Thanks,
>> 
>>             ~Mike C.
>> 
>>  
>> 
>> From: gluster-users-bounces at gluster.org [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>> Sent: Friday, February 01, 2013 12:46 PM
>> To: gluster-users at gluster.org
>> Subject: [Gluster-users] high CPU load on all bricks
>> 
>>  
>> 
>>             Gluster gurus ~
>> 
>>  
>> 
>>             I?ve deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on CentOS 6.3 with tcp transport. I was able to build, start, mount, and use the volume. On each system contributing a brick, however, my CPU usage (glusterfsd) is hovering around 20% (virtually zero memory usage thankfully). These are brand new, fairly beefy servers so 20% CPU load is quite a bit. The deployment is pretty plain with each brick mounting the volume to itself via a glusterfs mount. I assume this type of CPU usage is atypically high; is there anything I can do to investigate what?s soaking up CPU and minimize it? Total usable volume size is only about 22 TB (about 45 TB total with 2x replicate). 
>> 
>>  
>> 
>>             Thanks,
>> 
>>             ~Mike C.
>> 
>>  
>> 
>>  
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> 
>> 
>> 
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>  
>> 
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130214/4617595e/attachment-0001.html>

From mcolonno at stanford.edu  Thu Feb 14 19:58:36 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 14 Feb 2013 11:58:36 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <85058672.lsbGZuViaP@stunted>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
	<85058672.lsbGZuViaP@stunted>
Message-ID: <31EE9284-3079-429A-BCB0-C155F0D53717@stanford.edu>

Good place to start: do the bricks have to be clients as well? In other words if I copy a file to a Gluster brick without going through a glusterfs or NFS mount will that disrupt the parallel file system? I assumed files need to be routed through a glusterfs mount point for Gluster to be able to track them(?) What's recommended for bricks which also need i/o to the entire volume?

Thanks,
Mike C.

On Feb 14, 2013, at 10:28 AM, harry mangalam <harry.mangalam at uci.edu> wrote:

> While I don't understand your 'each brick system also being a client' setup - 
> you mean that each gluster brick is a native gluster client as well?  And that 
> is where much of your gluster access is coming from?  That seems .. suboptimal 
> if that's the setup.  Is there a reason for that setup?
> 
> We have a distributed-only glusterfs feeding a medium cluster over a similar 
> same setup QDR IPoIB with 4 servers with 2 bricks each.  On a fairly busy 
> system (~80MB/s background), I can get about 100-300MB/s writes to the gluster 
> fs on a large 1.7GB file.  (With tiny writes, the perf decreases 
> dramatically).
> 
> Here is my config: (if anyone spies something that I should change to increase 
> my perf, please feel free to point out my mistake)
> 
> gluster:
> Volume Name: gl
> Type: Distribute
> Volume ID: 21f480f7-fc5a-4fd8-a084-3964634a9332
> Status: Started
> Number of Bricks: 8
> Transport-type: tcp,rdma
> Bricks:
> Brick1: bs2:/raid1
> Brick2: bs2:/raid2
> Brick3: bs3:/raid1
> Brick4: bs3:/raid2
> Brick5: bs4:/raid1
> Brick6: bs4:/raid2
> Brick7: bs1:/raid1
> Brick8: bs1:/raid2
> Options Reconfigured:
> performance.write-behind-window-size: 1024MB
> performance.flush-behind: on
> performance.cache-size: 268435456
> nfs.disable: on
> performance.io-cache: on
> performance.quick-read: on
> performance.io-thread-count: 64
> auth.allow: 10.2.*.*,10.1.*.*
> 
> my RAID6s (via 3ware 9750s) are mounted with the following options
> 
> /dev/sdc /raid1 xfs rw,noatime,sunit=512,swidth=8192,allocsize=32m 0 0
> /dev/sdd /raid2 xfs rw,noatime,sunit=512,swidth=7680,allocsize=32m 0 0
> (and should probably be using 'nobarrier,inode64' as well. - testing this now)
> 
> There are some good refs on prepping XFS fs for max perf here:
> <http://www.mythtv.org/wiki/Optimizing_Performance#XFS-Specific_Tips>
> The script at:
> <http://www.mythtv.org/wiki/Optimizing_Performance#Further_Information>
> can help to setup the sunit/swidth options.
> <http://www.mysqlperformanceblog.com/2011/12/16/setting-up-xfs-the-simple-
> edition/>
> Your ib interfaces should be using large mtus (65536)
> 
> hjm
> 
> On Wednesday, February 13, 2013 10:35:12 PM Michael Colonno wrote:
>>            More data: I got the Infiniband network (QDR) working well and
>> switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since
>> it doesn't seem to be supported yet for 3.x). The filesystem was slightly
>> faster but still well short of what I would expect by a wide margin. Via an
>> informal test (timing the movement of a large file) I'm getting several MB/s
>> - well short of even a standard Gb network copy. With the faster network
>> the CPU load on the brick systems increased dramatically: now I'm seeing
>> 200%-250% usage by glusterfsd and glusterfs.
>> 
>>            This leads me to believe that gluster is really not enjoying my
>> eight-brick, 2x replication volume with each brick system also being a
>> client. I tried a rebalance but no measurable effect. Any suggestions for
>> improving the performance? Having each brick be a client of itself seemed
>> the most logical choice to remove interdependencies but now I'm doubting the
>> setup.
>> 
>> 
>> 
>>            Thanks,
>> 
>>            ~Mike C.
>> 
>> 
>> 
>> From: gluster-users-bounces at gluster.org
>> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
>> Sent: Sunday, February 03, 2013 11:47 AM
>> To: gluster-users at gluster.org
>> Subject: Re: [Gluster-users] high CPU load on all bricks
>> 
>> 
>> 
>> On 02/03/2013 11:22 AM, Michael Colonno wrote:
>> 
>> 
>> 
>>            Having taken a lot more data it does seem the glusterfsd and
>> glusterd processes (along with several ksoftirqd) spike up to near 100% on
>> both client and brick servers during any file transport across the mount.
>> Thankfully this is short-lived for the most part but I'm wondering if this
>> is expected behavior or what others have experienced(?) I'm a little
>> surprised such a large CPU load would be required to move small files and /
>> or use an application within a Gluster mount point.
>> 
>> 
>> If you're getting ksoftirqd spikes, that sounds like a hardware issue to me.
>> I never see huge spikes like that on my servers nor clients.
>> 
>> 
>> 
>> 
>> 
>> 
>>            I wanted to test this against an NFS mount of the same Gluster
>> volume. I managed to get rstatd installed and running but my attempts to
>> mount the volume via NFS are met with:
>> 
>> 
>> 
>>            mount.nfs: requested NFS version or transport protocol is not
>> supported
>> 
>> 
>> 
>>            Relevant line in /etc/fstab:
>> 
>> 
>> 
>>            node1:/volume    /volume    nfs
>> defaults,_netdev,vers=3,mountproto=tcp        0 0
>> 
>> 
>> 
>> It looks like CentOS 6.x has NFS version 4 built into everything. So a few
>> questions:
>> 
>> 
>> 
>> -       Has anyone else noted significant performance differences between a
>> glusterfs mount and NFS mount for volumes of 8+ bricks?
>> 
>> -       Is there a straightforward way to make the newer versions of CentOS
>> play nice with NFS version 3 + Gluster?
>> 
>> -       Are there any general performance tuning guidelines I can follow to
>> improve CPU performance? I found a few references to the cache settings but
>> nothing solid.
>> 
>> 
>> 
>> If the consensus is that NFS will not gain anything then I won't waste the
>> time setting it all up.
>> 
>> 
>> NFS gains you the use of FSCache to cache directories and file stats making
>> directory listings faster, but it adds overhead decreasing the overall
>> throughput (from all the reports I've seen).
>> 
>> I would suspect that you have the kernel nfs server running on your servers.
>> Make sure it's disabled.
>> 
>> 
>> 
>> 
>> 
>> 
>> Thanks,
>> 
>> ~Mike C.
>> 
>> 
>> 
>> 
>> 
>> From: gluster-users-bounces at gluster.org
>> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>> Sent: Friday, February 01, 2013 4:46 PM
>> To: gluster-users at gluster.org
>> Subject: Re: [Gluster-users] high CPU load on all bricks
>> 
>> 
>> 
>>            Update: after a few hours the CPU usage seems to have dropped
>> down to a small value. I did not change anything with respect to the
>> configuration or unmount / stop anything as I wanted to see if this would
>> persist for a long period of time. Both the client and the self-mounted
>> bricks are now showing CPU < 1% (as reported by top). Prior to the larger
>> CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is
>> this kind a transient behavior - by which I mean larger CPU loads after a
>> lot of filesystem activity in short time - typical? This is not a problem
>> in my deployment; I just want to know what to expect in the future and to
>> complete this thread for future users. If this is expected behavior we can
>> wrap up this thread. If not then I'll do more digging into the logs on the
>> client and brick sides.
>> 
>> 
>> 
>>            Thanks,
>> 
>>            ~Mike C.
>> 
>> 
>> 
>> From: Joe Julian [mailto:joe at julianfamily.org]
>> Sent: Friday, February 01, 2013 2:08 PM
>> To: Michael Colonno; gluster-users at gluster.org
>> Subject: Re: [Gluster-users] high CPU load on all bricks
>> 
>> 
>> 
>> Check the client log(s).
>> 
>> Michael Colonno <mcolonno at stanford.edu> wrote:
>> 
>>            Forgot to mention: on a client system (not a brick) the
>> glusterfs process is consuming ~ 68% CPU continuously. This is a much less
>> powerful desktop system so the CPU load can't be compared 1:1 with the
>> systems comprising the bricks but still very high. So the issue seems to
>> exist with both glusterfsd and glusterfs processes.
>> 
>> 
>> 
>>            Thanks,
>> 
>>            ~Mike C.
>> 
>> 
>> 
>> From: gluster-users-bounces at gluster.org
>> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>> Sent: Friday, February 01, 2013 12:46 PM
>> To: gluster-users at gluster.org
>> Subject: [Gluster-users] high CPU load on all bricks
>> 
>> 
>> 
>>            Gluster gurus ~
>> 
>> 
>> 
>>            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
>> CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
>> the volume. On each system contributing a brick, however, my CPU usage
>> (glusterfsd) is hovering around 20% (virtually zero memory usage
>> thankfully). These are brand new, fairly beefy servers so 20% CPU load is
>> quite a bit. The deployment is pretty plain with each brick mounting the
>> volume to itself via a glusterfs mount. I assume this type of CPU usage is
>> atypically high; is there anything I can do to investigate what's soaking up
>> CPU and minimize it? Total usable volume size is only about 22 TB (about 45
>> TB total with 2x replicate).
>> 
>> 
>> 
>>            Thanks,
>> 
>>            ~Mike C.
>> 
>> 
>> 
>> 
>>  _____
>> 
>> 
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> 
>> 
>> 
>> 
>> 
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
> ---
> Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
> [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
> 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
> MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
> ---
> "Something must be done. [X] is something. Therefore, we must do it."
> Bruce Schneier, on American response to just about anything.

From driver at megahappy.net  Thu Feb 14 20:31:58 2013
From: driver at megahappy.net (Bryan Whitehead)
Date: Thu, 14 Feb 2013 12:31:58 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <31EE9284-3079-429A-BCB0-C155F0D53717@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
	<85058672.lsbGZuViaP@stunted>
	<31EE9284-3079-429A-BCB0-C155F0D53717@stanford.edu>
Message-ID: <CAA3XVTwnv1jPaVwW6jDneXJLd7sj-k4KP8YqfmSdCunnFVHMpA@mail.gmail.com>

Yea, only write to the glusterfs mountpoint. Writing directly to the bricks
is bad and shouldn't be done.


On Thu, Feb 14, 2013 at 11:58 AM, Michael Colonno <mcolonno at stanford.edu>wrote:

> Good place to start: do the bricks have to be clients as well? In other
> words if I copy a file to a Gluster brick without going through a glusterfs
> or NFS mount will that disrupt the parallel file system? I assumed files
> need to be routed through a glusterfs mount point for Gluster to be able to
> track them(?) What's recommended for bricks which also need i/o to the
> entire volume?
>
> Thanks,
> Mike C.
>
> On Feb 14, 2013, at 10:28 AM, harry mangalam <harry.mangalam at uci.edu>
> wrote:
>
> > While I don't understand your 'each brick system also being a client'
> setup -
> > you mean that each gluster brick is a native gluster client as well?
>  And that
> > is where much of your gluster access is coming from?  That seems ..
> suboptimal
> > if that's the setup.  Is there a reason for that setup?
> >
> > We have a distributed-only glusterfs feeding a medium cluster over a
> similar
> > same setup QDR IPoIB with 4 servers with 2 bricks each.  On a fairly busy
> > system (~80MB/s background), I can get about 100-300MB/s writes to the
> gluster
> > fs on a large 1.7GB file.  (With tiny writes, the perf decreases
> > dramatically).
> >
> > Here is my config: (if anyone spies something that I should change to
> increase
> > my perf, please feel free to point out my mistake)
> >
> > gluster:
> > Volume Name: gl
> > Type: Distribute
> > Volume ID: 21f480f7-fc5a-4fd8-a084-3964634a9332
> > Status: Started
> > Number of Bricks: 8
> > Transport-type: tcp,rdma
> > Bricks:
> > Brick1: bs2:/raid1
> > Brick2: bs2:/raid2
> > Brick3: bs3:/raid1
> > Brick4: bs3:/raid2
> > Brick5: bs4:/raid1
> > Brick6: bs4:/raid2
> > Brick7: bs1:/raid1
> > Brick8: bs1:/raid2
> > Options Reconfigured:
> > performance.write-behind-window-size: 1024MB
> > performance.flush-behind: on
> > performance.cache-size: 268435456
> > nfs.disable: on
> > performance.io-cache: on
> > performance.quick-read: on
> > performance.io-thread-count: 64
> > auth.allow: 10.2.*.*,10.1.*.*
> >
> > my RAID6s (via 3ware 9750s) are mounted with the following options
> >
> > /dev/sdc /raid1 xfs rw,noatime,sunit=512,swidth=8192,allocsize=32m 0 0
> > /dev/sdd /raid2 xfs rw,noatime,sunit=512,swidth=7680,allocsize=32m 0 0
> > (and should probably be using 'nobarrier,inode64' as well. - testing
> this now)
> >
> > There are some good refs on prepping XFS fs for max perf here:
> > <http://www.mythtv.org/wiki/Optimizing_Performance#XFS-Specific_Tips>
> > The script at:
> > <http://www.mythtv.org/wiki/Optimizing_Performance#Further_Information>
> > can help to setup the sunit/swidth options.
> > <
> http://www.mysqlperformanceblog.com/2011/12/16/setting-up-xfs-the-simple-
> > edition/>
> > Your ib interfaces should be using large mtus (65536)
> >
> > hjm
> >
> > On Wednesday, February 13, 2013 10:35:12 PM Michael Colonno wrote:
> >>            More data: I got the Infiniband network (QDR) working well
> and
> >> switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA
> since
> >> it doesn't seem to be supported yet for 3.x). The filesystem was
> slightly
> >> faster but still well short of what I would expect by a wide margin.
> Via an
> >> informal test (timing the movement of a large file) I'm getting several
> MB/s
> >> - well short of even a standard Gb network copy. With the faster network
> >> the CPU load on the brick systems increased dramatically: now I'm seeing
> >> 200%-250% usage by glusterfsd and glusterfs.
> >>
> >>            This leads me to believe that gluster is really not enjoying
> my
> >> eight-brick, 2x replication volume with each brick system also being a
> >> client. I tried a rebalance but no measurable effect. Any suggestions
> for
> >> improving the performance? Having each brick be a client of itself
> seemed
> >> the most logical choice to remove interdependencies but now I'm
> doubting the
> >> setup.
> >>
> >>
> >>
> >>            Thanks,
> >>
> >>            ~Mike C.
> >>
> >>
> >>
> >> From: gluster-users-bounces at gluster.org
> >> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
> >> Sent: Sunday, February 03, 2013 11:47 AM
> >> To: gluster-users at gluster.org
> >> Subject: Re: [Gluster-users] high CPU load on all bricks
> >>
> >>
> >>
> >> On 02/03/2013 11:22 AM, Michael Colonno wrote:
> >>
> >>
> >>
> >>            Having taken a lot more data it does seem the glusterfsd and
> >> glusterd processes (along with several ksoftirqd) spike up to near 100%
> on
> >> both client and brick servers during any file transport across the
> mount.
> >> Thankfully this is short-lived for the most part but I'm wondering if
> this
> >> is expected behavior or what others have experienced(?) I'm a little
> >> surprised such a large CPU load would be required to move small files
> and /
> >> or use an application within a Gluster mount point.
> >>
> >>
> >> If you're getting ksoftirqd spikes, that sounds like a hardware issue
> to me.
> >> I never see huge spikes like that on my servers nor clients.
> >>
> >>
> >>
> >>
> >>
> >>
> >>            I wanted to test this against an NFS mount of the same
> Gluster
> >> volume. I managed to get rstatd installed and running but my attempts to
> >> mount the volume via NFS are met with:
> >>
> >>
> >>
> >>            mount.nfs: requested NFS version or transport protocol is not
> >> supported
> >>
> >>
> >>
> >>            Relevant line in /etc/fstab:
> >>
> >>
> >>
> >>            node1:/volume    /volume    nfs
> >> defaults,_netdev,vers=3,mountproto=tcp        0 0
> >>
> >>
> >>
> >> It looks like CentOS 6.x has NFS version 4 built into everything. So a
> few
> >> questions:
> >>
> >>
> >>
> >> -       Has anyone else noted significant performance differences
> between a
> >> glusterfs mount and NFS mount for volumes of 8+ bricks?
> >>
> >> -       Is there a straightforward way to make the newer versions of
> CentOS
> >> play nice with NFS version 3 + Gluster?
> >>
> >> -       Are there any general performance tuning guidelines I can
> follow to
> >> improve CPU performance? I found a few references to the cache settings
> but
> >> nothing solid.
> >>
> >>
> >>
> >> If the consensus is that NFS will not gain anything then I won't waste
> the
> >> time setting it all up.
> >>
> >>
> >> NFS gains you the use of FSCache to cache directories and file stats
> making
> >> directory listings faster, but it adds overhead decreasing the overall
> >> throughput (from all the reports I've seen).
> >>
> >> I would suspect that you have the kernel nfs server running on your
> servers.
> >> Make sure it's disabled.
> >>
> >>
> >>
> >>
> >>
> >>
> >> Thanks,
> >>
> >> ~Mike C.
> >>
> >>
> >>
> >>
> >>
> >> From: gluster-users-bounces at gluster.org
> >> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> >> Sent: Friday, February 01, 2013 4:46 PM
> >> To: gluster-users at gluster.org
> >> Subject: Re: [Gluster-users] high CPU load on all bricks
> >>
> >>
> >>
> >>            Update: after a few hours the CPU usage seems to have dropped
> >> down to a small value. I did not change anything with respect to the
> >> configuration or unmount / stop anything as I wanted to see if this
> would
> >> persist for a long period of time. Both the client and the self-mounted
> >> bricks are now showing CPU < 1% (as reported by top). Prior to the
> larger
> >> CPU loads I installed a bunch of software into the volume (~ 5 GB
> total). Is
> >> this kind a transient behavior - by which I mean larger CPU loads after
> a
> >> lot of filesystem activity in short time - typical? This is not a
> problem
> >> in my deployment; I just want to know what to expect in the future and
> to
> >> complete this thread for future users. If this is expected behavior we
> can
> >> wrap up this thread. If not then I'll do more digging into the logs on
> the
> >> client and brick sides.
> >>
> >>
> >>
> >>            Thanks,
> >>
> >>            ~Mike C.
> >>
> >>
> >>
> >> From: Joe Julian [mailto:joe at julianfamily.org]
> >> Sent: Friday, February 01, 2013 2:08 PM
> >> To: Michael Colonno; gluster-users at gluster.org
> >> Subject: Re: [Gluster-users] high CPU load on all bricks
> >>
> >>
> >>
> >> Check the client log(s).
> >>
> >> Michael Colonno <mcolonno at stanford.edu> wrote:
> >>
> >>            Forgot to mention: on a client system (not a brick) the
> >> glusterfs process is consuming ~ 68% CPU continuously. This is a much
> less
> >> powerful desktop system so the CPU load can't be compared 1:1 with the
> >> systems comprising the bricks but still very high. So the issue seems to
> >> exist with both glusterfsd and glusterfs processes.
> >>
> >>
> >>
> >>            Thanks,
> >>
> >>            ~Mike C.
> >>
> >>
> >>
> >> From: gluster-users-bounces at gluster.org
> >> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
> >> Sent: Friday, February 01, 2013 12:46 PM
> >> To: gluster-users at gluster.org
> >> Subject: [Gluster-users] high CPU load on all bricks
> >>
> >>
> >>
> >>            Gluster gurus ~
> >>
> >>
> >>
> >>            I've deployed and 8-brick (2x replicate) Gluster 3.3.1
> volume on
> >> CentOS 6.3 with tcp transport. I was able to build, start, mount, and
> use
> >> the volume. On each system contributing a brick, however, my CPU usage
> >> (glusterfsd) is hovering around 20% (virtually zero memory usage
> >> thankfully). These are brand new, fairly beefy servers so 20% CPU load
> is
> >> quite a bit. The deployment is pretty plain with each brick mounting the
> >> volume to itself via a glusterfs mount. I assume this type of CPU usage
> is
> >> atypically high; is there anything I can do to investigate what's
> soaking up
> >> CPU and minimize it? Total usable volume size is only about 22 TB
> (about 45
> >> TB total with 2x replicate).
> >>
> >>
> >>
> >>            Thanks,
> >>
> >>            ~Mike C.
> >>
> >>
> >>
> >>
> >>  _____
> >>
> >>
> >> Gluster-users mailing list
> >> Gluster-users at gluster.org
> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> >>
> >>
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> Gluster-users mailing list
> >> Gluster-users at gluster.org
> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> >
> > ---
> > Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
> > [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
> > 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
> > MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
> > ---
> > "Something must be done. [X] is something. Therefore, we must do it."
> > Bruce Schneier, on American response to just about anything.
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130214/62a39fdc/attachment-0001.html>

From mcolonno at stanford.edu  Thu Feb 14 21:08:22 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Thu, 14 Feb 2013 13:08:22 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <CAA3XVTwnv1jPaVwW6jDneXJLd7sj-k4KP8YqfmSdCunnFVHMpA@mail.gmail.com>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>
	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
	<85058672.lsbGZuViaP@stunted>
	<31EE9284-3079-429A-BCB0-C155F0D53717@stanford.edu>
	<CAA3XVTwnv1jPaVwW6jDneXJLd7sj-k4KP8YqfmSdCunnFVHMpA@mail.gmail.com>
Message-ID: <B2DAFBC8-39D3-4A65-8367-532B41AD8375@stanford.edu>

Ok, good - I had that right. In that case the bricks each need to mount the volume. I mounted the volume on each brick to itself (localhost) so they would not depend on each other re: mounting. 
But I am back to square one on the high CPU load / slow write speed issue. I'll try a test volume with a single node as brick and client to try any rule out anything network-related, though my network is performing well in every other way.

Thanks,
Mike C.

On Feb 14, 2013, at 12:31 PM, Bryan Whitehead <driver at megahappy.net> wrote:

> Yea, only write to the glusterfs mountpoint. Writing directly to the bricks is bad and shouldn't be done.
> 
> 
> On Thu, Feb 14, 2013 at 11:58 AM, Michael Colonno <mcolonno at stanford.edu> wrote:
>> Good place to start: do the bricks have to be clients as well? In other words if I copy a file to a Gluster brick without going through a glusterfs or NFS mount will that disrupt the parallel file system? I assumed files need to be routed through a glusterfs mount point for Gluster to be able to track them(?) What's recommended for bricks which also need i/o to the entire volume?
>> 
>> Thanks,
>> Mike C.
>> 
>> On Feb 14, 2013, at 10:28 AM, harry mangalam <harry.mangalam at uci.edu> wrote:
>> 
>> > While I don't understand your 'each brick system also being a client' setup -
>> > you mean that each gluster brick is a native gluster client as well?  And that
>> > is where much of your gluster access is coming from?  That seems .. suboptimal
>> > if that's the setup.  Is there a reason for that setup?
>> >
>> > We have a distributed-only glusterfs feeding a medium cluster over a similar
>> > same setup QDR IPoIB with 4 servers with 2 bricks each.  On a fairly busy
>> > system (~80MB/s background), I can get about 100-300MB/s writes to the gluster
>> > fs on a large 1.7GB file.  (With tiny writes, the perf decreases
>> > dramatically).
>> >
>> > Here is my config: (if anyone spies something that I should change to increase
>> > my perf, please feel free to point out my mistake)
>> >
>> > gluster:
>> > Volume Name: gl
>> > Type: Distribute
>> > Volume ID: 21f480f7-fc5a-4fd8-a084-3964634a9332
>> > Status: Started
>> > Number of Bricks: 8
>> > Transport-type: tcp,rdma
>> > Bricks:
>> > Brick1: bs2:/raid1
>> > Brick2: bs2:/raid2
>> > Brick3: bs3:/raid1
>> > Brick4: bs3:/raid2
>> > Brick5: bs4:/raid1
>> > Brick6: bs4:/raid2
>> > Brick7: bs1:/raid1
>> > Brick8: bs1:/raid2
>> > Options Reconfigured:
>> > performance.write-behind-window-size: 1024MB
>> > performance.flush-behind: on
>> > performance.cache-size: 268435456
>> > nfs.disable: on
>> > performance.io-cache: on
>> > performance.quick-read: on
>> > performance.io-thread-count: 64
>> > auth.allow: 10.2.*.*,10.1.*.*
>> >
>> > my RAID6s (via 3ware 9750s) are mounted with the following options
>> >
>> > /dev/sdc /raid1 xfs rw,noatime,sunit=512,swidth=8192,allocsize=32m 0 0
>> > /dev/sdd /raid2 xfs rw,noatime,sunit=512,swidth=7680,allocsize=32m 0 0
>> > (and should probably be using 'nobarrier,inode64' as well. - testing this now)
>> >
>> > There are some good refs on prepping XFS fs for max perf here:
>> > <http://www.mythtv.org/wiki/Optimizing_Performance#XFS-Specific_Tips>
>> > The script at:
>> > <http://www.mythtv.org/wiki/Optimizing_Performance#Further_Information>
>> > can help to setup the sunit/swidth options.
>> > <http://www.mysqlperformanceblog.com/2011/12/16/setting-up-xfs-the-simple-
>> > edition/>
>> > Your ib interfaces should be using large mtus (65536)
>> >
>> > hjm
>> >
>> > On Wednesday, February 13, 2013 10:35:12 PM Michael Colonno wrote:
>> >>            More data: I got the Infiniband network (QDR) working well and
>> >> switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since
>> >> it doesn't seem to be supported yet for 3.x). The filesystem was slightly
>> >> faster but still well short of what I would expect by a wide margin. Via an
>> >> informal test (timing the movement of a large file) I'm getting several MB/s
>> >> - well short of even a standard Gb network copy. With the faster network
>> >> the CPU load on the brick systems increased dramatically: now I'm seeing
>> >> 200%-250% usage by glusterfsd and glusterfs.
>> >>
>> >>            This leads me to believe that gluster is really not enjoying my
>> >> eight-brick, 2x replication volume with each brick system also being a
>> >> client. I tried a rebalance but no measurable effect. Any suggestions for
>> >> improving the performance? Having each brick be a client of itself seemed
>> >> the most logical choice to remove interdependencies but now I'm doubting the
>> >> setup.
>> >>
>> >>
>> >>
>> >>            Thanks,
>> >>
>> >>            ~Mike C.
>> >>
>> >>
>> >>
>> >> From: gluster-users-bounces at gluster.org
>> >> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
>> >> Sent: Sunday, February 03, 2013 11:47 AM
>> >> To: gluster-users at gluster.org
>> >> Subject: Re: [Gluster-users] high CPU load on all bricks
>> >>
>> >>
>> >>
>> >> On 02/03/2013 11:22 AM, Michael Colonno wrote:
>> >>
>> >>
>> >>
>> >>            Having taken a lot more data it does seem the glusterfsd and
>> >> glusterd processes (along with several ksoftirqd) spike up to near 100% on
>> >> both client and brick servers during any file transport across the mount.
>> >> Thankfully this is short-lived for the most part but I'm wondering if this
>> >> is expected behavior or what others have experienced(?) I'm a little
>> >> surprised such a large CPU load would be required to move small files and /
>> >> or use an application within a Gluster mount point.
>> >>
>> >>
>> >> If you're getting ksoftirqd spikes, that sounds like a hardware issue to me.
>> >> I never see huge spikes like that on my servers nor clients.
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>            I wanted to test this against an NFS mount of the same Gluster
>> >> volume. I managed to get rstatd installed and running but my attempts to
>> >> mount the volume via NFS are met with:
>> >>
>> >>
>> >>
>> >>            mount.nfs: requested NFS version or transport protocol is not
>> >> supported
>> >>
>> >>
>> >>
>> >>            Relevant line in /etc/fstab:
>> >>
>> >>
>> >>
>> >>            node1:/volume    /volume    nfs
>> >> defaults,_netdev,vers=3,mountproto=tcp        0 0
>> >>
>> >>
>> >>
>> >> It looks like CentOS 6.x has NFS version 4 built into everything. So a few
>> >> questions:
>> >>
>> >>
>> >>
>> >> -       Has anyone else noted significant performance differences between a
>> >> glusterfs mount and NFS mount for volumes of 8+ bricks?
>> >>
>> >> -       Is there a straightforward way to make the newer versions of CentOS
>> >> play nice with NFS version 3 + Gluster?
>> >>
>> >> -       Are there any general performance tuning guidelines I can follow to
>> >> improve CPU performance? I found a few references to the cache settings but
>> >> nothing solid.
>> >>
>> >>
>> >>
>> >> If the consensus is that NFS will not gain anything then I won't waste the
>> >> time setting it all up.
>> >>
>> >>
>> >> NFS gains you the use of FSCache to cache directories and file stats making
>> >> directory listings faster, but it adds overhead decreasing the overall
>> >> throughput (from all the reports I've seen).
>> >>
>> >> I would suspect that you have the kernel nfs server running on your servers.
>> >> Make sure it's disabled.
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> Thanks,
>> >>
>> >> ~Mike C.
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> From: gluster-users-bounces at gluster.org
>> >> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>> >> Sent: Friday, February 01, 2013 4:46 PM
>> >> To: gluster-users at gluster.org
>> >> Subject: Re: [Gluster-users] high CPU load on all bricks
>> >>
>> >>
>> >>
>> >>            Update: after a few hours the CPU usage seems to have dropped
>> >> down to a small value. I did not change anything with respect to the
>> >> configuration or unmount / stop anything as I wanted to see if this would
>> >> persist for a long period of time. Both the client and the self-mounted
>> >> bricks are now showing CPU < 1% (as reported by top). Prior to the larger
>> >> CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is
>> >> this kind a transient behavior - by which I mean larger CPU loads after a
>> >> lot of filesystem activity in short time - typical? This is not a problem
>> >> in my deployment; I just want to know what to expect in the future and to
>> >> complete this thread for future users. If this is expected behavior we can
>> >> wrap up this thread. If not then I'll do more digging into the logs on the
>> >> client and brick sides.
>> >>
>> >>
>> >>
>> >>            Thanks,
>> >>
>> >>            ~Mike C.
>> >>
>> >>
>> >>
>> >> From: Joe Julian [mailto:joe at julianfamily.org]
>> >> Sent: Friday, February 01, 2013 2:08 PM
>> >> To: Michael Colonno; gluster-users at gluster.org
>> >> Subject: Re: [Gluster-users] high CPU load on all bricks
>> >>
>> >>
>> >>
>> >> Check the client log(s).
>> >>
>> >> Michael Colonno <mcolonno at stanford.edu> wrote:
>> >>
>> >>            Forgot to mention: on a client system (not a brick) the
>> >> glusterfs process is consuming ~ 68% CPU continuously. This is a much less
>> >> powerful desktop system so the CPU load can't be compared 1:1 with the
>> >> systems comprising the bricks but still very high. So the issue seems to
>> >> exist with both glusterfsd and glusterfs processes.
>> >>
>> >>
>> >>
>> >>            Thanks,
>> >>
>> >>            ~Mike C.
>> >>
>> >>
>> >>
>> >> From: gluster-users-bounces at gluster.org
>> >> [mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
>> >> Sent: Friday, February 01, 2013 12:46 PM
>> >> To: gluster-users at gluster.org
>> >> Subject: [Gluster-users] high CPU load on all bricks
>> >>
>> >>
>> >>
>> >>            Gluster gurus ~
>> >>
>> >>
>> >>
>> >>            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
>> >> CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
>> >> the volume. On each system contributing a brick, however, my CPU usage
>> >> (glusterfsd) is hovering around 20% (virtually zero memory usage
>> >> thankfully). These are brand new, fairly beefy servers so 20% CPU load is
>> >> quite a bit. The deployment is pretty plain with each brick mounting the
>> >> volume to itself via a glusterfs mount. I assume this type of CPU usage is
>> >> atypically high; is there anything I can do to investigate what's soaking up
>> >> CPU and minimize it? Total usable volume size is only about 22 TB (about 45
>> >> TB total with 2x replicate).
>> >>
>> >>
>> >>
>> >>            Thanks,
>> >>
>> >>            ~Mike C.
>> >>
>> >>
>> >>
>> >>
>> >>  _____
>> >>
>> >>
>> >> Gluster-users mailing list
>> >> Gluster-users at gluster.org
>> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> >>
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> _______________________________________________
>> >> Gluster-users mailing list
>> >> Gluster-users at gluster.org
>> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> >
>> > ---
>> > Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
>> > [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
>> > 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
>> > MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
>> > ---
>> > "Something must be done. [X] is something. Therefore, we must do it."
>> > Bruce Schneier, on American response to just about anything.
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130214/77892de3/attachment-0001.html>

From michele at unipex.it  Fri Feb 15 09:07:24 2013
From: michele at unipex.it (mpe)
Date: Fri, 15 Feb 2013 10:07:24 +0100
Subject: [Gluster-users] HA with CTDB and NFS or CIFS
In-Reply-To: <CAMUFnfabAzS=-zX5-4x1zaGoW80EMnFGU7vBRGRXkOWZsKPdgA@mail.gmail.com>
References: <CAMUFnfabAzS=-zX5-4x1zaGoW80EMnFGU7vBRGRXkOWZsKPdgA@mail.gmail.com>
Message-ID: <kfkts9$sbq$1@ger.gmane.org>

On 14/02/2013 16:36, John Mark Walker wrote:
> If you're interested in setting up high availability with NFS or SMB,
> you may be interested in this webinar:
>
> https://vts.inxpo.com/scripts/Server.nxp
>

"""
PAGE ERROR c1-6

Sorry but it appears you are having an issue accessing this page.

Please try to refresh your screen by hitting (CTRL-R) for PC, 
(Command-R) for MAC, or the refresh button in your browser. If the 
problem continues to exist, please contact your system administrator or 
click on the help button within the virtual event if it is available.
"""

:(

Thanks,
Michele




From nux at li.nux.ro  Fri Feb 15 09:40:35 2013
From: nux at li.nux.ro (Nux!)
Date: Fri, 15 Feb 2013 09:40:35 +0000
Subject: [Gluster-users] HA with CTDB and NFS or CIFS
In-Reply-To: <kfkts9$sbq$1@ger.gmane.org>
References: <CAMUFnfabAzS=-zX5-4x1zaGoW80EMnFGU7vBRGRXkOWZsKPdgA@mail.gmail.com>
	<kfkts9$sbq$1@ger.gmane.org>
Message-ID: <b529148fa4c70a6585e5fffeddc47aee@li.nux.ro>

On 15.02.2013 09:07, mpe wrote:
> On 14/02/2013 16:36, John Mark Walker wrote:
>> If you're interested in setting up high availability with NFS or SMB,
>> you may be interested in this webinar:
>> 
>> https://vts.inxpo.com/scripts/Server.nxp
>> 
> 
> """
> PAGE ERROR c1-6
> 
> Sorry but it appears you are having an issue accessing this page.
> 
> Please try to refresh your screen by hitting (CTRL-R) for PC,
> (Command-R) for MAC, or the refresh button in your browser. If the
> problem continues to exist, please contact your system administrator
> or click on the help button within the virtual event if it is
> available.


The "NotLoggedIn" in the resulting URL hits to needing a registered 
user.

-- 
Sent from the Delta quadrant using Borg technology!

Nux!
www.nux.ro

From johnmark at redhat.com  Fri Feb 15 12:37:35 2013
From: johnmark at redhat.com (John Mark Walker)
Date: Fri, 15 Feb 2013 07:37:35 -0500 (EST)
Subject: [Gluster-users] HA with CTDB and NFS or CIFS
Message-ID: <4043wixhqqa34rfykyk0erqy.1360931850975@email.android.com>

Apologies - I thought this was open to the public, but it's for internal training purposes. Sorry for the confusion.

-JM


mpe <michele at unipex.it> wrote:

On 14/02/2013 16:36, John Mark Walker wrote:
> If you're interested in setting up high availability with NFS or SMB,
> you may be interested in this webinar:
>
> https://vts.inxpo.com/scripts/Server.nxp
>

"""
PAGE ERROR c1-6

Sorry but it appears you are having an issue accessing this page.

Please try to refresh your screen by hitting (CTRL-R) for PC, 
(Command-R) for MAC, or the refresh button in your browser. If the 
problem continues to exist, please contact your system administrator or 
click on the help button within the virtual event if it is available.
"""

:(

Thanks,
Michele



_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

From tom.bean at changingworkplace.com  Fri Feb 15 22:25:58 2013
From: tom.bean at changingworkplace.com (Tom Bean)
Date: Fri, 15 Feb 2013 22:25:58 -0000
Subject: [Gluster-users] Gluster Issues with ISCSI backend
Message-ID: <000001ce0bcb$6de42d20$49ac8760$@changingworkplace.com>

Hi All,

 

I have the following setup:

 

-       2x CentOS 6.3 servers with Gluster 3.2

-       I then have a MD3200i which has volumes mounted to both gluster
servers using iSCSI to give additional space.

-       The gluster server is then mounted as a primary SR to my Xen pool
and is used for hosting VHD's.

 

I am getting fantastic performance from this setup, however recently I have
started to face a few issues. Twice recently I have been created a new VM
using a template, which involves creating a new file on the gluster nfs
share. When this happens, a few of my servers in the Xen Pool loose
connection to the gluster share and are unable to read/write the heart beat
file which is stored on the share. When this happens, the Xen servers fence
(which basically means they shut themselves down, including any VM's running
on them).

 

Does anyone know if there is a reason why this might happen? Does Gluster
ever put a file lock on everything/anything in the directory, especially
during times of heavy write?

 

There is no geo-replication currently running on this setup.

 

Thanks for any input.

 


Tom Bean

 


Infrastructure & Support Analyst

 


The Changing Workplace


Phone: 

Fax: 

Email: 

Skype: 

Address: 

Website: 

News:

+44 (0)1444 44 1000 ext 316

+44 (0)1444 44 0944

 <mailto:tom.bean at changingworkplace.com> tom.bean at changingworkplace.com

Contact me via  <skype:thomas.bean.cwp?chat> Skype 

1 Boltro Road, Haywards Heath, RH16 1BY, United Kingdom

 <http://www.changingworkplace.com/> changingworkplace.com

Sign up for
<http://visitor.r20.constantcontact.com/d.jsp?llr=wbsdkygab&p=oi&m=110676998
7638> our newsletter


 <https://twitter.com/The_CWP> Description:
C:\Users\tomb\AppData\Roaming\Microsoft\Signatures\Tom
Bean_files\image001.gif <http://facebook.com/TheChangingWorkplace>
Description: C:\Users\tomb\AppData\Roaming\Microsoft\Signatures\Tom
Bean_files\image002.gif
<http://www.linkedin.com/company/the-changing-workplace> Description:
C:\Users\tomb\AppData\Roaming\Microsoft\Signatures\Tom
Bean_files\image003.gif


Changing the world we work in by providing best-in-class software services
to the Corporate Real Estate sector


Please don't print this e-mail unless you really need to

			

 

 



Important Information

This e-mail and the information it contains are confidential and/or privileged and are intended solely for the named recipient/s. If you have received this e-mail in error please notify the sender immediately and delete this email. You should not copy it for any purpose, or disclose its contents to any other person.

The Changing Workplace cannot accept any responsibility for the accuracy or completeness of the contents of this e-mail as it has been transmitted over a public network and Internet communications are not secure.

The views expressed in this message are those of the individual sender, except where specifically stated to be the view of The Changing Workplace. All statements made in this e-mail are subject to contract. The contents are not to be regarded as a contractual offer or acceptance. The sender is not authorised to bind The Changing Workplace contractually.

The Changing Workplace cannot guarantee that attachments are virus-free or compatible with your systems and does not accept any liability in respect of viruses or computer problems experienced.

The Changing Workplace is a limited company registered in England and Wales under company number 3614433 whose registered office is at 1 Boltro Road, Haywards Heath, West Sussex, RH16 1BY, United Kingdom.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130215/c30007eb/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 1360 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130215/c30007eb/attachment-0003.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.gif
Type: image/gif
Size: 1264 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130215/c30007eb/attachment-0004.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.gif
Type: image/gif
Size: 1355 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130215/c30007eb/attachment-0005.gif>

From mcolonno at stanford.edu  Sat Feb 16 18:02:21 2013
From: mcolonno at stanford.edu (Michael Colonno)
Date: Sat, 16 Feb 2013 10:02:21 -0800
Subject: [Gluster-users] high CPU load on all bricks
In-Reply-To: <003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
References: <017601ce00bd$2d4bbbc0$87e33340$@stanford.edu>	<019901ce00be$2d787ba0$886972e0$@stanford.edu>	<33aca106-2ca5-4b2c-87cf-9f3745bc4f1e@email.android.com>	<00d301ce00de$ad920250$08b606f0$@stanford.edu>	<015101ce0243$d464c000$7d2e4000$@stanford.edu>	<510EBE98.9000807@julianfamily.org>
	<003901ce0a7d$71fc6840$55f538c0$@stanford.edu>
Message-ID: <028d01ce0c6f$c4ce4a40$4e6adec0$@stanford.edu>

            I ran a series of iozone tests on three configurations: 1)
reading / writing directly to the local disk on a brick system, 2) reading /
writing over the glusterfs mount on a brick system, and 3) reading / writing
over an independent NFS mount between two of the brick systems. Here's the
data for test file sizes of 10 MB, 100 MB, 1,000 MB and 2,000 MB
respectively:

 

Writing (GB/s):

 

Local Drives                 GlusterFS                     NFS client

0.677799225                  0.028595924                  0.793493271

1.122471809                  0.028735161                  1.176805496

1.209827423                  0.041208267                  1.559321404

1.152087212                  0.043795586                  1.448302269

 

Reading (GB/s):

 

Local Drives                 GlusterFS                     NFS client

3.12509346                   0.184482574                  2.989168167

4.549301147                  0.191487312                  3.524992943

5.532466888                  0.194747925                  5.034347534

5.111581802                  0.199500084                  4.540629387

 

            The NFS results (some of which are faster than local i/o due
most likely to caching in memory) and local drive results clearly rule out
the drives and the network (QDR Infiniband) as bottlenecks. It's also clear
that something is going very awry on the glusterfs side of things. I'm not
sure what more I can do in terms of testing and analysis; is there any fix
or will I need to abandon this gluster deployment and chalk it up to an
unknown error?

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Wednesday, February 13, 2013 10:35 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

            More data: I got the Infiniband network (QDR) working well and
switched my gluster volume to the Infiniband fabric (IPoIB, not RDMA since
it doesn't seem to be supported yet for 3.x). The filesystem was slightly
faster but still well short of what I would expect by a wide margin. Via an
informal test (timing the movement of a large file) I'm getting several MB/s
- well short of even a standard Gb network copy. With the faster network the
CPU load on the brick systems increased dramatically: now I'm seeing
200%-250% usage by glusterfsd and glusterfs. 

            This leads me to believe that gluster is really not enjoying my
eight-brick, 2x replication volume with each brick system also being a
client. I tried a rebalance but no measurable effect. Any suggestions for
improving the performance? Having each brick be a client of itself seemed
the most logical choice to remove interdependencies but now I'm doubting the
setup.

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Joe Julian
Sent: Sunday, February 03, 2013 11:47 AM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

On 02/03/2013 11:22 AM, Michael Colonno wrote:

            Having taken a lot more data it does seem the glusterfsd and
glusterd processes (along with several ksoftirqd) spike up to near 100% on
both client and brick servers during any file transport across the mount.
Thankfully this is short-lived for the most part but I'm wondering if this
is expected behavior or what others have experienced(?) I'm a little
surprised such a large CPU load would be required to move small files and /
or use an application within a Gluster mount point. 


If you're getting ksoftirqd spikes, that sounds like a hardware issue to me.
I never see huge spikes like that on my servers nor clients.



 

            I wanted to test this against an NFS mount of the same Gluster
volume. I managed to get rstatd installed and running but my attempts to
mount the volume via NFS are met with: 

 

            mount.nfs: requested NFS version or transport protocol is not
supported

 

            Relevant line in /etc/fstab:

 

            node1:/volume    /volume    nfs
defaults,_netdev,vers=3,mountproto=tcp        0 0      

 

It looks like CentOS 6.x has NFS version 4 built into everything. So a few
questions:

 

-       Has anyone else noted significant performance differences between a
glusterfs mount and NFS mount for volumes of 8+ bricks? 

-       Is there a straightforward way to make the newer versions of CentOS
play nice with NFS version 3 + Gluster? 

-       Are there any general performance tuning guidelines I can follow to
improve CPU performance? I found a few references to the cache settings but
nothing solid. 

 

If the consensus is that NFS will not gain anything then I won't waste the
time setting it all up. 


NFS gains you the use of FSCache to cache directories and file stats making
directory listings faster, but it adds overhead decreasing the overall
throughput (from all the reports I've seen).

I would suspect that you have the kernel nfs server running on your servers.
Make sure it's disabled.



 

Thanks,

~Mike C. 

 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 4:46 PM
To: gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

            Update: after a few hours the CPU usage seems to have dropped
down to a small value. I did not change anything with respect to the
configuration or unmount / stop anything as I wanted to see if this would
persist for a long period of time. Both the client and the self-mounted
bricks are now showing CPU < 1% (as reported by top). Prior to the larger
CPU loads I installed a bunch of software into the volume (~ 5 GB total). Is
this kind a transient behavior - by which I mean larger CPU loads after a
lot of filesystem activity in short time - typical? This is not a problem in
my deployment; I just want to know what to expect in the future and to
complete this thread for future users. If this is expected behavior we can
wrap up this thread. If not then I'll do more digging into the logs on the
client and brick sides. 

 

            Thanks,

            ~Mike C. 

 

From: Joe Julian [mailto:joe at julianfamily.org] 
Sent: Friday, February 01, 2013 2:08 PM
To: Michael Colonno; gluster-users at gluster.org
Subject: Re: [Gluster-users] high CPU load on all bricks

 

Check the client log(s). 

Michael Colonno <mcolonno at stanford.edu> wrote:

            Forgot to mention: on a client system (not a brick) the
glusterfs process is consuming ~ 68% CPU continuously. This is a much less
powerful desktop system so the CPU load can't be compared 1:1 with the
systems comprising the bricks but still very high. So the issue seems to
exist with both glusterfsd and glusterfs processes. 

 

            Thanks,

            ~Mike C. 

 

From: gluster-users-bounces at gluster.org
[mailto:gluster-users-bounces at gluster.org] On Behalf Of Michael Colonno
Sent: Friday, February 01, 2013 12:46 PM
To: gluster-users at gluster.org
Subject: [Gluster-users] high CPU load on all bricks

 

            Gluster gurus ~

 

            I've deployed and 8-brick (2x replicate) Gluster 3.3.1 volume on
CentOS 6.3 with tcp transport. I was able to build, start, mount, and use
the volume. On each system contributing a brick, however, my CPU usage
(glusterfsd) is hovering around 20% (virtually zero memory usage
thankfully). These are brand new, fairly beefy servers so 20% CPU load is
quite a bit. The deployment is pretty plain with each brick mounting the
volume to itself via a glusterfs mount. I assume this type of CPU usage is
atypically high; is there anything I can do to investigate what's soaking up
CPU and minimize it? Total usable volume size is only about 22 TB (about 45
TB total with 2x replicate). 

 

            Thanks,

            ~Mike C. 

 


  _____  

 
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users





_______________________________________________
Gluster-users mailing list
Gluster-users at gluster.org
http://supercolony.gluster.org/mailman/listinfo/gluster-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130216/3bbd73e5/attachment-0001.html>

From huangql at ihep.ac.cn  Mon Feb 18 16:06:14 2013
From: huangql at ihep.ac.cn (huangql)
Date: Tue, 19 Feb 2013 00:06:14 +0800
Subject: [Gluster-users] missing some directories or files
Message-ID: <201302190006142502315@ihep.ac.cn>


Dear users,

        We got a strange phenomenon that some files or directories disappeared for hours or minutes. However, the files and directories exist actually.

So, sometimes we got abonomal results or even got nothing when "ls " some directory which contain many child directory and files.

It looks like this:

?????????? ? ?    ?      ?                ? 20120110
?????????? ? ?    ?      ?                ? 20120111?????????? ? ?    ?      ?                ? 20120112or total 0(it seems like an empty directory, actually, it was not)
And server side got many error messages as following:
[2013-02-18 23:46:28.999245] W [inode.c:1044:inode_path] (server_resolve_entry+0x6a) [0x2aaaabec7afa] 
(resolve_loc_touchup+0xb5) [0x2aaaabec7615]))) 0-/data1/gdata1/inode: no dentry for non-root inode -879461454: dfffdd16-54a3-46f7-8284-c675490fb53d
[2013-02-18 23:46:29.4964] W [inode.c:1044:inode_path] (server_resolve+0x98) [0x2aaaabec7ba8] (server_resolve_entry+0x6a) [0x2aaaabec7afa] 
(resolve_loc_touchup+0xb5) [0x2aaaabec7615]))) 0-/data1/gdata1/inode: no dentry for non-root inode -879461454: dfffdd16-54a3-46f7-8284-c675490fb53d
[2013-02-18 23:46:29.5433] W [inode.c:1044:inode_path] (server_resolve+0x98) [0x2aaaabec7ba8] (server_resolve_entry+0x6a) [0x2aaaabec7afa] (resolve_loc_touchup+0xb5) 
[0x2aaaabec7615]))) 0-/data1/gdata1/inode: no dentry for non-root inode -879461454: dfffdd16-54a3-46f7-8284-c675490fb53d
[2013-02-18 23:46:29.10817] W [inode.c:1044:inode_path] (server_resolve+0x98) [0x2aaaabec7ba8] (server_resolve_entry+0x6a) 
[0x2aaaabec7afa] (resolve_loc_touchup)

Each time I restarted brick daemons on server side, then the directory recovery. But the problem reproduce again when accessing it. And I found
the problem will be fixed itself sometimes. For these strange behavior, I have no idea. Does anyone get this? Could you give us some tips?
Thank you very much in advance.

Cheers,
Qiulan 

2013-02-18 



huangql 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130219/d2f6c994/attachment.html>

From ruben.malchow at googlemail.com  Mon Feb 18 18:33:17 2013
From: ruben.malchow at googlemail.com (ruben malchow)
Date: Mon, 18 Feb 2013 19:33:17 +0100
Subject: [Gluster-users] architecture question
Message-ID: <C4969608-AAB4-4CA9-9380-EF5D2EB4E7C8@gmail.com>



hi everyone,

i am having a few difficulties understanding some of the concepts - especially (i think) because i am not always sure wether i am reading 3.2 docs or 3.3 ?

the situation is this: we are doing an evaluation for a rather large system that will be used to store video data. we will need around 500TByte of storage and a sustained
throughput of somewhere north of 3GByte/s. we are in the process of setting up an initial, scaled-down version of this.

the current setup is this: 

we have 10 rather stupid storage nodes with 4 bricks and a single 1GBE connection, and another, more powerful machine with a 10GBE interface on the same network as the other nodes and another 10GBE interface on the same network as the clients. 

the idea was to have gluster stripe & replicate across these 2x (replicate) 20x (stripe) bricks, then have a client running on the big machine that exports the volume to the actual client machines. 

i have included the current config below, but i think i am missing something fundamentally ? ? and, maybe i am getting the terminology and lines dividing cluster, server and client wrong?

any help would be appreciated!

thank you!

.rm


# local bricks, 4 of these:
volume brick1
 type storage/posix
 option directory /media/brick1
end-volume

[?]

volume brickN
 type storage/posix
 option directory /media/brickN
end-volume

# 40 of these:
volume gv0-brick-$HOST_NUMBER-$BRICK_NUMBER
   type protocol/client
   option remote-host 10.25.178.$HOST_IP
   option remote-subvolume $BRICK_ON_HOST  # (i.e. brick1)
   option transport-type tcp
end-volume

#  stripe 1
volume gv0-stripe-01
 type cluster/stripe
 option block-size 1MB
 subvolumes gv0-brick-01-01 gv0-brick-02-01 [?]
end-volume

#  stripe 1
volume gv0-stripe-02
 type cluster/stripe
 option block-size 1MB
 subvolumes gv0-brick-05-01 gv0-brick-06-01 [?]
end-volume

#  replicate
volume gv0
   type cluster/replicate
   option background-self-heal-count 0
   option metadata-self-heal on
   option data-self-heal on
   option entry-self-heal on
   option self-heal-daemon on
   option iam-self-heal-daemon yes
   subvolumes gv0-stripe-01 gv0-stripe-02
end-volume

volume server
 type protocol/server
 option transport-type tcp/server
 option auth.addr.gv0.allow 10.25.178.*,127.0.0.1
 subvolumes gv0
end-volume








From douglas.colkitt at gmail.com  Mon Feb 18 19:29:19 2013
From: douglas.colkitt at gmail.com (Douglas Colkitt)
Date: Mon, 18 Feb 2013 14:29:19 -0500
Subject: [Gluster-users] Directory metadata inconsistencies and missing
 output ("mismatched layout" and "no dentry for inode" error)
Message-ID: <CADen9wHuacmRZKn3ffkbw+M7Bomt=o6dZeUOotr-BchhAmmd6A@mail.gmail.com>

Hi I'm running into a rather strange and frustrating bug and wondering if
anyone on the mailing list might have some insight about what might be
causing it. I'm running a cluster of two dozen nodes, where the processing
nodes are also the gluster bricks (using the SLURM resource manager). Each
node has the glusters mounted natively (not NFS). All nodes are using
v3.2.7. Each job in the node runs a shell script like so:

containerDir=$1
groupNum=$2
mkdir -p $containerDir
./generateGroupGen.py $groupNum >$containerDir/$groupNum.out

Then run the following jobs:

runGroupGen [glusterDirectory] 1
runGroupGen [glusterDirectory] 2
runGroupGen [glusterDirectory] 3
...

Typically about 200 jobs launch within milliseconds of each other so the
glusterfs/fuse directory system receives a large number of simultaneous
create directory and create file system calls within a very short time.

Some of the output files inside the directory have a file that exists but
no output. When this occurs it is always the case that either all jobs on a
node behave normally or all fail to produce output. It should be noted that
there are no error messages generated by the processes themselves, and all
processes on the no-output node return with no error code. In that sense
the failure is silent, but corrupts the data, which is dangerous. The only
indication of error are errors (on the no output nodes) in the
/var/log/distrib-glusterfs.log of the form:

[2013-02-18 05:55:31.382279] E [client3_1-fops.c:2228:client3_1_lookup_cbk]
0-volume1-client-16: remote operation failed: Stale NFS file handle
[2013-02-18 05:55:31.382302] E [client3_1-fops.c:2228:client3_1_lookup_cbk]
0-volume1-client-17: remote operation failed: Stale NFS file handle
[2013-02-18 05:55:31.382327] E [client3_1-fops.c:2228:client3_1_lookup_cbk]
0-volume1-client-18: remote operation failed: Stale NFS file handle
[2013-02-18 05:55:31.640791] W [inode.c:1044:inode_path]
(-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xe8fd) [0x7fa8341868fd]
(-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xa6bb) [0x7fa8341826bb]
(-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(fuse_loc_fill+0x1c6)
[0x7fa83417d156]))) 0-volume1/inode: no dentry for non-root inode
-69777006931: 0a37836d-e9e5-4cc1-8bd2-e8a49947959b
[2013-02-18 05:55:31.640865] W [fuse-bridge.c:561:fuse_getattr]
0-glusterfs-fuse: 2298073: GETATTR 140360215569520 (fuse_loc_fill() failed)
[2013-02-18 05:55:31.641672] W [inode.c:1044:inode_path]
(-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xe8fd) [0x7fa8341868fd]
(-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xa6bb) [0x7fa8341826bb]
(-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(fuse_loc_fill+0x1c6)
[0x7fa83417d156]))) 0-volume1/inode: no dentry for non-root inode
-69777006931: 0a37836d-e9e5-4cc1-8bd2-e8a49947959b
[2013-02-18 05:55:31.641724] W [fuse-bridge.c:561:fuse_getattr]
0-glusterfs-fuse: 2298079: GETATTR 140360215569520 (fuse_loc_fill() failed)
...

Sometimes on these events, and sometimes not, there will also be logs (on
both normal and abnormal nodes) of the form:

[2013-02-18 03:35:28.679681] I [dht-common.c:525:dht_revalidate_cbk]
0-volume1-dht: mismatching layouts for /inSample/pred/20110831

I understand from reading the mailing list that both the dentry errors and
the mismatched layout errors are both non-fatal warnings and that the
metadata will become internally consistent regardless. But these errors
only happen on times when I'm slamming the glusterfs system with the
creation of a bunch of small files in a very short burst like I described
above. So their presence seems to be related to the error.

I think the issue is almost assuredly related to the delayed propagation of
glusterfs directory metadata. Some nodes are creating directory
simultaneous to other nodes and the two are producing inconsistencies with
regards to the dht layout information. My hypothesis is that when Node A is
still writing that the process to resolve the inconsistencies with and
propagate the metadata from Node B is rendering the location that Node A is
writing to disconnected from its supposed path. (And hence the no dentry
errors).

I've taken some effort to go through the glusterfs source code,
particularly the dht related files. The way dht normalizes anomalies could
be the problem, but I've failed to find anything specific.

Has anyone else run into a problem like this, or have insight about what
might be causing it or how to avoid it?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130218/d2daa165/attachment.html>

From anand.avati at gmail.com  Mon Feb 18 19:54:14 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Mon, 18 Feb 2013 11:54:14 -0800
Subject: [Gluster-users] Directory metadata inconsistencies and missing
 output ("mismatched layout" and "no dentry for inode" error)
In-Reply-To: <CADen9wHuacmRZKn3ffkbw+M7Bomt=o6dZeUOotr-BchhAmmd6A@mail.gmail.com>
References: <CADen9wHuacmRZKn3ffkbw+M7Bomt=o6dZeUOotr-BchhAmmd6A@mail.gmail.com>
Message-ID: <CAFboF2wJM1021oPWX2+D4R1ny7nZUFbQ+dSou02C3PyitPKcow@mail.gmail.com>

A similar issue was fixed in the master branch recently. Can you apply
http://review.gluster.org/4459 to your source / rebuild / retest and see if
the issue gets fixed for you? It is quite a trivial patch and might even
just apply on 3.2.7 source.

Avati


On Mon, Feb 18, 2013 at 11:29 AM, Douglas Colkitt <douglas.colkitt at gmail.com
> wrote:

> Hi I'm running into a rather strange and frustrating bug and wondering if
> anyone on the mailing list might have some insight about what might be
> causing it. I'm running a cluster of two dozen nodes, where the processing
> nodes are also the gluster bricks (using the SLURM resource manager). Each
> node has the glusters mounted natively (not NFS). All nodes are using
> v3.2.7. Each job in the node runs a shell script like so:
>
> containerDir=$1
> groupNum=$2
> mkdir -p $containerDir
> ./generateGroupGen.py $groupNum >$containerDir/$groupNum.out
>
> Then run the following jobs:
>
> runGroupGen [glusterDirectory] 1
> runGroupGen [glusterDirectory] 2
> runGroupGen [glusterDirectory] 3
> ...
>
> Typically about 200 jobs launch within milliseconds of each other so the
> glusterfs/fuse directory system receives a large number of simultaneous
> create directory and create file system calls within a very short time.
>
> Some of the output files inside the directory have a file that exists but
> no output. When this occurs it is always the case that either all jobs on a
> node behave normally or all fail to produce output. It should be noted that
> there are no error messages generated by the processes themselves, and all
> processes on the no-output node return with no error code. In that sense
> the failure is silent, but corrupts the data, which is dangerous. The only
> indication of error are errors (on the no output nodes) in the
> /var/log/distrib-glusterfs.log of the form:
>
> [2013-02-18 05:55:31.382279] E
> [client3_1-fops.c:2228:client3_1_lookup_cbk] 0-volume1-client-16: remote
> operation failed: Stale NFS file handle
> [2013-02-18 05:55:31.382302] E
> [client3_1-fops.c:2228:client3_1_lookup_cbk] 0-volume1-client-17: remote
> operation failed: Stale NFS file handle
> [2013-02-18 05:55:31.382327] E
> [client3_1-fops.c:2228:client3_1_lookup_cbk] 0-volume1-client-18: remote
> operation failed: Stale NFS file handle
> [2013-02-18 05:55:31.640791] W [inode.c:1044:inode_path]
> (-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xe8fd) [0x7fa8341868fd]
> (-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xa6bb) [0x7fa8341826bb]
> (-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(fuse_loc_fill+0x1c6)
> [0x7fa83417d156]))) 0-volume1/inode: no dentry for non-root inode
> -69777006931: 0a37836d-e9e5-4cc1-8bd2-e8a49947959b
> [2013-02-18 05:55:31.640865] W [fuse-bridge.c:561:fuse_getattr]
> 0-glusterfs-fuse: 2298073: GETATTR 140360215569520 (fuse_loc_fill() failed)
> [2013-02-18 05:55:31.641672] W [inode.c:1044:inode_path]
> (-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xe8fd) [0x7fa8341868fd]
> (-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(+0xa6bb) [0x7fa8341826bb]
> (-->/usr/lib/glusterfs/3.2.7/xlator/mount/fuse.so(fuse_loc_fill+0x1c6)
> [0x7fa83417d156]))) 0-volume1/inode: no dentry for non-root inode
> -69777006931: 0a37836d-e9e5-4cc1-8bd2-e8a49947959b
> [2013-02-18 05:55:31.641724] W [fuse-bridge.c:561:fuse_getattr]
> 0-glusterfs-fuse: 2298079: GETATTR 140360215569520 (fuse_loc_fill() failed)
> ...
>
> Sometimes on these events, and sometimes not, there will also be logs (on
> both normal and abnormal nodes) of the form:
>
> [2013-02-18 03:35:28.679681] I [dht-common.c:525:dht_revalidate_cbk]
> 0-volume1-dht: mismatching layouts for /inSample/pred/20110831
>
> I understand from reading the mailing list that both the dentry errors and
> the mismatched layout errors are both non-fatal warnings and that the
> metadata will become internally consistent regardless. But these errors
> only happen on times when I'm slamming the glusterfs system with the
> creation of a bunch of small files in a very short burst like I described
> above. So their presence seems to be related to the error.
>
> I think the issue is almost assuredly related to the delayed propagation
> of glusterfs directory metadata. Some nodes are creating directory
> simultaneous to other nodes and the two are producing inconsistencies with
> regards to the dht layout information. My hypothesis is that when Node A is
> still writing that the process to resolve the inconsistencies with and
> propagate the metadata from Node B is rendering the location that Node A is
> writing to disconnected from its supposed path. (And hence the no dentry
> errors).
>
> I've taken some effort to go through the glusterfs source code,
> particularly the dht related files. The way dht normalizes anomalies could
> be the problem, but I've failed to find anything specific.
>
> Has anyone else run into a problem like this, or have insight about what
> might be causing it or how to avoid it?
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130218/e4ba525d/attachment-0001.html>

From ruben.malchow at googlemail.com  Mon Feb 18 21:51:47 2013
From: ruben.malchow at googlemail.com (ruben malchow)
Date: Mon, 18 Feb 2013 22:51:47 +0100
Subject: [Gluster-users] parity translator?
Message-ID: <7DB4261D-6F9C-4DB0-B330-AC5C0BCEF90C@gmail.com>



hi,

another question that keeps popping up here is this: why isnt there anything like a parity translator that - in combination with striping - takes n+x subvolumes and writes n blocks plus x copies of parity information? wouldn't this make sense for making striping more robust against failures? am i misunderstanding basic concepts again or would a somewhat RAID-like behaviour make it easier to balance robustness against failures against space used? 

.rm

From joe at julianfamily.org  Mon Feb 18 22:38:03 2013
From: joe at julianfamily.org (Joe Julian)
Date: Mon, 18 Feb 2013 14:38:03 -0800
Subject: [Gluster-users] parity translator?
In-Reply-To: <7DB4261D-6F9C-4DB0-B330-AC5C0BCEF90C@gmail.com>
References: <7DB4261D-6F9C-4DB0-B330-AC5C0BCEF90C@gmail.com>
Message-ID: <5122AD4B.5070503@julianfamily.org>

On 02/18/2013 01:51 PM, ruben malchow wrote:
> another question that keeps popping up here is this: why isnt there anything like a parity translator that - in combination with striping - takes n+x subvolumes and writes n blocks plus x copies of parity information? wouldn't this make sense for making striping more robust against failures? am i misunderstanding basic concepts again or would a somewhat RAID-like behaviour make it easier to balance robustness against failures against space used?
>
The short answer is that GlusterFS is not a block device.

Sure, striping exists and could possibly support a parity translator in 
RAID fashion, but the stripe translator isn't, imho, anywhere close to 
as functional as raid ( 
http://joejulian.name/blog/should-i-use-stripe-on-glusterfs/ ). Fault 
tolerance, self-healing, split-brain, etc. are all much more difficult 
to manage when you're not connecting your block storage to the raid 
controller and are, instead, storing stripes and parities of files in a 
filesystem instead of on blocks.

It's not a definite no, though. 3.4 includes a block-device translator 
for exposing raw block devices. Perhaps that could be utilized in a way 
that parity striping might eventually happen. I'm sure that if someone 
were to write a translator that worked, it would be happily accepted 
into the project.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130218/e3d3332a/attachment.html>

From ruben.malchow at googlemail.com  Tue Feb 19 08:38:30 2013
From: ruben.malchow at googlemail.com (ruben malchow)
Date: Tue, 19 Feb 2013 09:38:30 +0100
Subject: [Gluster-users] parity translator?
In-Reply-To: <5122AD4B.5070503@julianfamily.org>
References: <7DB4261D-6F9C-4DB0-B330-AC5C0BCEF90C@gmail.com>
	<5122AD4B.5070503@julianfamily.org>
Message-ID: <CA631EE7-6095-4154-99FF-BC4D639DF03A@gmail.com>



hi,

the problem with using a stripeset on the physical block device level is that any kind of parity information only protects you from failures within the node, not from a failure of the entire node itself. and with striping, it should behave like a block-ish device. anyway, in our situation now, it's ok not to have it (we just need more bricks, but then replication also has other benefits). 

the plans for 3.4 you mentioned - where can i read about it?

unfortunately, my expertise is not in c ? i'm a java guy (although i would LOVE to contribute, i just feel a bit unable to)

.rm


> The short answer is that GlusterFS is not a block device. 
> 
> Sure, striping exists and could possibly support a parity translator in RAID fashion, but the stripe translator isn't, imho, anywhere close to as functional as raid ( http://joejulian.name/blog/should-i-use-stripe-on-glusterfs/ ). Fault tolerance, self-healing, split-brain, etc. are all much more difficult to manage when you're not connecting your block storage to the raid controller and are, instead, storing stripes and parities of files in a filesystem instead of on blocks.
> 
> It's not a definite no, though. 3.4 includes a block-device translator for exposing raw block devices. Perhaps that could be utilized in a way that parity striping might eventually happen. I'm sure that if someone were to write a translator that worked, it would be happily accepted into the project.
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130219/d5341662/attachment.html>

From ruben.malchow at googlemail.com  Tue Feb 19 09:10:16 2013
From: ruben.malchow at googlemail.com (ruben malchow)
Date: Tue, 19 Feb 2013 10:10:16 +0100
Subject: [Gluster-users] striping & read-ahead & volfiles
Message-ID: <F68E0AC2-9B84-4277-B813-F8CD4FA9E51E@gmail.com>



hi,

i have a somewhat more specific question about the way striping and read-ahead work. our test setup is as follows:

10x dumb nodes with 4x1TByte bricks & 1GBE network running the server
1x head node with no bricks and 10GBE running both server and client.

the idea was to have these machines in one cluster, and have the head node participate in the cluster as well as mounting
it and re-exporting it.

for the workload we're planning, we have rather few clients (in the end, less than 20, for testing, something like 4) that need 
rather high throughputs - some clients need 400MBit/s, some need as much as possible north of that.

i thought i could achieve this by doing striping. in this scenarion, i would do stripe+replicate, with nodes 1-5 acting as
a stripe set and nodes 6-10 acting as it's replica. 

with this setup, i would expect something in the range of 300-500MByte/s ? but actually, i see something in the range 
of 100-200. 

i assume that i am doing something wrong - either in the way i deal with block sizes and/or with the way i try to use read-ahead.
i was hoping read-ahead could be configured to start preparing the next stripe while the first one is reading and so on 
as a remedy for latency issues - is this correct?

if yes, what would be the steps to creating this kind of stacking from the CLI?

client --> server --> unify --> read-ahead --> replicate 2 --> stripe 5 --> posix --> brick

and what is configurable there (in terms of read-ahead buffers)? or is this a completely wrong approach?

thanks!

.rm



From xhernandez at datalab.es  Tue Feb 19 17:04:28 2013
From: xhernandez at datalab.es (Xavier Hernandez)
Date: Tue, 19 Feb 2013 18:04:28 +0100
Subject: [Gluster-users] parity translator?
In-Reply-To: <7DB4261D-6F9C-4DB0-B330-AC5C0BCEF90C@gmail.com>
References: <7DB4261D-6F9C-4DB0-B330-AC5C0BCEF90C@gmail.com>
Message-ID: <5123B09C.7030809@datalab.es>

Hi Ruben,

we are working on something like that, but it is not ready for 
production environments yet. Some stability and performance improvements 
must be done. We have recently published an initial alpha version on 
GitHub (https://github.com/datalab-bcn). However it is basically 
intended for developer testing until it matures.

If you want/can test it we will be very happy to have some feedback and 
improve it.

Xavi

Al 18/02/13 22:51, En/na ruben malchow ha escrit:
>
> hi,
>
> another question that keeps popping up here is this: why isnt there anything like a parity translator that - in combination with striping - takes n+x subvolumes and writes n blocks plus x copies of parity information? wouldn't this make sense for making striping more robust against failures? am i misunderstanding basic concepts again or would a somewhat RAID-like behaviour make it easier to balance robustness against failures against space used?
>
> .rm
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From marc.seeger at acquia.com  Tue Feb 19 20:07:02 2013
From: marc.seeger at acquia.com (Marc Seeger)
Date: Tue, 19 Feb 2013 21:07:02 +0100
Subject: [Gluster-users] Problems running dbench on 3.3
Message-ID: <BF5E470D-7D1F-4F3D-AD7C-59FCA1407BD9@acquia.com>

To test gluster's behavior under heavy load, I'm currently doing this on two machines sharing a common /mnt/gfs gluster mount:

ssh bal-6.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
ssh bal-7.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs


One of the processes usually dies pretty quickly like this:

[608] open /mnt/gfs/clients/client5/~dmtmp/PWRPNT/PCBENCHM.PPT failed for handle 10003 (No such file or directory)
(610) ERROR: handle 10003 was not found,
Child failed with status 1


And the logs are full of things like this (ignore the initial timestamp, that's from our logging):

[2013-02-19 14:38:38.714493] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client5/~dmtmp/PM/MOVED.DOC, 
[2013-02-19 14:38:38.724494] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp, 
[2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client4/~dmtmp/PM/EVENTS.DOC, 
[2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PM/MOVED.DOC, 
[2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client1/~dmtmp/PM/MOVED.DOC, 
[2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client0/~dmtmp/PM/MOVED.DOC, 
[2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client4/~dmtmp/PM,  [build-2 system.rb:340], I,  
[2013-02-19T14:39:50.189970 #20802]  INFO -- : 
[2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
[2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
[2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
[2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
[2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
[2013-02-19 14:38:36.051890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients, 
[2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2, 
[2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3, 
[2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2, 
[2013-02-19 14:38:36.081890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3, 
[2013-02-19 14:38:36.091890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp, 
[2013-02-19 14:38:36.091890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp, 
[2013-02-19 14:38:36.101890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp, 
[2013-02-19 14:38:36.101890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp, 
[2013-02-19 14:38:36.111890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD, 
[2013-02-19 14:38:36.111890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD, 
[2013-02-19 14:38:36.131890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/WORD, 
[2013-02-19 14:38:36.141890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/WORD, 
[2013-02-19 14:38:36.151890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/CHAP10.DOC, 
[2013-02-19 14:38:36.151890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/CHAP10.DOC, 
[2013-02-19 14:38:36.161890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/BASEMACH.DOC, 
[2013-02-19 14:38:36.161890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/BASEMACH.DOC, 
[2013-02-19 14:38:36.171890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entr [build-2 system.rb:340], I,  
[2013-02-19T14:39:50.189970 #20802]  INFO -- : y missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/FACTS.DOC, 
[2013-02-19 14:38:36.181890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/FACTS.DOC, 
[2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL, 
[2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL, 
[2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/EXCEL, 
[2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/EXCEL, 
[2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client0/~dmtmp, 
[2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL/PCMAGCD.XLS, 
[2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL/PCMAGCD.XLS, 
[2013-02-19 14:38:36.241890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL/SALES.XLS, 
[2013-02-19 14:38:36.241890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL/SALES.XLS, 
[2013-02-19 14:38:36.271890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT, 
[2013-02-19 14:38:36.271890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT, 
[2013-02-19 14:38:36.281890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/PWRPNT, 
[2013-02-19 14:38:36.281890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/PWRPNT, 
[2013-02-19 14:38:36.291890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/PCBENCHM.PPT, 
[2013-02-19 14:38:36.311890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT/PCBENCHM.PPT, 
[2013-02-19 14:38:36.351890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/ZD16.BMP, 
[2013-02-19 14:38:36.351890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT/ZD16.BMP, 
[2013-02-19 14:38:36.381890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/PPTOOLS1.PPA, 
[2013-02-19 14:38:36.391890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid sel [build-2 system.rb:340]



Any ideas? Can somebody confirm this happens for them too?

The setup is ubuntu lucid machines running 3.3.1 from this PPA: https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.3

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130219/c07954d0/attachment.html>

From anand.avati at gmail.com  Tue Feb 19 20:22:50 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Tue, 19 Feb 2013 12:22:50 -0800
Subject: [Gluster-users] Problems running dbench on 3.3
In-Reply-To: <BF5E470D-7D1F-4F3D-AD7C-59FCA1407BD9@acquia.com>
References: <BF5E470D-7D1F-4F3D-AD7C-59FCA1407BD9@acquia.com>
Message-ID: <CAFboF2x15vEKuoja6UiPX9xx8vLd=vncQRFE8p84J1m=_sdiyw@mail.gmail.com>

We run dbench in the pre-commit script for every patch. Looks like you are
running dbench on the same directory from multiple clients. Is that even
supposed to work? Did you instead lunch to run each dbench within their own
subdirectory?

Avati

On Tue, Feb 19, 2013 at 12:07 PM, Marc Seeger <marc.seeger at acquia.com>wrote:

> To test gluster's behavior under heavy load, I'm currently doing this on
> two machines sharing a common /mnt/gfs gluster mount:
>
> ssh bal-6.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
> ssh bal-7.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
>
>
> One of the processes usually dies pretty quickly like this:
>
> [608] open /mnt/gfs/clients/client5/~dmtmp/PWRPNT/PCBENCHM.PPT failed for
> handle 10003 (No such file or directory)
> (610) ERROR: handle 10003 was not found,
> Child failed with status 1
>
>
> And the logs are full of things like this (ignore the initial timestamp,
> that's from our logging):
>
> [2013-02-19 14:38:38.714493] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  data missing-entry gfid self-heal failed on
> /clients/client5/~dmtmp/PM/MOVED.DOC,
> [2013-02-19 14:38:38.724494] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client3/~dmtmp,
> [2013-02-19 14:38:38.734495] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  data missing-entry gfid self-heal failed on
> /clients/client4/~dmtmp/PM/EVENTS.DOC,
> [2013-02-19 14:38:38.734495] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  data missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/PM/MOVED.DOC,
> [2013-02-19 14:38:38.734495] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  data missing-entry gfid self-heal failed on
> /clients/client1/~dmtmp/PM/MOVED.DOC,
> [2013-02-19 14:38:38.734495] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  data missing-entry gfid self-heal failed on
> /clients/client0/~dmtmp/PM/MOVED.DOC,
> [2013-02-19 14:38:38.734495] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client4/~dmtmp/PM,  [build-2
> system.rb:340], I,
> [2013-02-19T14:39:50.189970 #20802]  INFO -- :
> [2013-02-19 14:38:36.041890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /,
> [2013-02-19 14:38:36.041890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /,
> [2013-02-19 14:38:36.041890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /,
> [2013-02-19 14:38:36.041890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /,
> [2013-02-19 14:38:36.041890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /,
> [2013-02-19 14:38:36.051890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients,
> [2013-02-19 14:38:36.071890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2,
> [2013-02-19 14:38:36.071890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3,
> [2013-02-19 14:38:36.071890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client2,
> [2013-02-19 14:38:36.081890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client3,
> [2013-02-19 14:38:36.091890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp,
> [2013-02-19 14:38:36.091890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp,
> [2013-02-19 14:38:36.101890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client2/~dmtmp,
> [2013-02-19 14:38:36.101890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client3/~dmtmp,
> [2013-02-19 14:38:36.111890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/WORD,
> [2013-02-19 14:38:36.111890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/WORD,
> [2013-02-19 14:38:36.131890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client2/~dmtmp/WORD,
> [2013-02-19 14:38:36.141890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client3/~dmtmp/WORD,
> [2013-02-19 14:38:36.151890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/WORD/CHAP10.DOC,
> [2013-02-19 14:38:36.151890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/WORD/CHAP10.DOC,
> [2013-02-19 14:38:36.161890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/WORD/BASEMACH.DOC,
> [2013-02-19 14:38:36.161890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/WORD/BASEMACH.DOC,
> [2013-02-19 14:38:36.171890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entr [build-2 system.rb:340], I,
> [2013-02-19T14:39:50.189970 #20802]  INFO -- : y missing-entry gfid
> self-heal failed on /clients/client2/~dmtmp/WORD/FACTS.DOC,
> [2013-02-19 14:38:36.181890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/WORD/FACTS.DOC,
> [2013-02-19 14:38:36.201890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/EXCEL,
> [2013-02-19 14:38:36.201890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/EXCEL,
> [2013-02-19 14:38:36.201890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client2/~dmtmp/EXCEL,
> [2013-02-19 14:38:36.201890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client3/~dmtmp/EXCEL,
> [2013-02-19 14:38:36.211890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client0/~dmtmp,
> [2013-02-19 14:38:36.211890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/EXCEL/PCMAGCD.XLS,
> [2013-02-19 14:38:36.211890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/EXCEL/PCMAGCD.XLS,
> [2013-02-19 14:38:36.241890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/EXCEL/SALES.XLS,
> [2013-02-19 14:38:36.241890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/EXCEL/SALES.XLS,
> [2013-02-19 14:38:36.271890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/PWRPNT,
> [2013-02-19 14:38:36.271890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/PWRPNT,
> [2013-02-19 14:38:36.281890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client2/~dmtmp/PWRPNT,
> [2013-02-19 14:38:36.281890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  entry self-heal failed on /clients/client3/~dmtmp/PWRPNT,
> [2013-02-19 14:38:36.291890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/PWRPNT/PCBENCHM.PPT,
> [2013-02-19 14:38:36.311890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/PWRPNT/PCBENCHM.PPT,
> [2013-02-19 14:38:36.351890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/PWRPNT/ZD16.BMP,
> [2013-02-19 14:38:36.351890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client3/~dmtmp/PWRPNT/ZD16.BMP,
> [2013-02-19 14:38:36.381890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid self-heal failed on
> /clients/client2/~dmtmp/PWRPNT/PPTOOLS1.PPA,
> [2013-02-19 14:38:36.391890] E
> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
> background  meta-data data entry missing-entry gfid sel [build-2
> system.rb:340]
>
>
>
> Any ideas? Can somebody confirm this happens for them too?
>
> The setup is ubuntu lucid machines running 3.3.1 from this PPA:
> https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.3
>
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130219/875c1e04/attachment-0001.html>

From marc.seeger at acquia.com  Tue Feb 19 22:20:14 2013
From: marc.seeger at acquia.com (Marc Seeger)
Date: Tue, 19 Feb 2013 23:20:14 +0100
Subject: [Gluster-users] Problems running dbench on 3.3
In-Reply-To: <CAFboF2x15vEKuoja6UiPX9xx8vLd=vncQRFE8p84J1m=_sdiyw@mail.gmail.com>
References: <BF5E470D-7D1F-4F3D-AD7C-59FCA1407BD9@acquia.com>
	<CAFboF2x15vEKuoja6UiPX9xx8vLd=vncQRFE8p84J1m=_sdiyw@mail.gmail.com>
Message-ID: <606F6DBF-D7AD-4DEF-9993-84BC9AAA5C73@acquia.com>

I'll give it a try. Isn't it somewhat concerning that this would heal to missing gfids and failing self-heals though?

On 19.02.2013, at 21:22, Anand Avati <anand.avati at gmail.com> wrote:

> We run dbench in the pre-commit script for every patch. Looks like you are running dbench on the same directory from multiple clients. Is that even supposed to work? Did you instead lunch to run each dbench within their own subdirectory?
> 
> Avati
> 
> On Tue, Feb 19, 2013 at 12:07 PM, Marc Seeger <marc.seeger at acquia.com> wrote:
> To test gluster's behavior under heavy load, I'm currently doing this on two machines sharing a common /mnt/gfs gluster mount:
> 
> ssh bal-6.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
> ssh bal-7.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
> 
> 
> One of the processes usually dies pretty quickly like this:
> 
> [608] open /mnt/gfs/clients/client5/~dmtmp/PWRPNT/PCBENCHM.PPT failed for handle 10003 (No such file or directory)
> (610) ERROR: handle 10003 was not found,
> Child failed with status 1
> 
> 
> And the logs are full of things like this (ignore the initial timestamp, that's from our logging):
> 
> [2013-02-19 14:38:38.714493] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client5/~dmtmp/PM/MOVED.DOC, 
> [2013-02-19 14:38:38.724494] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp, 
> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client4/~dmtmp/PM/EVENTS.DOC, 
> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PM/MOVED.DOC, 
> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client1/~dmtmp/PM/MOVED.DOC, 
> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client0/~dmtmp/PM/MOVED.DOC, 
> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client4/~dmtmp/PM,  [build-2 system.rb:340], I,  
> [2013-02-19T14:39:50.189970 #20802]  INFO -- : 
> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
> [2013-02-19 14:38:36.051890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients, 
> [2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2, 
> [2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3, 
> [2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2, 
> [2013-02-19 14:38:36.081890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3, 
> [2013-02-19 14:38:36.091890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp, 
> [2013-02-19 14:38:36.091890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp, 
> [2013-02-19 14:38:36.101890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp, 
> [2013-02-19 14:38:36.101890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp, 
> [2013-02-19 14:38:36.111890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD, 
> [2013-02-19 14:38:36.111890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD, 
> [2013-02-19 14:38:36.131890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/WORD, 
> [2013-02-19 14:38:36.141890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/WORD, 
> [2013-02-19 14:38:36.151890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/CHAP10.DOC, 
> [2013-02-19 14:38:36.151890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/CHAP10.DOC, 
> [2013-02-19 14:38:36.161890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/BASEMACH.DOC, 
> [2013-02-19 14:38:36.161890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/BASEMACH.DOC, 
> [2013-02-19 14:38:36.171890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entr [build-2 system.rb:340], I,  
> [2013-02-19T14:39:50.189970 #20802]  INFO -- : y missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/FACTS.DOC, 
> [2013-02-19 14:38:36.181890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/FACTS.DOC, 
> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL, 
> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL, 
> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/EXCEL, 
> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/EXCEL, 
> [2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client0/~dmtmp, 
> [2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL/PCMAGCD.XLS, 
> [2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL/PCMAGCD.XLS, 
> [2013-02-19 14:38:36.241890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL/SALES.XLS, 
> [2013-02-19 14:38:36.241890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL/SALES.XLS, 
> [2013-02-19 14:38:36.271890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT, 
> [2013-02-19 14:38:36.271890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT, 
> [2013-02-19 14:38:36.281890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/PWRPNT, 
> [2013-02-19 14:38:36.281890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/PWRPNT, 
> [2013-02-19 14:38:36.291890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/PCBENCHM.PPT, 
> [2013-02-19 14:38:36.311890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT/PCBENCHM.PPT, 
> [2013-02-19 14:38:36.351890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/ZD16.BMP, 
> [2013-02-19 14:38:36.351890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT/ZD16.BMP, 
> [2013-02-19 14:38:36.381890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/PPTOOLS1.PPA, 
> [2013-02-19 14:38:36.391890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid sel [build-2 system.rb:340]
> 
> 
> 
> Any ideas? Can somebody confirm this happens for them too?
> 
> The setup is ubuntu lucid machines running 3.3.1 from this PPA: https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.3
> 
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130219/0001a074/attachment.html>

From anand.avati at gmail.com  Tue Feb 19 22:32:10 2013
From: anand.avati at gmail.com (Anand Avati)
Date: Tue, 19 Feb 2013 14:32:10 -0800
Subject: [Gluster-users] Problems running dbench on 3.3
In-Reply-To: <606F6DBF-D7AD-4DEF-9993-84BC9AAA5C73@acquia.com>
References: <BF5E470D-7D1F-4F3D-AD7C-59FCA1407BD9@acquia.com>
	<CAFboF2x15vEKuoja6UiPX9xx8vLd=vncQRFE8p84J1m=_sdiyw@mail.gmail.com>
	<606F6DBF-D7AD-4DEF-9993-84BC9AAA5C73@acquia.com>
Message-ID: <CAFboF2yxe9QmYj_xHmB3=bmHo8604jkJ9Ch93SBFM50N9JaDag@mail.gmail.com>

They were just self-heal double-checks performed in the lookup path, as we
perform the initial lookup unlocked for performance reasons. Those lines by
themselves do not indicate anything harmful (yet). I admit the log messages
could have been better though!

Avati

On Tue, Feb 19, 2013 at 2:20 PM, Marc Seeger <marc.seeger at acquia.com> wrote:

> I'll give it a try. Isn't it somewhat concerning that this would heal to
> missing gfids and failing self-heals though?
>
> On 19.02.2013, at 21:22, Anand Avati <anand.avati at gmail.com> wrote:
>
> We run dbench in the pre-commit script for every patch. Looks like you are
> running dbench on the same directory from multiple clients. Is that even
> supposed to work? Did you instead lunch to run each dbench within their own
> subdirectory?
>
> Avati
>
> On Tue, Feb 19, 2013 at 12:07 PM, Marc Seeger <marc.seeger at acquia.com>wrote:
>
>> To test gluster's behavior under heavy load, I'm currently doing this on
>> two machines sharing a common /mnt/gfs gluster mount:
>>
>> ssh bal-6.example.com apt-get install dbench && dbench 6 -t 60 -D
>> /mnt/gfs
>> ssh bal-7.example.com apt-get install dbench && dbench 6 -t 60 -D
>> /mnt/gfs
>>
>>
>> One of the processes usually dies pretty quickly like this:
>>
>> [608] open /mnt/gfs/clients/client5/~dmtmp/PWRPNT/PCBENCHM.PPT failed for
>> handle 10003 (No such file or directory)
>> (610) ERROR: handle 10003 was not found,
>> Child failed with status 1
>>
>>
>> And the logs are full of things like this (ignore the initial timestamp,
>> that's from our logging):
>>
>> [2013-02-19 14:38:38.714493] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  data missing-entry gfid self-heal failed on
>> /clients/client5/~dmtmp/PM/MOVED.DOC,
>> [2013-02-19 14:38:38.724494] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client3/~dmtmp,
>> [2013-02-19 14:38:38.734495] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  data missing-entry gfid self-heal failed on
>> /clients/client4/~dmtmp/PM/EVENTS.DOC,
>> [2013-02-19 14:38:38.734495] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  data missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/PM/MOVED.DOC,
>> [2013-02-19 14:38:38.734495] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  data missing-entry gfid self-heal failed on
>> /clients/client1/~dmtmp/PM/MOVED.DOC,
>> [2013-02-19 14:38:38.734495] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  data missing-entry gfid self-heal failed on
>> /clients/client0/~dmtmp/PM/MOVED.DOC,
>> [2013-02-19 14:38:38.734495] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client4/~dmtmp/PM,  [build-2
>> system.rb:340], I,
>> [2013-02-19T14:39:50.189970 #20802]  INFO -- :
>> [2013-02-19 14:38:36.041890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /,
>> [2013-02-19 14:38:36.041890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /,
>> [2013-02-19 14:38:36.041890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /,
>> [2013-02-19 14:38:36.041890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /,
>> [2013-02-19 14:38:36.041890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /,
>> [2013-02-19 14:38:36.051890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients,
>> [2013-02-19 14:38:36.071890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2,
>> [2013-02-19 14:38:36.071890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3,
>> [2013-02-19 14:38:36.071890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client2,
>> [2013-02-19 14:38:36.081890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client3,
>> [2013-02-19 14:38:36.091890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp,
>> [2013-02-19 14:38:36.091890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp,
>> [2013-02-19 14:38:36.101890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client2/~dmtmp,
>> [2013-02-19 14:38:36.101890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client3/~dmtmp,
>> [2013-02-19 14:38:36.111890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/WORD,
>> [2013-02-19 14:38:36.111890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/WORD,
>> [2013-02-19 14:38:36.131890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client2/~dmtmp/WORD,
>> [2013-02-19 14:38:36.141890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client3/~dmtmp/WORD,
>> [2013-02-19 14:38:36.151890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/WORD/CHAP10.DOC,
>> [2013-02-19 14:38:36.151890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/WORD/CHAP10.DOC,
>> [2013-02-19 14:38:36.161890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/WORD/BASEMACH.DOC,
>> [2013-02-19 14:38:36.161890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/WORD/BASEMACH.DOC,
>> [2013-02-19 14:38:36.171890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entr [build-2 system.rb:340], I,
>> [2013-02-19T14:39:50.189970 #20802]  INFO -- : y missing-entry gfid
>> self-heal failed on /clients/client2/~dmtmp/WORD/FACTS.DOC,
>> [2013-02-19 14:38:36.181890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/WORD/FACTS.DOC,
>> [2013-02-19 14:38:36.201890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/EXCEL,
>> [2013-02-19 14:38:36.201890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/EXCEL,
>> [2013-02-19 14:38:36.201890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client2/~dmtmp/EXCEL,
>> [2013-02-19 14:38:36.201890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client3/~dmtmp/EXCEL,
>> [2013-02-19 14:38:36.211890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client0/~dmtmp,
>> [2013-02-19 14:38:36.211890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/EXCEL/PCMAGCD.XLS,
>> [2013-02-19 14:38:36.211890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/EXCEL/PCMAGCD.XLS,
>> [2013-02-19 14:38:36.241890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/EXCEL/SALES.XLS,
>> [2013-02-19 14:38:36.241890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/EXCEL/SALES.XLS,
>> [2013-02-19 14:38:36.271890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/PWRPNT,
>> [2013-02-19 14:38:36.271890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/PWRPNT,
>> [2013-02-19 14:38:36.281890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client2/~dmtmp/PWRPNT,
>> [2013-02-19 14:38:36.281890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  entry self-heal failed on /clients/client3/~dmtmp/PWRPNT,
>> [2013-02-19 14:38:36.291890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/PWRPNT/PCBENCHM.PPT,
>> [2013-02-19 14:38:36.311890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/PWRPNT/PCBENCHM.PPT,
>> [2013-02-19 14:38:36.351890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/PWRPNT/ZD16.BMP,
>> [2013-02-19 14:38:36.351890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client3/~dmtmp/PWRPNT/ZD16.BMP,
>> [2013-02-19 14:38:36.381890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid self-heal failed on
>> /clients/client2/~dmtmp/PWRPNT/PPTOOLS1.PPA,
>> [2013-02-19 14:38:36.391890] E
>> [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0:
>> background  meta-data data entry missing-entry gfid sel [build-2
>> system.rb:340]
>>
>>
>>
>> Any ideas? Can somebody confirm this happens for them too?
>>
>> The setup is ubuntu lucid machines running 3.3.1 from this PPA:
>> https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.3
>>
>>
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130219/0eec10ce/attachment-0001.html>

From lanning at lanning.cc  Tue Feb 19 22:42:58 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Tue, 19 Feb 2013 14:42:58 -0800
Subject: [Gluster-users] striping & read-ahead & volfiles
In-Reply-To: <F68E0AC2-9B84-4277-B813-F8CD4FA9E51E@gmail.com>
References: <F68E0AC2-9B84-4277-B813-F8CD4FA9E51E@gmail.com>
Message-ID: <5123FFF2.5020809@lanning.cc>

On 02/19/13 01:10, ruben malchow wrote:
> i thought i could achieve this by doing striping. in this scenarion, i would do stripe+replicate, with nodes 1-5 acting as
> a stripe set and nodes 6-10 acting as it's replica.
>
> with this setup, i would expect something in the range of 300-500MByte/s ? but actually, i see something in the range
> of 100-200.
>
> i assume that i am doing something wrong - either in the way i deal with block sizes and/or with the way i try to use read-ahead.
> i was hoping read-ahead could be configured to start preparing the next stripe while the first one is reading and so on
> as a remedy for latency issues - is this correct?
>

Striping isn't what you think it is...
http://joejulian.name/blog/should-i-use-stripe-on-glusterfs/

-- 
Mr. Flibble
King of the Potato People

From tompos at martos.bme.hu  Wed Feb 20 12:17:33 2013
From: tompos at martos.bme.hu (Papp Tamas)
Date: Wed, 20 Feb 2013 13:17:33 +0100
Subject: [Gluster-users] windows access
Message-ID: <5124BEDD.5090602@martos.bme.hu>

hi All,

What is the status currently accessing a gluster volume from Windows?

I found this irclog entry:

http://irclog.perlgeek.de/gluster/2013-02-07


18:52
johnmark
we've been developing libgfapi, and we'll do some SAMBA VFS integration with the API

18:52
but that will be in 3.5, which might be out by the end of this year


Can we expect significant speed improvements?



What are the best samba settings for exporting a volume by cifs?

We currently use this smb.conf, but the performance is rather poor, about half or less of the fuse 
client an much.
Re-exporting NFS mount is totally disaster.


[global]
     socket options =  IPTOS_THROUGHPUT TCP_NODELAY IPTOS_LOWDELAY SO_SNDBUF=131072 SO_RCVBUF=131072
     read raw = yes
     server string = %h
     write raw = yes
     #oplocks = yes
     max xmit = 131072
     dead time = 15
     getwd cache = yes
     use sendfile=yes
     block size = 131072
     load printers = no
     aio write size = 16384
     #aio write size = 262144
     aio write behind = /*.*/
     #write cache size = 262144
     wins support = no
     local master = no
     wins server = 192.168.3.7
     veto files = /.AppleDouble/
     delete veto files = yes
     hide dot files = yes
     printing = BSD

[projects]
         path = /W/Projects
         browseable = yes
         public = yes
         guest ok = yes
         read only = no
	force user = user
	force group = user


When we want to use some non-conventional sw, like Adobe Premiere, Photoshop...etc, a more strict 
config is necessary with even less speed.


Any suggestion?

Thank you,
tamas

From toby.corkindale at strategicdata.com.au  Thu Feb 21 01:05:35 2013
From: toby.corkindale at strategicdata.com.au (Toby Corkindale)
Date: Thu, 21 Feb 2013 12:05:35 +1100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
Message-ID: <512572DF.3030200@strategicdata.com.au>

logrotate.d/glusterfs-common (in the debian package for 3.3.1) is faulty.
It rotates the log files, but it doesn't tell glusterd to re-open them, 
so it continues to write to what is now .1 (and then later it gets 
gziped and corrupted)

I also note that the debian packages do not include the man pages for 
any of the gluster binaries! (gluster, glusterfs, glusterd)

From marc.seeger at acquia.com  Thu Feb 21 09:23:07 2013
From: marc.seeger at acquia.com (Marc Seeger)
Date: Thu, 21 Feb 2013 10:23:07 +0100
Subject: [Gluster-users] Problems running dbench on 3.3
In-Reply-To: <CAFboF2yxe9QmYj_xHmB3=bmHo8604jkJ9Ch93SBFM50N9JaDag@mail.gmail.com>
References: <BF5E470D-7D1F-4F3D-AD7C-59FCA1407BD9@acquia.com>
	<CAFboF2x15vEKuoja6UiPX9xx8vLd=vncQRFE8p84J1m=_sdiyw@mail.gmail.com>
	<606F6DBF-D7AD-4DEF-9993-84BC9AAA5C73@acquia.com>
	<CAFboF2yxe9QmYj_xHmB3=bmHo8604jkJ9Ch93SBFM50N9JaDag@mail.gmail.com>
Message-ID: <CDD0CB09-6686-4B04-87A1-C95BDE79527A@acquia.com>

Ok, confirmed. Letting dbench work on different subfolders makes it work fine.
In terms of log messages:
I would love to monitor for problems with gluster. Failing self heals in the logs might not really be a problem per-se, any suggestions of what else I could parse the logs for?

Cheers,
Marc

On 19.02.2013, at 23:32, Anand Avati <anand.avati at gmail.com> wrote:

> They were just self-heal double-checks performed in the lookup path, as we perform the initial lookup unlocked for performance reasons. Those lines by themselves do not indicate anything harmful (yet). I admit the log messages could have been better though!
> 
> Avati
> 
> On Tue, Feb 19, 2013 at 2:20 PM, Marc Seeger <marc.seeger at acquia.com> wrote:
> I'll give it a try. Isn't it somewhat concerning that this would heal to missing gfids and failing self-heals though?
> 
> On 19.02.2013, at 21:22, Anand Avati <anand.avati at gmail.com> wrote:
> 
>> We run dbench in the pre-commit script for every patch. Looks like you are running dbench on the same directory from multiple clients. Is that even supposed to work? Did you instead lunch to run each dbench within their own subdirectory?
>> 
>> Avati
>> 
>> On Tue, Feb 19, 2013 at 12:07 PM, Marc Seeger <marc.seeger at acquia.com> wrote:
>> To test gluster's behavior under heavy load, I'm currently doing this on two machines sharing a common /mnt/gfs gluster mount:
>> 
>> ssh bal-6.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
>> ssh bal-7.example.com apt-get install dbench && dbench 6 -t 60 -D /mnt/gfs
>> 
>> 
>> One of the processes usually dies pretty quickly like this:
>> 
>> [608] open /mnt/gfs/clients/client5/~dmtmp/PWRPNT/PCBENCHM.PPT failed for handle 10003 (No such file or directory)
>> (610) ERROR: handle 10003 was not found,
>> Child failed with status 1
>> 
>> 
>> And the logs are full of things like this (ignore the initial timestamp, that's from our logging):
>> 
>> [2013-02-19 14:38:38.714493] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client5/~dmtmp/PM/MOVED.DOC, 
>> [2013-02-19 14:38:38.724494] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp, 
>> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client4/~dmtmp/PM/EVENTS.DOC, 
>> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PM/MOVED.DOC, 
>> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client1/~dmtmp/PM/MOVED.DOC, 
>> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  data missing-entry gfid self-heal failed on /clients/client0/~dmtmp/PM/MOVED.DOC, 
>> [2013-02-19 14:38:38.734495] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client4/~dmtmp/PM,  [build-2 system.rb:340], I,  
>> [2013-02-19T14:39:50.189970 #20802]  INFO -- : 
>> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
>> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
>> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
>> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
>> [2013-02-19 14:38:36.041890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /, 
>> [2013-02-19 14:38:36.051890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients, 
>> [2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2, 
>> [2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3, 
>> [2013-02-19 14:38:36.071890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2, 
>> [2013-02-19 14:38:36.081890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3, 
>> [2013-02-19 14:38:36.091890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp, 
>> [2013-02-19 14:38:36.091890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp, 
>> [2013-02-19 14:38:36.101890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp, 
>> [2013-02-19 14:38:36.101890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp, 
>> [2013-02-19 14:38:36.111890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD, 
>> [2013-02-19 14:38:36.111890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD, 
>> [2013-02-19 14:38:36.131890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/WORD, 
>> [2013-02-19 14:38:36.141890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/WORD, 
>> [2013-02-19 14:38:36.151890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/CHAP10.DOC, 
>> [2013-02-19 14:38:36.151890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/CHAP10.DOC, 
>> [2013-02-19 14:38:36.161890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/BASEMACH.DOC, 
>> [2013-02-19 14:38:36.161890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/BASEMACH.DOC, 
>> [2013-02-19 14:38:36.171890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entr [build-2 system.rb:340], I,  
>> [2013-02-19T14:39:50.189970 #20802]  INFO -- : y missing-entry gfid self-heal failed on /clients/client2/~dmtmp/WORD/FACTS.DOC, 
>> [2013-02-19 14:38:36.181890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/WORD/FACTS.DOC, 
>> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL, 
>> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL, 
>> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/EXCEL, 
>> [2013-02-19 14:38:36.201890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/EXCEL, 
>> [2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client0/~dmtmp, 
>> [2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL/PCMAGCD.XLS, 
>> [2013-02-19 14:38:36.211890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL/PCMAGCD.XLS, 
>> [2013-02-19 14:38:36.241890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/EXCEL/SALES.XLS, 
>> [2013-02-19 14:38:36.241890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/EXCEL/SALES.XLS, 
>> [2013-02-19 14:38:36.271890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT, 
>> [2013-02-19 14:38:36.271890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT, 
>> [2013-02-19 14:38:36.281890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client2/~dmtmp/PWRPNT, 
>> [2013-02-19 14:38:36.281890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  entry self-heal failed on /clients/client3/~dmtmp/PWRPNT, 
>> [2013-02-19 14:38:36.291890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/PCBENCHM.PPT, 
>> [2013-02-19 14:38:36.311890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT/PCBENCHM.PPT, 
>> [2013-02-19 14:38:36.351890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/ZD16.BMP, 
>> [2013-02-19 14:38:36.351890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client3/~dmtmp/PWRPNT/ZD16.BMP, 
>> [2013-02-19 14:38:36.381890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid self-heal failed on /clients/client2/~dmtmp/PWRPNT/PPTOOLS1.PPA, 
>> [2013-02-19 14:38:36.391890] E [afr-self-heal-common.c:2160:afr_self_heal_completion_cbk] 0-replicate0: background  meta-data data entry missing-entry gfid sel [build-2 system.rb:340]
>> 
>> 
>> 
>> Any ideas? Can somebody confirm this happens for them too?
>> 
>> The setup is ubuntu lucid machines running 3.3.1 from this PPA: https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.3
>> 
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> 
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130221/cf24a5e9/attachment-0001.html>

From mht at mail.dfci.harvard.edu  Thu Feb 21 14:33:10 2013
From: mht at mail.dfci.harvard.edu (Matthew Temple)
Date: Thu, 21 Feb 2013 09:33:10 -0500
Subject: [Gluster-users] One brick in replicated/distributed volume not
	being written to.
Message-ID: <CAMFFy-cOOTEb=b3bZYxCa_qkbXUEd0RQdRYV05r06iX5G8dH9w@mail.gmail.com>

Hi, all.

   I thought I had everything set correctly on my volume, but something is
wrong.   Here is the volume, made of 4 bricks:

Volume Name: gf2
Type: Distributed-Replicate
Volume ID: a9e64630-9166-4957-8243-e2933791b24b
Status: Started
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: gf2ibp-1:/mnt/d0-0
Brick2: gf2ibp-1r:/mnt/d0-0
Brick3: gf2ibp-2:/mnt/d0-0
Brick4: gf2ibp-2r:/mnt/d0-0

I have Volume gf2 mounted by a computer we call "rcapps"
About 6 TB have been written to the volume.
When I look at /mnt/d0-0 on all 4 bricks, 3 look correct, but
    Brick1 only has 48GB written to it.
    Brick2, which should replicate Brick1, has 4TB.
    Brick3 and Brick4 seem to have the same amount of data.

The status of the volume looks correct:

gluster> volume status gf2
Status of volume: gf2
Gluster process Port Online Pid
------------------------------------------------------------------------------
Brick gf2ibp-1:/mnt/d0-0 24011 Y 30754
Brick gf2ibp-1r:/mnt/d0-0 24011 Y 17824
Brick gf2ibp-2:/mnt/d0-0 24011 Y 31516
Brick gf2ibp-2r:/mnt/d0-0 24011 Y 29119
NFS Server on localhost 38467 Y 30760
Self-heal Daemon on localhost N/A Y 30766
NFS Server on gf2ibp-2 38467 Y 31522
Self-heal Daemon on gf2ibp-2 N/A Y 31528
NFS Server on gf2ibp-2r 38467 Y 29125
Self-heal Daemon on gf2ibp-2r N/A Y 29131
NFS Server on gf2ibp-1r 38467 Y 17830
Self-heal Daemon on gf2ibp-1r N/A Y 17836

I then saw I had the firewall turned on for brick2 (even though it could be
written) which I then turned off.
I thought I should try to heal the volume but when I tried this through the
gluster console, the operation failed.
In the log file I see (it can't get a lock which is held by itself?):

[2013-02-21 09:24:39.501612] I
[glusterd-volume-ops.c:492:glusterd_handle_cli_heal_volume] 0-management:
Received heal vol req for volume gf2
[2013-02-21 09:24:39.501732] E [glusterd-utils.c:277:glusterd_lock]
0-glusterd: Unable to get lock for uuid:
f5edea20-9467-48ed-b4f1-dc566a9b6d02, lock held by:
f5edea20-9467-48ed-b4f1-dc566a9b6d02
[2013-02-21 09:24:39.501759] E
[glusterd-handler.c:458:glusterd_op_txn_begin] 0-management: Unable to
acquire local lock, ret: -1

And here is what I see in cli.log, which I can't interpret.

2013-02-21 09:31:38.689316] W [cli-rl.c:116:cli_rl_process_line]
0-glusterfs: failed to process line
[2013-02-21 09:31:48.952950] I
[cli-rpc-ops.c:5928:gf_cli3_1_heal_volume_cbk] 0-cli: Received resp to heal
volume
[2013-02-21 09:31:48.953366] W [dict.c:2339:dict_unserialize]
(-->/usr/lib64/libgfrpc.so.0(rpc_clnt_notify+0x120) [0x333440f8b0]
(-->/usr/lib64/libgfrpc.so.0(rpc_clnt_handle_reply+0xa5) [0x333440f0b5]
(-->gluster(gf_cli3_1_heal_volume_cbk+0x2e3) [0x41ca43]))) 0-dict: buf is
null!
[2013-02-21 09:31:48.953410] E
[cli-rpc-ops.c:5968:gf_cli3_1_heal_volume_cbk] 0-: Unable to allocate memory
[2013-02-21 09:31:48.953490] W [cli-rl.c:116:cli_rl_process_line]
0-glusterfs: failed to process line
[2013-02-21 09:31:56.419708] I
[cli-rpc-ops.c:5928:gf_cli3_1_heal_volume_cbk] 0-cli: Received resp to heal
volume
[2013-02-21 09:31:56.419859] W [dict.c:2339:dict_unserialize]
(-->/usr/lib64/libgfrpc.so.0(rpc_clnt_notify+0x120) [0x333440f8b0]
(-->/usr/lib64/libgfrpc.so.0(rpc_clnt_handle_reply+0xa5) [0x333440f0b5]
(-->gluster(gf_cli3_1_heal_volume_cbk+0x2e3) [0x41ca43]))) 0-dict: buf is
null!
[2013-02-21 09:31:56.419894] E
[cli-rpc-ops.c:5968:gf_cli3_1_heal_volume_cbk] 0-: Unable to allocate memory
[2013-02-21 09:31:56.419979] W [cli-rl.c:116:cli_rl_process_line]
0-glusterfs: failed to process line

Any ideas of what I should do next?
Right now I have a pair of bricks that replicate fine and a pair that does
not, in a distributed/replicated cluster.
I need to get get brick2 to send its files back to brick1.

Thanks in advance.

Matt Temple

------
Matt Temple
Director, Research Computing
Dana-Farber Cancer Institute.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130221/3c1c8b10/attachment.html>

From d.a.bretherton at reading.ac.uk  Thu Feb 21 14:46:40 2013
From: d.a.bretherton at reading.ac.uk (Dan Bretherton)
Date: Thu, 21 Feb 2013 14:46:40 +0000
Subject: [Gluster-users] I/O error repaired only by owner or root access
Message-ID: <51263350.4090707@reading.ac.uk>

Dear All-
Several users are having a lot of trouble reading files belonging to 
other users.  Here is an example.

[sms05dab at jupiter ~]$ ls -l /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
ls: /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl: Input/output error

The corresponding nfs.log messages are shown below.

[2013-02-21 12:11:39.204659] W [nfs3.c:727:nfs3svc_getattr_stat_cbk] 
0-nfs: fe2ba5b8: /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN => -1 
(Invalid argument)
[2013-02-21 12:11:39.204778] W [nfs3-helpers.c:3389:nfs3_log_common_res] 
0-nfs-nfsv3: XID: fe2ba5b8, GETATTR: NFS: 22(Invalid argument for 
operation), POSIX: 22(Invalid argument)
[2013-02-21 12:11:39.215345] I 
[dht-common.c:954:dht_lookup_everywhere_cbk] 0-nemo2-dht: deleting stale 
linkfile /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN on nemo2-replicate-0
[2013-02-21 12:11:39.225674] W 
[client3_1-fops.c:592:client3_1_unlink_cbk] 0-nemo2-client-1: remote 
operation failed: Permission denied
[2013-02-21 12:11:39.225786] W 
[client3_1-fops.c:592:client3_1_unlink_cbk] 0-nemo2-client-0: remote 
operation failed: Permission denied
[2013-02-21 12:11:39.681029] W 
[client3_1-fops.c:258:client3_1_mknod_cbk] 0-nemo2-client-18: remote 
operation failed: Permission denied. Path: 
/gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN 
(1662aa0a-d43b-4c2e-9be9-407eb7a89e85)
[2013-02-21 12:11:39.681400] W 
[client3_1-fops.c:258:client3_1_mknod_cbk] 0-nemo2-client-19: remote 
operation failed: Permission denied. Path: 
/gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN 
(1662aa0a-d43b-4c2e-9be9-407eb7a89e85)
[2013-02-21 12:11:39.682268] W [nfs3.c:1627:nfs3svc_readlink_cbk] 0-nfs: 
2ca5b8: /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN => -1 (Invalid argument)
[2013-02-21 12:11:39.682338] W 
[nfs3-helpers.c:3403:nfs3_log_readlink_res] 0-nfs-nfsv3: XID: 2ca5b8, 
READLINK: NFS: 22(Invalid argument for operation), POSIX: 22(Invalid 
argument), target: (null)

I managed to access the same directory as the owner (or any user with 
write access including root) without any trouble, and after that access 
from my normal user account was fine as well.  The permissions on the 
directory allowed read access by everyone, but the "Permission denied" 
messages in nfs.log indicate that some sort of operation is not being 
allowed when the directory is accessed by other users.   I have seen 
this happen with files and directories, and with the GlusterFS native 
client and NFS.

I presume this is a bug; I would be grateful if someone could confirm 
this.  I would file a bug report, but the trouble is that I don't know 
how to reproduce the problem that causes the I/O error in the first 
place.  It only happens with some files and directories, not all.  Would 
a bug report without any way to reproduce the error be any use, and can 
anyone suggest a way to dig deeper (eg looking at xattrs) next time I 
come across an example?

-Dan.

--
Dan Bretherton
ESSC Computer System Manager
Department of Meteorology
Harry Pitt Building, 3 Earley Gate
University of Reading
Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
UK
Tel. +44 118 378 5205, Fax: +44 118 378 6413
-- 
## Please sponsor me to run in VSO's 30km Race to the Eye ##
##        http://www.justgiving.com/DanBretherton         ##


From dmitry.kuznetsov at ubisoft.com  Thu Feb 21 15:51:04 2013
From: dmitry.kuznetsov at ubisoft.com (Dmitry Kuznetsov)
Date: Thu, 21 Feb 2013 10:51:04 -0500
Subject: [Gluster-users] Gluster volume synchronization issue with concurent
	access
Message-ID: <52453AC9595A414497C4F8769D40F11E0339749E18@MDC-MAIL-CMS01.ubisoft.org>

Hello,

We are experiencing an issue when trying to write to gluster volume concurrently from several servers.
Our gluster consists of 8 servers, each having 2 bricks. The system is designed to be used for very fast file access (write/read).
We have a php web servers running on each of the servers which will be writing to a gluster volume.
What is happening right now is that php fails (throws an exception) when it tries to create a directory (mkdir).
Actually it can't properly determine if a directory in question exists or not.
By the time that it makes a decision if(!is_dir($directory)){ mkdir($directory, 0755, true);  } , the directory in question is already created by
another server and mkdir crashes. After that the volume becomes corrupted and unusable.
Does anyone know how this synchronization issue can be dealt with?
Does configuring bricks in Array0 (Hardware) with SSDs will be enough to speed up the IO for Gluster to synchronise volume data?

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130221/5fce21e5/attachment.html>

From nock at nocko.se  Thu Feb 21 21:59:03 2013
From: nock at nocko.se (Shawn Nock)
Date: Thu, 21 Feb 2013 16:59:03 -0500
Subject: [Gluster-users] Conservative merge fails on client3_1_mknod_cbk
Message-ID: <suv38wpib14.fsf@nock.cfmi.georgetown.edu>


Since I've added two bricks to my 2x2 (now 3x2) distribute-replicate
volume (and run fix-layout), accessing many of the volumes files via
fuse fails with "invalid argument".

[2013-02-21 12:48:53.121691] I [afr-self-heal-entry.c:2333:afr_sh_entry_fix] 0-mirror-replicate-2: [...]/140.ACQ: Performing conservative merge
[2013-02-21 12:48:54.100924] W [client3_1-fops.c:258:client3_1_mknod_cbk] 0-mirror-client-5: remote operation failed: Permission denied. Path: [...]/140.ACQ/1.3.12.2.1107.5.2.32.35052.2011011711023368433700039.IMA (ddef2b1a-b3cc-424c-a663-995bb77cd4c4)
[2013-02-21 12:48:54.101005] W [client3_1-fops.c:258:client3_1_mknod_cbk] 0-mirror-client-4: remote operation failed: Permission denied. Path: [...]/140.ACQ/1.3.12.2.1107.5.2.32.35052.2011011711023368433700039.IMA (ddef2b1a-b3cc-424c-a663-995bb77cd4c4)
[2013-02-21 13:20:31.971211] W [fuse-bridge.c:713:fuse_fd_cbk]
0-glusterfs-fuse: 1360169: OPEN()
[...]/140.ACQ/1.3.12.2.1107.5.2.32.35052.2011011711023368433700039.IMA
=> -1 (Invalid argument)

Additional information is here:
https://bugzilla.redhat.com/show_bug.cgi?id=913699

Strangely, opening the troublesome files on a fuse mount on any of the
block-servers completes without issue, AND afterward other fuse clients
can open the file as well.

Any thoughts would be helpful... right now I am opening every file on
that volume on one of the block-servers as a work around. I have many
small files and directories; It'll likely take days.

-- 
Shawn Nock (OpenPGP: 0x65118FA5)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 835 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130221/057cc277/attachment-0001.sig>

From torbjorn at trollweb.no  Thu Feb 21 22:53:48 2013
From: torbjorn at trollweb.no (=?ISO-8859-1?Q?Torbj=F8rn_Thorsen?=)
Date: Thu, 21 Feb 2013 23:53:48 +0100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512572DF.3030200@strategicdata.com.au>
References: <512572DF.3030200@strategicdata.com.au>
Message-ID: <CAD2iGhWJ-fhyUE608MN5JvskjiowZUNveFysYbP9pxKKz9Wc9g@mail.gmail.com>

Is the source for the Debian packaging available anywhere ?

I looked for it in the Github repo and on the Debian repo on
download.gluster.org, but I couldn't find it.


On Thu, Feb 21, 2013 at 2:05 AM, Toby Corkindale
<toby.corkindale at strategicdata.com.au> wrote:
> logrotate.d/glusterfs-common (in the debian package for 3.3.1) is faulty.
> It rotates the log files, but it doesn't tell glusterd to re-open them, so
> it continues to write to what is now .1 (and then later it gets gziped and
> corrupted)
>
> I also note that the debian packages do not include the man pages for any of
> the gluster binaries! (gluster, glusterfs, glusterd)
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users



--
Vennlig hilsen
Torbj?rn Thorsen
Utvikler / driftstekniker

Trollweb Solutions AS
- Professional Magento Partner
www.trollweb.no

Telefon dagtid: +47 51215300
Telefon kveld/helg: For kunder med Serviceavtale

Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
Postadresse: Maurholen 57, 4316 Sandnes

Husk at alle v?re standard-vilk?r alltid er gjeldende

From bdc at redhat.com  Fri Feb 22 00:07:31 2013
From: bdc at redhat.com (Brad Childs)
Date: Thu, 21 Feb 2013 19:07:31 -0500 (EST)
Subject: [Gluster-users] Error after removing/reinstall gluster
In-Reply-To: <1330915765.5760633.1361490249620.JavaMail.root@redhat.com>
Message-ID: <2008276417.5764907.1361491651180.JavaMail.root@redhat.com>

Hi All,

I expunged gluster from my system (yum remove gluster; yum remove gluster-fuse ; find . -name "*gluster*" -exec rm -rf {} \; )

After I re-intalled:
??
??wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo
??yum install glusterfs{-fuse,-server}

the 'gluster' command just returns. ?'gluster help' doesn't show anything either.

looking at /var/log/glusterfs/cli.log, i see:

[2013-02-21 17:41:52.906754] W [rpc-transport.c:174:rpc_transport_load] 0-rpc-transport: missing 'option transport-type'. defaulting to "socket"
[2013-02-21 17:41:52.906794] D [rpc-transport.c:248:rpc_transport_load] 0-rpc-transport: attempt to load file /usr/lib/glusterfs/3git/rpc-transport/socket.so
[2013-02-21 17:41:52.906855] E [rpc-transport.c:252:rpc_transport_load] 0-rpc-transport: /usr/lib/glusterfs/3git/rpc-transport/socket.so: cannot open shared object file: No such file or directory


what have i done?! ?any ideas how to fix?


-bc

From joe at julianfamily.org  Fri Feb 22 00:12:19 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 21 Feb 2013 16:12:19 -0800
Subject: [Gluster-users] Error after removing/reinstall gluster
In-Reply-To: <2008276417.5764907.1361491651180.JavaMail.root@redhat.com>
References: <2008276417.5764907.1361491651180.JavaMail.root@redhat.com>
Message-ID: <5126B7E3.3000501@julianfamily.org>

Somehow you're still looking at the version you build from source. The 
EPEL versions don't reference 3git, only a version built from source could.

On 02/21/2013 04:07 PM, Brad Childs wrote:
> Hi All,
>
> I expunged gluster from my system (yum remove gluster; yum remove gluster-fuse ; find . -name "*gluster*" -exec rm -rf {} \; )
>
> After I re-intalled:
>    
>    wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/EPEL.repo/glusterfs-epel.repo
>    yum install glusterfs{-fuse,-server}
>
> the 'gluster' command just returns.  'gluster help' doesn't show anything either.
>
> looking at /var/log/glusterfs/cli.log, i see:
>
> [2013-02-21 17:41:52.906754] W [rpc-transport.c:174:rpc_transport_load] 0-rpc-transport: missing 'option transport-type'. defaulting to "socket"
> [2013-02-21 17:41:52.906794] D [rpc-transport.c:248:rpc_transport_load] 0-rpc-transport: attempt to load file /usr/lib/glusterfs/3git/rpc-transport/socket.so
> [2013-02-21 17:41:52.906855] E [rpc-transport.c:252:rpc_transport_load] 0-rpc-transport: /usr/lib/glusterfs/3git/rpc-transport/socket.so: cannot open shared object file: No such file or directory
>
>
> what have i done?!  any ideas how to fix?
>
>
> -bc
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From joe at julianfamily.org  Fri Feb 22 00:18:07 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 21 Feb 2013 16:18:07 -0800
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512572DF.3030200@strategicdata.com.au>
References: <512572DF.3030200@strategicdata.com.au>
Message-ID: <5126B93F.4070604@julianfamily.org>

On 02/20/2013 05:05 PM, Toby Corkindale wrote:
> logrotate.d/glusterfs-common (in the debian package for 3.3.1) is faulty.
> It rotates the log files, but it doesn't tell glusterd to re-open 
> them, so it continues to write to what is now .1 (and then later it 
> gets gziped and corrupted)
>
> I also note that the debian packages do not include the man pages for 
> any of the gluster binaries! (gluster, glusterfs, glusterd)
Please file a bug report at 
https://bugs.launchpad.net/ubuntu/+source/glusterfs/+filebug
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130221/4629acbe/attachment.html>

From joe at julianfamily.org  Fri Feb 22 00:23:23 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 21 Feb 2013 16:23:23 -0800
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <CAD2iGhWJ-fhyUE608MN5JvskjiowZUNveFysYbP9pxKKz9Wc9g@mail.gmail.com>
References: <512572DF.3030200@strategicdata.com.au>
	<CAD2iGhWJ-fhyUE608MN5JvskjiowZUNveFysYbP9pxKKz9Wc9g@mail.gmail.com>
Message-ID: <5126BA7B.9010906@julianfamily.org>

On 02/21/2013 02:53 PM, Torbj?rn Thorsen wrote:
> Is the source for the Debian packaging available anywhere ?
>
> I looked for it in the Github repo and on the Debian repo on
> download.gluster.org, but I couldn't find it.
>
>
> On Thu, Feb 21, 2013 at 2:05 AM, Toby Corkindale
> <toby.corkindale at strategicdata.com.au> wrote:
>> logrotate.d/glusterfs-common (in the debian package for 3.3.1) is faulty.
>> It rotates the log files, but it doesn't tell glusterd to re-open them, so
>> it continues to write to what is now .1 (and then later it gets gziped and
>> corrupted)
>>
>> I also note that the debian packages do not include the man pages for any of
>> the gluster binaries! (gluster, glusterfs, glusterd)
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

The source can be found by viewing the package details at:
https://launchpad.net/~semiosis/+archive/ubuntu-glusterfs-3.3 
<https://launchpad.net/%7Esemiosis/+archive/ubuntu-glusterfs-3.3>

From toby.corkindale at strategicdata.com.au  Fri Feb 22 05:52:38 2013
From: toby.corkindale at strategicdata.com.au (Toby Corkindale)
Date: Fri, 22 Feb 2013 16:52:38 +1100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <5126B93F.4070604@julianfamily.org>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
Message-ID: <512707A6.3070906@strategicdata.com.au>

On 22/02/13 11:18, Joe Julian wrote:
> On 02/20/2013 05:05 PM, Toby Corkindale wrote:
>> logrotate.d/glusterfs-common (in the debian package for 3.3.1) is faulty.
>> It rotates the log files, but it doesn't tell glusterd to re-open
>> them, so it continues to write to what is now .1 (and then later it
>> gets gziped and corrupted)
>>
>> I also note that the debian packages do not include the man pages for
>> any of the gluster binaries! (gluster, glusterfs, glusterd)
> Please file a bug report at
> https://bugs.launchpad.net/ubuntu/+source/glusterfs/+filebug

Oh, I think you misunderstand -- I'm talking about the 
officially-provided-by-glusterfs debian packages here, not the ones 
created by Debian or Ubuntu themselves.


From toby.corkindale at strategicdata.com.au  Fri Feb 22 05:56:41 2013
From: toby.corkindale at strategicdata.com.au (Toby Corkindale)
Date: Fri, 22 Feb 2013 16:56:41 +1100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <CAD2iGhWJ-fhyUE608MN5JvskjiowZUNveFysYbP9pxKKz9Wc9g@mail.gmail.com>
References: <512572DF.3030200@strategicdata.com.au>
	<CAD2iGhWJ-fhyUE608MN5JvskjiowZUNveFysYbP9pxKKz9Wc9g@mail.gmail.com>
Message-ID: <51270899.4060803@strategicdata.com.au>

On 22/02/13 09:53, Torbj?rn Thorsen wrote:
> Is the source for the Debian packaging available anywhere ?
>
> I looked for it in the Github repo and on the Debian repo on
> download.gluster.org, but I couldn't find it.

The packages came from here:
http://download.gluster.org/pub/gluster/glusterfs/3.3/3.3.1/Debian/squeeze.repo/pool/main/g/glusterfs/

The source for those packages is here:
https://github.com/semiosis/glusterfs-debian




> On Thu, Feb 21, 2013 at 2:05 AM, Toby Corkindale
> <toby.corkindale at strategicdata.com.au> wrote:
>> logrotate.d/glusterfs-common (in the debian package for 3.3.1) is faulty.
>> It rotates the log files, but it doesn't tell glusterd to re-open them, so
>> it continues to write to what is now .1 (and then later it gets gziped and
>> corrupted)
>>
>> I also note that the debian packages do not include the man pages for any of
>> the gluster binaries! (gluster, glusterfs, glusterd)
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From nikagar17 at gmail.com  Fri Feb 22 08:17:25 2013
From: nikagar17 at gmail.com (Nikhil Agarwal)
Date: Fri, 22 Feb 2013 13:47:25 +0530
Subject: [Gluster-users] MapReduce on glusterfs in Hadoop
Message-ID: <CADtjTUJn-F5xvh8dgo+6KT8soJGdynQdqnjh3w0=PCXzEGOvTA@mail.gmail.com>

Hi All,



Thanks a lot for taking out your time to answer my question.



I am trying to implement a file system in hadoop under irg.apache.hadoop.fs
package something similar to KFS, glusterfs, etc. I wanted to know is that
in README.txt of glusterfs it is mentioned :



>> # ./bin/start-mapred.sh

  If the map/reduce job/task trackers are up, all I/O will be done to
GlusterFS.



So, suppose my input files are scattered in different nodes(glusterfs
servers), how do I(hadoop client having glusterfs plugged in) issue a
Mapreduce command?

Moreover, after issuing a Mapreduce command would my hadoop client fetch
all the data from different servers to my local machine and then do a
Mapreduce or would it start the TaskTracker daemons on the machine(s) where
the input file(s) are located and perform a Mapreduce there?

Please rectify me if I am wrong but I suppose that the location of input
files top Mapreduce is being returned by the function *getFileBlockLocations
* *(*FileStatus file*,* *long* start*,* *long* len*). *



Thank you very much for your time and helping me out.



Regards,

Nikhil
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130222/03707df5/attachment-0001.html>

From yongtaofu at gmail.com  Fri Feb 22 10:30:46 2013
From: yongtaofu at gmail.com (=?GB2312?B?t/vTwMzO?=)
Date: Fri, 22 Feb 2013 18:30:46 +0800
Subject: [Gluster-users] help posix_fallocate too slow on glusterfs client
Message-ID: <CADFMGu+-1ojvK1oKe+7eFoxz-mfT80kduvuH1eqqOZ1jeFWK_A@mail.gmail.com>

Dear gluster experts,

Recently I have encountered a problem about posix_fallocate
performance on glusterfs client.
I use posix_fallocate to allocate a file with specified size on
glusterfs client. For example if I create a file with size of
1907658896, it will take about 20 seconds on glusterfs but on local
xfs or ext4 it takes less than 1 second.
What's the problem? How can I improve posix_fallocate performance for glusterfs?

Thank you very much.

BTW
The volume info is as bellow:
sudo gluster volume info volume_e

Volume Name: volume_e
Type: Distributed-Replicate
Volume ID: 81702024-f327-4ae1-b06a-1f2b877d5ebb
Status: Started
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: glusterfs-test-dev001.qiyi.virtual:/mnt/xfsd/volume_e
Brick2: glusterfs-test-dev002.qiyi.virtual:/mnt/xfsd/volume_e
Brick3: glusterfs-test-dev003.qiyi.virtual:/mnt/xfsd/volume_e
Brick4: glusterfs-test-dev004.qiyi.virtual:/mnt/xfsd/volume_e

-- 
???

From sejal1.s at tcs.com  Fri Feb 22 10:42:19 2013
From: sejal1.s at tcs.com (Sejal1 S)
Date: Fri, 22 Feb 2013 16:12:19 +0530
Subject: [Gluster-users] Sync status of underlying bricks for replicated
	volume
In-Reply-To: <CADFMGu+-1ojvK1oKe+7eFoxz-mfT80kduvuH1eqqOZ1jeFWK_A@mail.gmail.com>
References: <CADFMGu+-1ojvK1oKe+7eFoxz-mfT80kduvuH1eqqOZ1jeFWK_A@mail.gmail.com>
Message-ID: <OF928EDD3C.CB618648-ON65257B1A.003A16C3-65257B1A.003ACEB1@tcs.com>

Hi Gluster Experts,


I am trying in integrate glusterfs as a replication file system in my 
product. 
As a background, I will be creating a distributed replicated (glusterfs) 
volume on two bricks present on two different server.

Can you please guide me to find out the sync status of all bricks under 
one volume i.e; given point of time whether all underlying bricks are in 
IN-SYNC, Out-Of-SYNC or synchronization is going on?

Please note I am naive in the glusterfs.


Thanks in anticipation.

Regards
Sejal 
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130222/40eb9bfe/attachment.html>

From rajesh at redhat.com  Fri Feb 22 11:56:31 2013
From: rajesh at redhat.com (Rajesh Amaravathi)
Date: Fri, 22 Feb 2013 06:56:31 -0500 (EST)
Subject: [Gluster-users] I/O error repaired only by owner or root access
In-Reply-To: <51263350.4090707@reading.ac.uk>
Message-ID: <10257952.36.1361534190609.JavaMail.raj@california>

Hi Dan,
Could you please provide the following info>
(1) the exact permissions of the file you are accessing and
its parent directory,
(2) the user from which 'ls -l' is issued, and
(3) the owner of the file, and the parent directory.

You could open a bug for it if it is seen several times.

Regards, 
Rajesh Amaravathi, 
Software Engineer, GlusterFS 
RedHat Inc. 

----- Original Message -----
> From: "Dan Bretherton" <d.a.bretherton at reading.ac.uk>
> To: "gluster-users" <gluster-users at gluster.org>
> Sent: Thursday, February 21, 2013 8:16:40 PM
> Subject: [Gluster-users] I/O error repaired only by owner or root access
> 
> Dear All-
> Several users are having a lot of trouble reading files belonging to
> other users.  Here is an example.
> 
> [sms05dab at jupiter ~]$ ls -l /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
> ls: /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl: Input/output error
> 
> The corresponding nfs.log messages are shown below.
> 
> [2013-02-21 12:11:39.204659] W [nfs3.c:727:nfs3svc_getattr_stat_cbk]
> 0-nfs: fe2ba5b8: /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN => -1
> (Invalid argument)
> [2013-02-21 12:11:39.204778] W
> [nfs3-helpers.c:3389:nfs3_log_common_res]
> 0-nfs-nfsv3: XID: fe2ba5b8, GETATTR: NFS: 22(Invalid argument for
> operation), POSIX: 22(Invalid argument)
> [2013-02-21 12:11:39.215345] I
> [dht-common.c:954:dht_lookup_everywhere_cbk] 0-nemo2-dht: deleting
> stale
> linkfile /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN on
> nemo2-replicate-0
> [2013-02-21 12:11:39.225674] W
> [client3_1-fops.c:592:client3_1_unlink_cbk] 0-nemo2-client-1: remote
> operation failed: Permission denied
> [2013-02-21 12:11:39.225786] W
> [client3_1-fops.c:592:client3_1_unlink_cbk] 0-nemo2-client-0: remote
> operation failed: Permission denied
> [2013-02-21 12:11:39.681029] W
> [client3_1-fops.c:258:client3_1_mknod_cbk] 0-nemo2-client-18: remote
> operation failed: Permission denied. Path:
> /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
> (1662aa0a-d43b-4c2e-9be9-407eb7a89e85)
> [2013-02-21 12:11:39.681400] W
> [client3_1-fops.c:258:client3_1_mknod_cbk] 0-nemo2-client-19: remote
> operation failed: Permission denied. Path:
> /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
> (1662aa0a-d43b-4c2e-9be9-407eb7a89e85)
> [2013-02-21 12:11:39.682268] W [nfs3.c:1627:nfs3svc_readlink_cbk]
> 0-nfs:
> 2ca5b8: /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN => -1 (Invalid
> argument)
> [2013-02-21 12:11:39.682338] W
> [nfs3-helpers.c:3403:nfs3_log_readlink_res] 0-nfs-nfsv3: XID: 2ca5b8,
> READLINK: NFS: 22(Invalid argument for operation), POSIX: 22(Invalid
> argument), target: (null)
> 
> I managed to access the same directory as the owner (or any user with
> write access including root) without any trouble, and after that
> access
> from my normal user account was fine as well.  The permissions on the
> directory allowed read access by everyone, but the "Permission
> denied"
> messages in nfs.log indicate that some sort of operation is not being
> allowed when the directory is accessed by other users.   I have seen
> this happen with files and directories, and with the GlusterFS native
> client and NFS.
> 
> I presume this is a bug; I would be grateful if someone could confirm
> this.  I would file a bug report, but the trouble is that I don't
> know
> how to reproduce the problem that causes the I/O error in the first
> place.  It only happens with some files and directories, not all.
>  Would
> a bug report without any way to reproduce the error be any use, and
> can
> anyone suggest a way to dig deeper (eg looking at xattrs) next time I
> come across an example?
> 
> -Dan.
> 
> --
> Dan Bretherton
> ESSC Computer System Manager
> Department of Meteorology
> Harry Pitt Building, 3 Earley Gate
> University of Reading
> Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
> UK
> Tel. +44 118 378 5205, Fax: +44 118 378 6413
> --
> ## Please sponsor me to run in VSO's 30km Race to the Eye ##
> ##        http://www.justgiving.com/DanBretherton         ##
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 

From d.a.bretherton at reading.ac.uk  Fri Feb 22 16:01:10 2013
From: d.a.bretherton at reading.ac.uk (Dan Bretherton)
Date: Fri, 22 Feb 2013 16:01:10 +0000
Subject: [Gluster-users] Is having millions of directories unusual?
Message-ID: <51279646.5080009@reading.ac.uk>

Dear All-
I have just discovered that the volume I've been having the most trouble 
with contains over 3.3 million directories.  Most of these are output 
from a storm tracking model that produces output in a large number of 
directories, each containing a handful of small files.  Is this number 
of directories in a distributed-replicate volume unusual, and should I 
expect it to cause problems for GlusterFS?  The layout fix operation 
that follows the addition of new bricks takes a very long time 
(weeks-months) and seems to result in a high CPU load.  The volume has a 
capacity of 52TB and the bricks are 3.3TB in size.

-Dan.

--
Dan Bretherton
ESSC Computer System Manager
Department of Meteorology
Harry Pitt Building, 3 Earley Gate
University of Reading
Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
UK
Tel. +44 118 378 5205, Fax: +44 118 378 6413
-- 
## Please sponsor me to run in VSO's 30km Race to the Eye ##
##        http://www.justgiving.com/DanBretherton         ##


From jayunit100 at gmail.com  Fri Feb 22 16:55:52 2013
From: jayunit100 at gmail.com (Jay Vyas)
Date: Fri, 22 Feb 2013 11:55:52 -0500
Subject: [Gluster-users] MapReduce on glusterfs in Hadoop
In-Reply-To: <CADtjTUJn-F5xvh8dgo+6KT8soJGdynQdqnjh3w0=PCXzEGOvTA@mail.gmail.com>
References: <CADtjTUJn-F5xvh8dgo+6KT8soJGdynQdqnjh3w0=PCXzEGOvTA@mail.gmail.com>
Message-ID: <42B4C3E1-DFC8-4C8F-A010-70594B1197EA@gmail.com>

Hi nithin: 

The fuse mount is what allows the filesystem to access distributed files in gluster: that is,  GlusterFS has its own fuse mount ...  And GlusterFileSystem wraps that in hadoop FileSystem semantics.

Meanwhile, The mapreduce jobs are invoked using on custom core-site and mapred-site XML nodes which specify GlusterFileSystem as the dfs.

On Feb 22, 2013, at 3:17 AM, Nikhil Agarwal <nikagar17 at gmail.com> wrote:

> Hi All,
> 
>  
> 
> Thanks a lot for taking out your time to answer my question.
> 
>  
> 
> I am trying to implement a file system in hadoop under irg.apache.hadoop.fs package something similar to KFS, glusterfs, etc. I wanted to know is that in README.txt of glusterfs it is mentioned :
> 
>  
> 
> >> # ./bin/start-mapred.sh
>   If the map/reduce job/task trackers are up, all I/O will be done to GlusterFS.
> 
>  
> 
> So, suppose my input files are scattered in different nodes(glusterfs servers), how do I(hadoop client having glusterfs plugged in) issue a Mapreduce command?
> 
> Moreover, after issuing a Mapreduce command would my hadoop client fetch all the data from different servers to my local machine and then do a Mapreduce or would it start the TaskTracker daemons on the machine(s) where the input file(s) are located and perform a Mapreduce there?
> 
> Please rectify me if I am wrong but I suppose that the location of input files top Mapreduce is being returned by the function getFileBlockLocations (FileStatus file, long start, long len).
> 
>  
> 
> Thank you very much for your time and helping me out.
> 
>  
> 
> Regards,
> 
> Nikhil
> 
>  
> 
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130222/d1e54e41/attachment.html>

From joe at julianfamily.org  Fri Feb 22 17:27:04 2013
From: joe at julianfamily.org (Joe Julian)
Date: Fri, 22 Feb 2013 09:27:04 -0800
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512707A6.3070906@strategicdata.com.au>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
	<512707A6.3070906@strategicdata.com.au>
Message-ID: <2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>

Yes, I am aware. Louis' packaging is part of the official debian development and bugs against it are tracked at launchpad. 

Toby Corkindale <toby.corkindale at strategicdata.com.au> wrote:

>On 22/02/13 11:18, Joe Julian wrote:
>> On 02/20/2013 05:05 PM, Toby Corkindale wrote:
>>> logrotate.d/glusterfs-common (in the debian package for 3.3.1) is
>faulty.
>>> It rotates the log files, but it doesn't tell glusterd to re-open
>>> them, so it continues to write to what is now .1 (and then later it
>>> gets gziped and corrupted)
>>>
>>> I also note that the debian packages do not include the man pages
>for
>>> any of the gluster binaries! (gluster, glusterfs, glusterd)
>> Please file a bug report at
>> https://bugs.launchpad.net/ubuntu/+source/glusterfs/+filebug
>
>Oh, I think you misunderstand -- I'm talking about the 
>officially-provided-by-glusterfs debian packages here, not the ones 
>created by Debian or Ubuntu themselves.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130222/cf732592/attachment.html>

From tony at filmsolutions.com  Fri Feb 22 21:50:26 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Fri, 22 Feb 2013 21:50:26 +0000
Subject: [Gluster-users] Peer Probe
Message-ID: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>

Hey,

I was wondering if I could get a bit of help.. I installed a new Infiniband card into my servers but I'm unable to get it to come up as a peer. Is there something I'm missing?

[root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
Probe on host fpsgluster2ib port 0 already in peer list

[root at fpsgluster testvault]# yum list installed | grep gluster
glusterfs.x86_64                       3.3.1-1.el6              installed       
glusterfs-devel.x86_64                 3.3.1-1.el6              installed       
glusterfs-fuse.x86_64                  3.3.1-1.el6              installed       
glusterfs-geo-replication.x86_64       3.3.1-1.el6              installed       
glusterfs-rdma.x86_64                  3.3.1-1.el6              installed       
glusterfs-server.x86_64                3.3.1-1.el6              installed   

Thanks. 

From gandalf.corvotempesta at gmail.com  Sun Feb 24 11:52:53 2013
From: gandalf.corvotempesta at gmail.com (Gandalf Corvotempesta)
Date: Sun, 24 Feb 2013 12:52:53 +0100
Subject: [Gluster-users] Gluster Virtual Appliance
Message-ID: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>

Hi all,
i'm planning an installation where 4 gluster nodes are virtualized by
KVM. One gluster for one phisical server (4 in total)
The same phisical node hosting gluster image, will also host some
machines that should boot from gluster.
Obviously, gluster should be started at first.

Recap: 4 phisical nodes, each node will host at least 10 VM plus 1 gluster VM
Each VM should boot from the gluster VM

Do you have any advice on this configuration?

From tompos at martos.bme.hu  Sun Feb 24 12:15:03 2013
From: tompos at martos.bme.hu (Papp Tamas)
Date: Sun, 24 Feb 2013 13:15:03 +0100
Subject: [Gluster-users] Gluster Virtual Appliance
In-Reply-To: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
References: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
Message-ID: <512A0447.8030905@martos.bme.hu>

On 02/24/2013 12:52 PM, Gandalf Corvotempesta wrote:
>
> Hi all,
> i'm planning an installation where 4 gluster nodes are virtualized by
> KVM. One gluster for one phisical server (4 in total)
> The same phisical node hosting gluster image, will also host some
> machines that should boot from gluster.
> Obviously, gluster should be started at first.
>
> Recap: 4 phisical nodes, each node will host at least 10 VM plus 1 gluster VM
> Each VM should boot from the gluster VM
>
> Do you have any advice on this configuration?

What does it make sense to keep gluster on VMs?

tamas


From matt at redhat.com  Sun Feb 24 13:40:29 2013
From: matt at redhat.com (Matthew Farrellee)
Date: Sun, 24 Feb 2013 08:40:29 -0500
Subject: [Gluster-users] Gluster Virtual Appliance
In-Reply-To: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
References: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
Message-ID: <512A184D.5000003@redhat.com>

On 02/24/2013 06:52 AM, Gandalf Corvotempesta wrote:
> Hi all,
> i'm planning an installation where 4 gluster nodes are virtualized by
> KVM. One gluster for one phisical server (4 in total)
> The same phisical node hosting gluster image, will also host some
> machines that should boot from gluster.
> Obviously, gluster should be started at first.
>
> Recap: 4 phisical nodes, each node will host at least 10 VM plus 1 gluster VM
> Each VM should boot from the gluster VM
>
> Do you have any advice on this configuration?

If I understand you, the boot path is,

  0. boot 4 physical servers
  1. boot 4 gluster VMs, one per server w/ mapped physical disks
  2. mount gluster volumes on each server
  3. boot generic VMs, 10 per server

If you didn't do gluster VMs,

  a. boot 4 physical servers
  b. mount gluster volumes on each server
  c. boot generic VMs, 10 per server

Doing non-virt gluster saves storing the gluster images (small, 
15-20GB), the mapping of physical disks into the gluster VMs, and any 
potential overhead introduced by gluster working against virtualized disks.

Unless you have some reason to put gluster in a VM, some advice is to 
keep it physical.

You didn't mention it, but the 40 VMs might have just their boot 
partitions mounted from gluster or have both their boot and data 
partitions mounted from gluster. If they have both, you'll want to pay 
special attention to the w/r path through gluster (skip virt, possibly 
multiple volumes w/ bricks spread over spindles).

Best,


matt

From gandalf.corvotempesta at gmail.com  Sun Feb 24 21:08:19 2013
From: gandalf.corvotempesta at gmail.com (Gandalf Corvotempesta)
Date: Sun, 24 Feb 2013 22:08:19 +0100
Subject: [Gluster-users] Gluster Virtual Appliance
In-Reply-To: <512A184D.5000003@redhat.com>
References: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
	<512A184D.5000003@redhat.com>
Message-ID: <CAJH6TXiggzeG=cQQyTSMWDM-OwLscBqwanbn-Q4HHvN8TXbYoA@mail.gmail.com>

2013/2/24 Matthew Farrellee <matt at redhat.com>:
> If I understand you, the boot path is,
>
>  0. boot 4 physical servers
>  1. boot 4 gluster VMs, one per server w/ mapped physical disks
>  2. mount gluster volumes on each server
>  3. boot generic VMs, 10 per server

Correct.

> Doing non-virt gluster saves storing the gluster images (small, 15-20GB),
> the mapping of physical disks into the gluster VMs, and any potential
> overhead introduced by gluster working against virtualized disks.
>
> Unless you have some reason to put gluster in a VM, some advice is to keep
> it physical.

Ok, i'll go with phisical installation

From toby.corkindale at strategicdata.com.au  Mon Feb 25 04:41:51 2013
From: toby.corkindale at strategicdata.com.au (Toby Corkindale)
Date: Mon, 25 Feb 2013 15:41:51 +1100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
	<512707A6.3070906@strategicdata.com.au>
	<2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>
Message-ID: <512AEB8F.304@strategicdata.com.au>

I have filed separate bugs for both the missing man pages, and the 
failed log rotations there.

In the meantime, could someone advise me on the correct way to tell 
Glusterfs to rotate the logs for the bricks and mounts?

I mean, it's easy to do a kill -HUP `cat /var/run/glusterd.pid`, but how 
am I supposed to find the PIDs for the gluster mounts on the clients, or 
the individual brick daemons on the server?

Thanks,
Toby


On 23/02/13 04:27, Joe Julian wrote:
> Yes, I am aware. Louis' packaging is part of the official debian
> development and bugs against it are tracked at launchpad.
>
> Toby Corkindale <toby.corkindale at strategicdata.com.au> wrote:
>
>     On 22/02/13 11:18, Joe Julian wrote:
>
>         On 02/20/2013 05:05 PM, Toby Corkindale wrote:
>
>             logrotate.d/glusterfs-common (in the debian package for
>             3.3.1) is faulty.
>             It rotates the log files, but it doesn't tell glusterd to
>             re-open
>             them, so it continues to write to what is now .1 (and then
>             later it
>             gets gziped and corrupted)
>
>             I also note that the debian packages do not include the man
>             pages for
>             any of the gluster binaries! (gluster, glusterfs, glusterd)
>
>         Please file a bug report at
>         https://bugs.launchpad.net/ubuntu/+source/glusterfs/+filebug
>
>
>     Oh, I think you misunderstand -- I'm talking about the
>     officially-provided-by-glusterfs debian packages here, not the ones
>     created by Debian or Ubuntu themselves.
>


From amarts at redhat.com  Mon Feb 25 05:21:15 2013
From: amarts at redhat.com (Amar Tumballi)
Date: Mon, 25 Feb 2013 10:51:15 +0530
Subject: [Gluster-users] [Gluster-devel] Automated regression tests now
 part of development work flow
In-Reply-To: <CAFboF2yuveTTwLnyUQLQdGVtVJtqQx-5S3SabcTYX84zGvrNPg@mail.gmail.com>
References: <CAFboF2yuveTTwLnyUQLQdGVtVJtqQx-5S3SabcTYX84zGvrNPg@mail.gmail.com>
Message-ID: <512AF4CB.3030207@redhat.com>

On 10/21/2012 02:53 AM, Anand Avati wrote:

> More details on how to write test cases with examples and how the
> framework works can be found at -
>
> 1. "tests: pre-commit regression tests"
> https://github.com/gluster/glusterfs/commit/bb41c8ab88f1a3d8c54b635674d0a72133623496
> 2. tests/README in glusterfs.git
> 3. Example new change - http://review.gluster.org/4114
>
> Please offer feedback on how the framework can be improved (inclusion of
> more tools?) to capture test cases better.
>

This is a simple question on guide lines of writing a test case, and has 
raised due to failure of tests for patch http://review.gluster.org/4567 
in particular.

Looks like the failure of the tests for this patch (logs here 
http://build.gluster.org/job/regression/729/consoleFull ) are due to 
missing 'cleanup;' at the end of the script added in this patch. But the 
question is, should the test script always 'assume' it starts from a 
clean slate, or should it call a 'cleanup;' at the beginning all the time?

It would help me to review the patchsets by having clear answer to this.

Regards,
Amar


From purpleidea at gmail.com  Mon Feb 25 05:38:51 2013
From: purpleidea at gmail.com (James)
Date: Mon, 25 Feb 2013 00:38:51 -0500
Subject: [Gluster-users] Automated regression tests now part of
 development work flow
In-Reply-To: <CAFboF2yuveTTwLnyUQLQdGVtVJtqQx-5S3SabcTYX84zGvrNPg@mail.gmail.com>
References: <CAFboF2yuveTTwLnyUQLQdGVtVJtqQx-5S3SabcTYX84zGvrNPg@mail.gmail.com>
Message-ID: <1361770731.2557.10.camel@freed.purpleidea.com>

On Sat, 2012-10-20 at 14:23 -0700, Anand Avati wrote:
> Hello all,
> 
> The recent ongoing changes to the development work flow are now complete.
> The highlight of the rework is the automated regression tests.
This is awesome!

> 
> So far we have only had fixed smoke tests that were run against every patch
> pre-commit (dbench, POSIX compliance). Here after, every change submitted
> to Gerrit must be accompanied with test case scripts which (attempts to)
> prove the correctness of the code change - as part of the new regression
> test in the work flow.
> 
> These test cases will be much more concentrated and focused on the changes
> brought in by the patch itself. The test cases, once committed will be part
> of every future pre-commit test. This will make sure no commit in the
> future will accidentally break or change the current commit either directly
> or with side effects. If a future change must change the behavior of the
> current patch, then the future patch must include corresponding changes to
> the test cases as well. This policy will keep code and test cases in sync.
I don't suppose someone knows offhand of a way to automatically "watch"
for proposals to certain test cases? This could be a useful way for a
programmer to get a chance to speak up about an upcoming incompatible
change. Anyways, low priority, if someone has already hacked this
together with git, or if you'd be interested in this sort of thing, let
me know.

> 
> The wiring and framework for supporting this are already available in
> glusterfs.git and Jenkins. Currently the infrastructure supports test cases
> limited to single node. Most of the test cases can actually be implemented
> in single node by having multiple instances of bricks and client mounts.
> Support of multi node test cases will be added soon.
> 
> The automated smoke test's voting power in Gerrit is now downgraded to +0
> Verified for PASS and -1 Verified for FAILURE. Regression test's voting
> powers are now +1 Verified and -1 Verified. This means just passing the
> smoke test can no more be a qualifier for verification. For those patches
> for which a regression test is required but currently not automatable into
> a test script, must be tested and voted manually for the Verification vote.
> 
> As the above described changes are already in effect, most of the currently
> submitted patches under review will have to be resubmitted with test cases.
> This is necessary to make the changes start being effective immediately. If
> you have currently
> 
> Note that this opens up a new avenue for contributors in the community. You
> can now contribute test cases. These contributions need not include source
> code changes but just extend our regression test set. These test cases can
> either be for coverage of old code and functionality, or for coverage of
> your use cases which could be applicable to others as well.
I will put it onto my TODO list to write some test cases that "watch"
the functionality used by my puppet-gluster module.

> 
> The workflow document is also updated at
> http://www.gluster.org/community/documentation/index.php/Development_Work_Flow.
> Specifically:
> Sec 1.4
> http://www.gluster.org/community/documentation/index.php/Development_Work_Flow#Commit_policy
> Sec 1.9
> http://www.gluster.org/community/documentation/index.php/Development_Work_Flow#Regression_tests_and_test_cases
> 
> More details on how to write test cases with examples and how the framework
> works can be found at -
> 
> 1. "tests: pre-commit regression tests"
> https://github.com/gluster/glusterfs/commit/bb41c8ab88f1a3d8c54b635674d0a72133623496
> 2. tests/README in glusterfs.git
> 3. Example new change - http://review.gluster.org/4114
> 
> Please offer feedback on how the framework can be improved (inclusion of
> more tools?) to capture test cases better.
> 
> Happy Hacking!
> Avati
Thanks!!
James

> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: This is a digitally signed message part
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130225/385f0f4d/attachment.sig>

From lanning at lanning.cc  Mon Feb 25 09:50:57 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Mon, 25 Feb 2013 01:50:57 -0800
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512AEB8F.304@strategicdata.com.au>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
	<512707A6.3070906@strategicdata.com.au>
	<2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>
	<512AEB8F.304@strategicdata.com.au>
Message-ID: <512B3401.3010503@lanning.cc>

On 02/24/13 20:41, Toby Corkindale wrote:
> In the meantime, could someone advise me on the correct way to tell
> Glusterfs to rotate the logs for the bricks and mounts?

Have you tried:
# gluster volume log rotate <VOLNAME> [BRICK]

-- 
Mr. Flibble
King of the Potato People

From B.Candler at pobox.com  Mon Feb 25 10:09:47 2013
From: B.Candler at pobox.com (Brian Candler)
Date: Mon, 25 Feb 2013 10:09:47 +0000
Subject: [Gluster-users] Gluster Virtual Appliance
In-Reply-To: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
References: <CAJH6TXiN1wpgje4yN=CPj0xbmbThN1MhjMrVmvctkm6Uuy_RJg@mail.gmail.com>
Message-ID: <20130225100947.GB58072@nsrc.org>

On Sun, Feb 24, 2013 at 12:52:53PM +0100, Gandalf Corvotempesta wrote:
> Recap: 4 phisical nodes, each node will host at least 10 VM plus 1 gluster VM
> Each VM should boot from the gluster VM

By "boot from" I guess you mean that the VM's root device, e.g. hda/vda,
will be a disk image file stored on the gluster filesystem?

> Do you have any advice on this configuration?

Yes: test it carefully to ensure it does what you want.

* In my experience, write performance of KVM -> FUSE mount -> gluster is
very poor (I was getting about 6MB/s).  In your case you have another layer
of KVM in this too.

* Test carefully all the various failure scenarios, e.g. halting and
restarting the gluster VMs, rebooting the whole server, pulling the power
out of the whole server and restarting it.  Better to learn the failure
scenarios here than when in production, because Gluster has precious little
documentation on how to cope with them.

If the *only* thing you need to do is provide backing storage for VMs, then
there are other solutions which might suit you better - you could look at
Ganeti and Sheepdog.

Ganeti uses LVM to create storage volumes for each VM and then creates DRBD
instances on top of them to synchronise replicas between hosts.  It provides
a full VM cluster manager too, so the command line lets you manage all your
VMs from one point.

Sheepdog provides a virtual block-storage layer for KVM, where chunks of
each volume are distributed and replicated between hosts.

However, neither provides a general-purpose shared filesystem as Gluster
does.

Regards,

Brian.

From amarts at redhat.com  Mon Feb 25 11:04:23 2013
From: amarts at redhat.com (Amar Tumballi)
Date: Mon, 25 Feb 2013 16:34:23 +0530
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512B3401.3010503@lanning.cc>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
	<512707A6.3070906@strategicdata.com.au>
	<2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>
	<512AEB8F.304@strategicdata.com.au> <512B3401.3010503@lanning.cc>
Message-ID: <512B4537.90706@redhat.com>

On 02/25/2013 03:20 PM, Robert Hajime Lanning wrote:
> On 02/24/13 20:41, Toby Corkindale wrote:
>> In the meantime, could someone advise me on the correct way to tell
>> Glusterfs to rotate the logs for the bricks and mounts?
>
> Have you tried:
> # gluster volume log rotate <VOLNAME> [BRICK]
>

This should work, but in future, we would like to depend on 'logrotate' 
package and be friendly with it, so that it gets triggered at regular 
interval, and as an admin, users of gluster need not bother about log 
file rotations.

So, would like experts in this area to help us with enhancing 
'extras/glusterfs-logrotate' [1] logrotate config file.

-Amar

[1] - 
https://github.com/gluster/glusterfs/blob/master/extras/glusterfs-logrotate

From torbjorn at trollweb.no  Mon Feb 25 11:16:37 2013
From: torbjorn at trollweb.no (=?ISO-8859-1?Q?Torbj=F8rn_Thorsen?=)
Date: Mon, 25 Feb 2013 12:16:37 +0100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512B4537.90706@redhat.com>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
	<512707A6.3070906@strategicdata.com.au>
	<2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>
	<512AEB8F.304@strategicdata.com.au> <512B3401.3010503@lanning.cc>
	<512B4537.90706@redhat.com>
Message-ID: <CAD2iGhVPfY9Qp-J8tU+va1LCmfpfebRH6Eq5sR-0ywt=rCUMvA@mail.gmail.com>

If the gluster binaries accept SIGHUP and re-opens their logging fd's
on that signal,
then the current logrotate script doesn't seem too bad.
I'm guessing the logrotate stuff in the github repo is newer than the
3.3.1 packages from the download.gluster.org repo, as that is not the
version I have installed.

It's maybe not ideal that it will send SIGHUP to all instances of the
binaries, though, and not only the one instance managed by the service
scripts.

In my experience, implementing a "/etc/init.d/$SERVICE reload" command
that handles the signal sending, and then having the logrotate script
call that works pretty well.
The service management script in /etc/init.d/ has a clearer way of the
PID file handling, and can probably pick out the correct instance of
glusterd and the others.

I could give this idea a whirl if there is any interest in using this approach.

On Mon, Feb 25, 2013 at 12:04 PM, Amar Tumballi <amarts at redhat.com> wrote:
> On 02/25/2013 03:20 PM, Robert Hajime Lanning wrote:
>>
>> On 02/24/13 20:41, Toby Corkindale wrote:
>>>
>>> In the meantime, could someone advise me on the correct way to tell
>>> Glusterfs to rotate the logs for the bricks and mounts?
>>
>>
>> Have you tried:
>> # gluster volume log rotate <VOLNAME> [BRICK]
>>
>
> This should work, but in future, we would like to depend on 'logrotate'
> package and be friendly with it, so that it gets triggered at regular
> interval, and as an admin, users of gluster need not bother about log file
> rotations.
>
> So, would like experts in this area to help us with enhancing
> 'extras/glusterfs-logrotate' [1] logrotate config file.
>
> -Amar
>
> [1] -
> https://github.com/gluster/glusterfs/blob/master/extras/glusterfs-logrotate
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users



--
Vennlig hilsen
Torbj?rn Thorsen
Utvikler / driftstekniker

Trollweb Solutions AS
- Professional Magento Partner
www.trollweb.no

Telefon dagtid: +47 51215300
Telefon kveld/helg: For kunder med Serviceavtale

Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
Postadresse: Maurholen 57, 4316 Sandnes

Husk at alle v?re standard-vilk?r alltid er gjeldende

From d.a.bretherton at reading.ac.uk  Mon Feb 25 13:16:47 2013
From: d.a.bretherton at reading.ac.uk (Dan Bretherton)
Date: Mon, 25 Feb 2013 13:16:47 +0000
Subject: [Gluster-users] I/O error repaired only by owner or root access
In-Reply-To: <10257952.36.1361534190609.JavaMail.raj@california>
References: <10257952.36.1361534190609.JavaMail.raj@california>
Message-ID: <512B643F.6030706@reading.ac.uk>

Hello Rajesh- Here are the permissions.  The path in question is a 
directory.

[sms05dab at jupiter ~]$ ls -ld /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
drwxr-xr-x 60 vq901510 nemo 110592 Feb 23 04:37 
/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
[sms05dab at jupiter ~]$ ls -ld /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
lrwxrwxrwx 1 gcs nemo 49 Feb  1  2012 
/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN -> 
/data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
[sms05dab at jupiter ~]$ ls -ld 
/data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
drwxr-xr-x 27 gcs nemo 99210 Feb 23 03:14 
/data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN

As you can see the parent directory in this case was a symlink but 
that's not significant.  I ran the "ls -l" commands using my account - 
sms05dab, but the problem was originally reported by user vq901510.  
until I did "ls -l" as root neither of us could access the directory, 
because the parent directory was owned by user gcs.  Usually the problem 
is related to ownership of the file or directory itself.  This is the 
first time I have seen the I/O error caused by parent directory permissions.

This problem seems to have started following an add-brick operation a 
few weeks ago, after which I started "gluster volume rebalance <VOLNAME> 
fix-layout" (which is still running). It occurred to me that the problem 
could be related to link files, many of which need to be rewritten 
following add-brick operations.  This could explain why the ownership of 
the parent directory is significant, because users sms05dab and vq901510 
don't have permission to write in the parent directory owned by user 
gcs.  Normally this wouldn't be a problem because only read access to 
other users' data is required, but it appears as though read access was 
being denied because the new link file couldn't be written by 
unprivileged users.  Is this a plausible explanation of the I/O error do 
you think?

-Dan.

--
Dan Bretherton
ESSC Computer System Manager
Department of Meteorology
Harry Pitt Building, 3 Earley Gate
University of Reading
Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
UK
Tel. +44 118 378 5205, Fax: +44 118 378 6413
-- 
## Please sponsor me to run in VSO's 30km Race to the Eye ##
##        http://www.justgiving.com/DanBretherton         ##

On 02/22/2013 11:56 AM, Rajesh Amaravathi wrote:
> Hi Dan,
> Could you please provide the following info>
> (1) the exact permissions of the file you are accessing and
> its parent directory,
> (2) the user from which 'ls -l' is issued, and
> (3) the owner of the file, and the parent directory.
>
> You could open a bug for it if it is seen several times.
>
> Regards,
> Rajesh Amaravathi,
> Software Engineer, GlusterFS
> RedHat Inc.
>
> ----- Original Message -----
>> From: "Dan Bretherton" <d.a.bretherton at reading.ac.uk>
>> To: "gluster-users" <gluster-users at gluster.org>
>> Sent: Thursday, February 21, 2013 8:16:40 PM
>> Subject: [Gluster-users] I/O error repaired only by owner or root access
>>
>> Dear All-
>> Several users are having a lot of trouble reading files belonging to
>> other users.  Here is an example.
>>
>> [sms05dab at jupiter ~]$ ls -l /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
>> ls: /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl: Input/output error
>>
>> The corresponding nfs.log messages are shown below.
>>
>> [2013-02-21 12:11:39.204659] W [nfs3.c:727:nfs3svc_getattr_stat_cbk]
>> 0-nfs: fe2ba5b8: /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN => -1
>> (Invalid argument)
>> [2013-02-21 12:11:39.204778] W
>> [nfs3-helpers.c:3389:nfs3_log_common_res]
>> 0-nfs-nfsv3: XID: fe2ba5b8, GETATTR: NFS: 22(Invalid argument for
>> operation), POSIX: 22(Invalid argument)
>> [2013-02-21 12:11:39.215345] I
>> [dht-common.c:954:dht_lookup_everywhere_cbk] 0-nemo2-dht: deleting
>> stale
>> linkfile /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN on
>> nemo2-replicate-0
>> [2013-02-21 12:11:39.225674] W
>> [client3_1-fops.c:592:client3_1_unlink_cbk] 0-nemo2-client-1: remote
>> operation failed: Permission denied
>> [2013-02-21 12:11:39.225786] W
>> [client3_1-fops.c:592:client3_1_unlink_cbk] 0-nemo2-client-0: remote
>> operation failed: Permission denied
>> [2013-02-21 12:11:39.681029] W
>> [client3_1-fops.c:258:client3_1_mknod_cbk] 0-nemo2-client-18: remote
>> operation failed: Permission denied. Path:
>> /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>> (1662aa0a-d43b-4c2e-9be9-407eb7a89e85)
>> [2013-02-21 12:11:39.681400] W
>> [client3_1-fops.c:258:client3_1_mknod_cbk] 0-nemo2-client-19: remote
>> operation failed: Permission denied. Path:
>> /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>> (1662aa0a-d43b-4c2e-9be9-407eb7a89e85)
>> [2013-02-21 12:11:39.682268] W [nfs3.c:1627:nfs3svc_readlink_cbk]
>> 0-nfs:
>> 2ca5b8: /gorgon/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN => -1 (Invalid
>> argument)
>> [2013-02-21 12:11:39.682338] W
>> [nfs3-helpers.c:3403:nfs3_log_readlink_res] 0-nfs-nfsv3: XID: 2ca5b8,
>> READLINK: NFS: 22(Invalid argument for operation), POSIX: 22(Invalid
>> argument), target: (null)
>>
>> I managed to access the same directory as the owner (or any user with
>> write access including root) without any trouble, and after that
>> access
>> from my normal user account was fine as well.  The permissions on the
>> directory allowed read access by everyone, but the "Permission
>> denied"
>> messages in nfs.log indicate that some sort of operation is not being
>> allowed when the directory is accessed by other users.   I have seen
>> this happen with files and directories, and with the GlusterFS native
>> client and NFS.
>>
>> I presume this is a bug; I would be grateful if someone could confirm
>> this.  I would file a bug report, but the trouble is that I don't
>> know
>> how to reproduce the problem that causes the I/O error in the first
>> place.  It only happens with some files and directories, not all.
>>   Would
>> a bug report without any way to reproduce the error be any use, and
>> can
>> anyone suggest a way to dig deeper (eg looking at xattrs) next time I
>> come across an example?
>>
>> -Dan.
>>
>> --
>> Dan Bretherton
>> ESSC Computer System Manager
>> Department of Meteorology
>> Harry Pitt Building, 3 Earley Gate
>> University of Reading
>> Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
>> UK
>> Tel. +44 118 378 5205, Fax: +44 118 378 6413
>> --
>> ## Please sponsor me to run in VSO's 30km Race to the Eye ##
>> ##        http://www.justgiving.com/DanBretherton         ##
>>
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>


From vbellur at redhat.com  Mon Feb 25 14:29:51 2013
From: vbellur at redhat.com (Vijay Bellur)
Date: Mon, 25 Feb 2013 19:59:51 +0530
Subject: [Gluster-users] [Gluster-devel] Automated regression tests now
 part of development work flow
In-Reply-To: <512AF4CB.3030207@redhat.com>
References: <CAFboF2yuveTTwLnyUQLQdGVtVJtqQx-5S3SabcTYX84zGvrNPg@mail.gmail.com>
	<512AF4CB.3030207@redhat.com>
Message-ID: <512B755F.609@redhat.com>

On 02/25/2013 10:51 AM, Amar Tumballi wrote:
> On 10/21/2012 02:53 AM, Anand Avati wrote:
>
>> More details on how to write test cases with examples and how the
>> framework works can be found at -
>>
>> 1. "tests: pre-commit regression tests"
>> https://github.com/gluster/glusterfs/commit/bb41c8ab88f1a3d8c54b635674d0a72133623496
>>
>> 2. tests/README in glusterfs.git
>> 3. Example new change - http://review.gluster.org/4114
>>
>> Please offer feedback on how the framework can be improved (inclusion of
>> more tools?) to capture test cases better.
>>
>
> This is a simple question on guide lines of writing a test case, and has
> raised due to failure of tests for patch http://review.gluster.org/4567
> in particular.
>
> Looks like the failure of the tests for this patch (logs here
> http://build.gluster.org/job/regression/729/consoleFull ) are due to
> missing 'cleanup;' at the end of the script added in this patch. But the
> question is, should the test script always 'assume' it starts from a
> clean slate, or should it call a 'cleanup;' at the beginning all the time?
>
> It would help me to review the patchsets by having clear answer to this.
>

I consider bug-000000.t to be the template and it does contain cleanup 
at the end of it. Maybe we should add this detail to tests/README?

-Vijay


From nock at nocko.se  Mon Feb 25 15:49:36 2013
From: nock at nocko.se (Shawn Nock)
Date: Mon, 25 Feb 2013 10:49:36 -0500
Subject: [Gluster-users] I/O error repaired only by owner or root access
In-Reply-To: <512B643F.6030706@reading.ac.uk> (Dan Bretherton's message of
	"Mon, 25 Feb 2013 13:16:47 +0000")
References: <10257952.36.1361534190609.JavaMail.raj@california>
	<512B643F.6030706@reading.ac.uk>
Message-ID: <suvppzo75rj.fsf@nock.cfmi.georgetown.edu>

Dan Bretherton <d.a.bretherton at reading.ac.uk> writes:

> Hello Rajesh- Here are the permissions.  The path in question is a
> directory.
>
> [sms05dab at jupiter ~]$ ls -ld /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
> drwxr-xr-x 60 vq901510 nemo 110592 Feb 23 04:37
> /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl [sms05dab at jupiter ~]$ ls -ld
> /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN lrwxrwxrwx 1 gcs nemo 49 Feb 1
> 2012 /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN ->
> /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN [sms05dab at jupiter
> ~]$ ls -ld /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
> drwxr-xr-x 27 gcs nemo 99210 Feb 23 03:14
> /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>
> As you can see the parent directory in this case was a symlink but
> that's not significant.  I ran the "ls -l" commands using my account -
> sms05dab, but the problem was originally reported by user vq901510.
> until I did "ls -l" as root neither of us could access the directory,
> because the parent directory was owned by user gcs.  Usually the
> problem is related to ownership of the file or directory itself.  This
> is the first time I have seen the I/O error caused by parent directory
> permissions.
>
> This problem seems to have started following an add-brick operation a
> few weeks ago, after which I started "gluster volume rebalance
> <VOLNAME> fix-layout" (which is still running). It occurred to me that
> the problem could be related to link files, many of which need to be
> rewritten following add-brick operations.  This could explain why the
> ownership of the parent directory is significant, because users
> sms05dab and vq901510 don't have permission to write in the parent
> directory owned by user gcs.  Normally this wouldn't be a problem
> because only read access to other users' data is required, but it
> appears as though read access was being denied because the new link
> file couldn't be written by unprivileged users.  Is this a plausible
> explanation of the I/O error do you think?

This sounds like my recent bug:
https://bugzilla.redhat.com/show_bug.cgi?id=913699 

In the bug, I said that writing on the fuse mount of one of the brick
servers fixed the problem.... but those were the only hosts I was
attempting access as root.

-- 
Shawn Nock (OpenPGP: 0x65118FA5)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 835 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130225/0e771873/attachment.sig>

From d.a.bretherton at reading.ac.uk  Mon Feb 25 16:44:50 2013
From: d.a.bretherton at reading.ac.uk (Dan Bretherton)
Date: Mon, 25 Feb 2013 16:44:50 +0000
Subject: [Gluster-users] I/O error repaired only by owner or root access
In-Reply-To: <suvppzo75rj.fsf@nock.cfmi.georgetown.edu>
References: <10257952.36.1361534190609.JavaMail.raj@california>
	<512B643F.6030706@reading.ac.uk>
	<suvppzo75rj.fsf@nock.cfmi.georgetown.edu>
Message-ID: <512B9502.8040709@reading.ac.uk>

On 02/25/2013 03:49 PM, Shawn Nock wrote:
> Dan Bretherton <d.a.bretherton at reading.ac.uk> writes:
>
>> Hello Rajesh- Here are the permissions.  The path in question is a
>> directory.
>>
>> [sms05dab at jupiter ~]$ ls -ld /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
>> drwxr-xr-x 60 vq901510 nemo 110592 Feb 23 04:37
>> /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl [sms05dab at jupiter ~]$ ls -ld
>> /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN lrwxrwxrwx 1 gcs nemo 49 Feb 1
>> 2012 /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN ->
>> /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN [sms05dab at jupiter
>> ~]$ ls -ld /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>> drwxr-xr-x 27 gcs nemo 99210 Feb 23 03:14
>> /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>>
>> As you can see the parent directory in this case was a symlink but
>> that's not significant.  I ran the "ls -l" commands using my account -
>> sms05dab, but the problem was originally reported by user vq901510.
>> until I did "ls -l" as root neither of us could access the directory,
>> because the parent directory was owned by user gcs.  Usually the
>> problem is related to ownership of the file or directory itself.  This
>> is the first time I have seen the I/O error caused by parent directory
>> permissions.
>>
>> This problem seems to have started following an add-brick operation a
>> few weeks ago, after which I started "gluster volume rebalance
>> <VOLNAME> fix-layout" (which is still running). It occurred to me that
>> the problem could be related to link files, many of which need to be
>> rewritten following add-brick operations.  This could explain why the
>> ownership of the parent directory is significant, because users
>> sms05dab and vq901510 don't have permission to write in the parent
>> directory owned by user gcs.  Normally this wouldn't be a problem
>> because only read access to other users' data is required, but it
>> appears as though read access was being denied because the new link
>> file couldn't be written by unprivileged users.  Is this a plausible
>> explanation of the I/O error do you think?
> This sounds like my recent bug:
> https://bugzilla.redhat.com/show_bug.cgi?id=913699
>
> In the bug, I said that writing on the fuse mount of one of the brick
> servers fixed the problem.... but those were the only hosts I was
> attempting access as root.
>
Thanks Shawn. To confirm that we are seeing the same bug I will try 
accessing affected files from a FUSE mount on a server the next time it 
happens.
-Dan.

--
Dan Bretherton
ESSC Computer System Manager
Department of Meteorology
Harry Pitt Building, 3 Earley Gate
University of Reading
Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
UK
Tel. +44 118 378 5205, Fax: +44 118 378 6413
-- 
## Please sponsor me to run in VSO's 30km Race to the Eye ##
##        http://www.justgiving.com/DanBretherton         ##


From brockp at umich.edu  Mon Feb 25 17:56:51 2013
From: brockp at umich.edu (Brock Palen)
Date: Mon, 25 Feb 2013 12:56:51 -0500
Subject: [Gluster-users] Gluster Podcast available
Message-ID: <C38EA6A5-2DFA-44CB-AC5A-2D836973D755@umich.edu>

Thanks again to Jeff for his time answering our questions about Gluster.

I hope people who are new to Gluster find this useful:
http://www.rce-cast.com/Podcast/rce-79-gluster-fs.html

If you have questions about the podcast please ping me off list.  Thanks!

Brock Palen
www.umich.edu/~brockp
CAEN Advanced Computing
brockp at umich.edu
(734)936-1985




From tony at filmsolutions.com  Mon Feb 25 18:28:01 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Mon, 25 Feb 2013 18:28:01 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
Message-ID: <34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>

Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.

On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com> wrote:

> Hey,
> 
> I was wondering if I could get a bit of help.. I installed a new Infiniband card into my servers but I'm unable to get it to come up as a peer. Is there something I'm missing?
> 
> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
> Probe on host fpsgluster2ib port 0 already in peer list
> 
> [root at fpsgluster testvault]# yum list installed | grep gluster
> glusterfs.x86_64                       3.3.1-1.el6              installed       
> glusterfs-devel.x86_64                 3.3.1-1.el6              installed       
> glusterfs-fuse.x86_64                  3.3.1-1.el6              installed       
> glusterfs-geo-replication.x86_64       3.3.1-1.el6              installed       
> glusterfs-rdma.x86_64                  3.3.1-1.el6              installed       
> glusterfs-server.x86_64                3.3.1-1.el6              installed   
> 
> Thanks.


From torbjorn at trollweb.no  Mon Feb 25 18:51:12 2013
From: torbjorn at trollweb.no (=?ISO-8859-1?Q?Torbj=F8rn_Thorsen?=)
Date: Mon, 25 Feb 2013 19:51:12 +0100
Subject: [Gluster-users] Peer Probe
In-Reply-To: <34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
Message-ID: <CAD2iGhUVt47_Bd753u-Km0PgxW1gAryciEOprcr7rAr=dST8dg@mail.gmail.com>

Your error message seems to indicate that the peer is already in the
storage pool ?
What is the output of "gluster peer status" ?

On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com> wrote:
> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
>
> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com> wrote:
>
>> Hey,
>>
>> I was wondering if I could get a bit of help.. I installed a new Infiniband card into my servers but I'm unable to get it to come up as a peer. Is there something I'm missing?
>>
>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>> Probe on host fpsgluster2ib port 0 already in peer list
>>
>> [root at fpsgluster testvault]# yum list installed | grep gluster
>> glusterfs.x86_64                       3.3.1-1.el6              installed
>> glusterfs-devel.x86_64                 3.3.1-1.el6              installed
>> glusterfs-fuse.x86_64                  3.3.1-1.el6              installed
>> glusterfs-geo-replication.x86_64       3.3.1-1.el6              installed
>> glusterfs-rdma.x86_64                  3.3.1-1.el6              installed
>> glusterfs-server.x86_64                3.3.1-1.el6              installed
>>
>> Thanks.
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users



--
Vennlig hilsen
Torbj?rn Thorsen
Utvikler / driftstekniker

Trollweb Solutions AS
- Professional Magento Partner
www.trollweb.no

Telefon dagtid: +47 51215300
Telefon kveld/helg: For kunder med Serviceavtale

Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
Postadresse: Maurholen 57, 4316 Sandnes

Husk at alle v?re standard-vilk?r alltid er gjeldende

From tony at filmsolutions.com  Mon Feb 25 19:46:00 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Mon, 25 Feb 2013 19:46:00 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <CAD2iGhUVt47_Bd753u-Km0PgxW1gAryciEOprcr7rAr=dST8dg@mail.gmail.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
	<CAD2iGhUVt47_Bd753u-Km0PgxW1gAryciEOprcr7rAr=dST8dg@mail.gmail.com>
Message-ID: <2E94BF43-4381-4EC4-AAF0-6C33D7431E81@filmsolutions.com>

It shows this but it's still going through my NIC cards and not the Infiniband. (Checked the traffic on the cards themselves)

[root at fpsgluster ~]# gluster peer status
Number of Peers: 1

Hostname: fpsgluster2
Uuid: 9b7e7c2d-f05b-4cc8-b55a-571e383328d0
State: Peer in Cluster (Connected)



On Feb 25, 2013, at 10:51 AM, Torbj?rn Thorsen <torbjorn at trollweb.no> wrote:

> Your error message seems to indicate that the peer is already in the
> storage pool ?
> What is the output of "gluster peer status" ?
> 
> On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com> wrote:
>> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
>> 
>> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com> wrote:
>> 
>>> Hey,
>>> 
>>> I was wondering if I could get a bit of help.. I installed a new Infiniband card into my servers but I'm unable to get it to come up as a peer. Is there something I'm missing?
>>> 
>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>> Probe on host fpsgluster2ib port 0 already in peer list
>>> 
>>> [root at fpsgluster testvault]# yum list installed | grep gluster
>>> glusterfs.x86_64                       3.3.1-1.el6              installed
>>> glusterfs-devel.x86_64                 3.3.1-1.el6              installed
>>> glusterfs-fuse.x86_64                  3.3.1-1.el6              installed
>>> glusterfs-geo-replication.x86_64       3.3.1-1.el6              installed
>>> glusterfs-rdma.x86_64                  3.3.1-1.el6              installed
>>> glusterfs-server.x86_64                3.3.1-1.el6              installed
>>> 
>>> Thanks.
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
> 
> 
> --
> Vennlig hilsen
> Torbj?rn Thorsen
> Utvikler / driftstekniker
> 
> Trollweb Solutions AS
> - Professional Magento Partner
> www.trollweb.no
> 
> Telefon dagtid: +47 51215300
> Telefon kveld/helg: For kunder med Serviceavtale
> 
> Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
> Postadresse: Maurholen 57, 4316 Sandnes
> 
> Husk at alle v?re standard-vilk?r alltid er gjeldende


From harry.mangalam at uci.edu  Mon Feb 25 19:57:15 2013
From: harry.mangalam at uci.edu (harry mangalam)
Date: Mon, 25 Feb 2013 11:57:15 -0800
Subject: [Gluster-users] Peer Probe
In-Reply-To: <2E94BF43-4381-4EC4-AAF0-6C33D7431E81@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<CAD2iGhUVt47_Bd753u-Km0PgxW1gAryciEOprcr7rAr=dST8dg@mail.gmail.com>
	<2E94BF43-4381-4EC4-AAF0-6C33D7431E81@filmsolutions.com>
Message-ID: <6458650.1QYlMVX8eE@stunted>

It /might be/probably is/ DNS-related. 

Are you trying to do this with RDMA or IPoIB?

If IPoIB, are ALL your /etc/hosts files in sync (IB names separate and 
distinct from the ethernet interfaces) and responsive to the appropriate 
interfaces?  

Do the IB interfaces show up as distinct (and connected) on an 'ifconfig -a' 
and 'ibstat' dump?  

Do all the peers show up on an 'ibhosts' query?

What is the output of:
gluster volume status <your_volume>
and 
gluster volume status <your_volume> detail


hjm

On Monday, February 25, 2013 07:46:00 PM Tony Saenz wrote:
> It shows this but it's still going through my NIC cards and not the
> Infiniband. (Checked the traffic on the cards themselves)
> 
> [root at fpsgluster ~]# gluster peer status
> Number of Peers: 1
> 
> Hostname: fpsgluster2
> Uuid: 9b7e7c2d-f05b-4cc8-b55a-571e383328d0
> State: Peer in Cluster (Connected)
> 
> On Feb 25, 2013, at 10:51 AM, Torbj?rn Thorsen <torbjorn at trollweb.no> wrote:
> > Your error message seems to indicate that the peer is already in the
> > storage pool ?
> > What is the output of "gluster peer status" ?
> > 
> > On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com> 
wrote:
> >> Any help please? The regular NICs are fine which is what it currently
> >> sees but I'd like to move them over to the Infiniband cards.>> 
> >> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com> 
wrote:
> >>> Hey,
> >>> 
> >>> I was wondering if I could get a bit of help.. I installed a new
> >>> Infiniband card into my servers but I'm unable to get it to come up as
> >>> a peer. Is there something I'm missing?
> >>> 
> >>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
> >>> Probe on host fpsgluster2ib port 0 already in peer list
> >>> 
> >>> [root at fpsgluster testvault]# yum list installed | grep gluster
> >>> glusterfs.x86_64                       3.3.1-1.el6             
> >>> installed
> >>> glusterfs-devel.x86_64                 3.3.1-1.el6             
> >>> installed
> >>> glusterfs-fuse.x86_64                  3.3.1-1.el6             
> >>> installed
> >>> glusterfs-geo-replication.x86_64       3.3.1-1.el6             
> >>> installed
> >>> glusterfs-rdma.x86_64                  3.3.1-1.el6             
> >>> installed
> >>> glusterfs-server.x86_64                3.3.1-1.el6             
> >>> installed
> >>> 
> >>> Thanks.
> >> 
> >> _______________________________________________
> >> Gluster-users mailing list
> >> Gluster-users at gluster.org
> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> > 
> > --
> > Vennlig hilsen
> > Torbj?rn Thorsen
> > Utvikler / driftstekniker
> > 
> > Trollweb Solutions AS
> > - Professional Magento Partner
> > www.trollweb.no
> > 
> > Telefon dagtid: +47 51215300
> > Telefon kveld/helg: For kunder med Serviceavtale
> > 
> > Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
> > Postadresse: Maurholen 57, 4316 Sandnes
> > 
> > Husk at alle v?re standard-vilk?r alltid er gjeldende
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

---
Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
[m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
415 South Circle View Dr, Irvine, CA, 92697 [shipping]
MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
---
"Something must be done. [X] is something. Therefore, we must do it."
Bruce Schneier, on American response to just about anything.

From tony at filmsolutions.com  Mon Feb 25 21:50:02 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Mon, 25 Feb 2013 21:50:02 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <6458650.1QYlMVX8eE@stunted>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<CAD2iGhUVt47_Bd753u-Km0PgxW1gAryciEOprcr7rAr=dST8dg@mail.gmail.com>
	<2E94BF43-4381-4EC4-AAF0-6C33D7431E81@filmsolutions.com>
	<6458650.1QYlMVX8eE@stunted>
Message-ID: <1F275178-B77F-4416-8032-F542E268AED7@filmsolutions.com>

Trying to first get this working with IPoIB

[root at fpsgluster ~]# ibhosts 
Ca	: 0x00117500007937b2 ports 1 "fpsgluster2 qib0"
Ca	: 0x0011750000792af2 ports 1 "fpsgluster qib0"

I'm able to ping the other box from Infiniband to Infiniband card

Ifconfig uses the ioctl access method to get the full address information, which limits hardware addresses to 8 bytes.
Because Infiniband address has 20 bytes, only the first 8 bytes are displayed correctly.
Ifconfig is obsolete! For replacement check ip.
ib0       Link encap:InfiniBand  HWaddr 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00  
          inet addr:10.0.4.35  Bcast:10.0.4.255  Mask:255.255.255.0
          inet6 addr: fe80::211:7500:79:2af2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1
          RX packets:1567 errors:0 dropped:0 overruns:0 frame:0
          TX packets:587 errors:0 dropped:24 overruns:0 carrier:0
          collisions:0 txqueuelen:256 
          RX bytes:342622 (334.5 KiB)  TX bytes:96554 (94.2 KiB)

[root at fpsgluster2 ~]# ifconfig ib0
Ifconfig uses the ioctl access method to get the full address information, which limits hardware addresses to 8 bytes.
Because Infiniband address has 20 bytes, only the first 8 bytes are displayed correctly.
Ifconfig is obsolete! For replacement check ip.
ib0       Link encap:InfiniBand  HWaddr 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00  
          inet addr:10.0.4.34  Bcast:10.0.4.255  Mask:255.255.255.0
          inet6 addr: fe80::211:7500:79:37b2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1
          RX packets:599 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1558 errors:0 dropped:8 overruns:0 carrier:0
          collisions:0 txqueuelen:256 
          RX bytes:95180 (92.9 KiB)  TX bytes:346728 (338.6 KiB)

[root at fpsgluster ~]# ping -I ib0 10.0.4.34
PING 10.0.4.34 (10.0.4.34) from 10.0.4.35 ib0: 56(84) bytes of data.
64 bytes from 10.0.4.34: icmp_seq=1 ttl=64 time=12.6 ms
64 bytes from 10.0.4.34: icmp_seq=2 ttl=64 time=0.184 ms

/etc/hosts looks correct

[root at fpsgluster2 ~]# cat /etc/hosts | grep ib
10.0.4.35      fpsglusterib
10.0.4.34      fpsgluster2ib

[root at fpsgluster ~]# cat /etc/hosts| grep ib
10.0.4.35      fpsglusterib
10.0.4.34      fpsgluster2ib

I haven't created the new volume yet as I can't get the peer probe to work off the Infiniband card. It's only seeing the NIC cards I currently have it hooked in to.


On Feb 25, 2013, at 11:57 AM, harry mangalam <harry.mangalam at uci.edu>
 wrote:

> It /might be/probably is/ DNS-related. 
> 
> Are you trying to do this with RDMA or IPoIB?
> 
> If IPoIB, are ALL your /etc/hosts files in sync (IB names separate and 
> distinct from the ethernet interfaces) and responsive to the appropriate 
> interfaces?  
> 
> Do the IB interfaces show up as distinct (and connected) on an 'ifconfig -a' 
> and 'ibstat' dump?  
> 
> Do all the peers show up on an 'ibhosts' query?
> 
> What is the output of:
> gluster volume status <your_volume>
> and 
> gluster volume status <your_volume> detail
> 
> 
> hjm
> 
> On Monday, February 25, 2013 07:46:00 PM Tony Saenz wrote:
>> It shows this but it's still going through my NIC cards and not the
>> Infiniband. (Checked the traffic on the cards themselves)
>> 
>> [root at fpsgluster ~]# gluster peer status
>> Number of Peers: 1
>> 
>> Hostname: fpsgluster2
>> Uuid: 9b7e7c2d-f05b-4cc8-b55a-571e383328d0
>> State: Peer in Cluster (Connected)
>> 
>> On Feb 25, 2013, at 10:51 AM, Torbj?rn Thorsen <torbjorn at trollweb.no> wrote:
>>> Your error message seems to indicate that the peer is already in the
>>> storage pool ?
>>> What is the output of "gluster peer status" ?
>>> 
>>> On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com> 
> wrote:
>>>> Any help please? The regular NICs are fine which is what it currently
>>>> sees but I'd like to move them over to the Infiniband cards.>> 
>>>> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com> 
> wrote:
>>>>> Hey,
>>>>> 
>>>>> I was wondering if I could get a bit of help.. I installed a new
>>>>> Infiniband card into my servers but I'm unable to get it to come up as
>>>>> a peer. Is there something I'm missing?
>>>>> 
>>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>>>> Probe on host fpsgluster2ib port 0 already in peer list
>>>>> 
>>>>> [root at fpsgluster testvault]# yum list installed | grep gluster
>>>>> glusterfs.x86_64                       3.3.1-1.el6             
>>>>> installed
>>>>> glusterfs-devel.x86_64                 3.3.1-1.el6             
>>>>> installed
>>>>> glusterfs-fuse.x86_64                  3.3.1-1.el6             
>>>>> installed
>>>>> glusterfs-geo-replication.x86_64       3.3.1-1.el6             
>>>>> installed
>>>>> glusterfs-rdma.x86_64                  3.3.1-1.el6             
>>>>> installed
>>>>> glusterfs-server.x86_64                3.3.1-1.el6             
>>>>> installed
>>>>> 
>>>>> Thanks.
>>>> 
>>>> _______________________________________________
>>>> Gluster-users mailing list
>>>> Gluster-users at gluster.org
>>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>> 
>>> --
>>> Vennlig hilsen
>>> Torbj?rn Thorsen
>>> Utvikler / driftstekniker
>>> 
>>> Trollweb Solutions AS
>>> - Professional Magento Partner
>>> www.trollweb.no
>>> 
>>> Telefon dagtid: +47 51215300
>>> Telefon kveld/helg: For kunder med Serviceavtale
>>> 
>>> Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
>>> Postadresse: Maurholen 57, 4316 Sandnes
>>> 
>>> Husk at alle v?re standard-vilk?r alltid er gjeldende
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
> ---
> Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
> [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
> 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
> MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
> ---
> "Something must be done. [X] is something. Therefore, we must do it."
> Bruce Schneier, on American response to just about anything.


From harry.mangalam at uci.edu  Mon Feb 25 22:40:24 2013
From: harry.mangalam at uci.edu (harry mangalam)
Date: Mon, 25 Feb 2013 14:40:24 -0800
Subject: [Gluster-users] Peer Probe
In-Reply-To: <1F275178-B77F-4416-8032-F542E268AED7@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<6458650.1QYlMVX8eE@stunted>
	<1F275178-B77F-4416-8032-F542E268AED7@filmsolutions.com>
Message-ID: <2432025.sDFVujSrFS@stunted>

That looks OK (but your 2 MTUs are mismatched - should fix that).

>           UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1  <- 1st
>           UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1 <- 2nd
                                                ^^^^^
IBDEV=ibX
modprobe ib_umad 
modprobe ib_ipoib
echo connected > /sys/class/net/${IBDEV}/mode
echo 65520 > /sys/class/net/${IBDEV}/mtu

how did you set up the peering? By name?  by IP#?
(I assume pinging by hostname also works both ways?)

If you can't get the peers to ack, then what do the logs say on failure to:

peer probe <host>
  or create the volume 
gluster volume create <volname>    host1ib:/gl_part   host2ib:/gl_part

hjm

On Monday, February 25, 2013 09:50:02 PM Tony Saenz wrote:
> Trying to first get this working with IPoIB
> 
> [root at fpsgluster ~]# ibhosts
> Ca	: 0x00117500007937b2 ports 1 "fpsgluster2 qib0"
> Ca	: 0x0011750000792af2 ports 1 "fpsgluster qib0"
> 
> I'm able to ping the other box from Infiniband to Infiniband card
> 
> Ifconfig uses the ioctl access method to get the full address information,
> which limits hardware addresses to 8 bytes. Because Infiniband address has
> 20 bytes, only the first 8 bytes are displayed correctly. Ifconfig is
> obsolete! For replacement check ip.
> ib0       Link encap:InfiniBand  HWaddr
> 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00 inet
> addr:10.0.4.35  Bcast:10.0.4.255  Mask:255.255.255.0 inet6 addr:
> fe80::211:7500:79:2af2/64 Scope:Link
>           UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1
>           RX packets:1567 errors:0 dropped:0 overruns:0 frame:0
>           TX packets:587 errors:0 dropped:24 overruns:0 carrier:0
>           collisions:0 txqueuelen:256
>           RX bytes:342622 (334.5 KiB)  TX bytes:96554 (94.2 KiB)
> 
> [root at fpsgluster2 ~]# ifconfig ib0
> Ifconfig uses the ioctl access method to get the full address information,
> which limits hardware addresses to 8 bytes. Because Infiniband address has
> 20 bytes, only the first 8 bytes are displayed correctly. Ifconfig is
> obsolete! For replacement check ip.
> ib0       Link encap:InfiniBand  HWaddr
> 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00 inet
> addr:10.0.4.34  Bcast:10.0.4.255  Mask:255.255.255.0 inet6 addr:
> fe80::211:7500:79:37b2/64 Scope:Link
>           UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1
>           RX packets:599 errors:0 dropped:0 overruns:0 frame:0
>           TX packets:1558 errors:0 dropped:8 overruns:0 carrier:0
>           collisions:0 txqueuelen:256
>           RX bytes:95180 (92.9 KiB)  TX bytes:346728 (338.6 KiB)
> 
> [root at fpsgluster ~]# ping -I ib0 10.0.4.34
> PING 10.0.4.34 (10.0.4.34) from 10.0.4.35 ib0: 56(84) bytes of data.
> 64 bytes from 10.0.4.34: icmp_seq=1 ttl=64 time=12.6 ms
> 64 bytes from 10.0.4.34: icmp_seq=2 ttl=64 time=0.184 ms
> 
> /etc/hosts looks correct
> 
> [root at fpsgluster2 ~]# cat /etc/hosts | grep ib
> 10.0.4.35      fpsglusterib
> 10.0.4.34      fpsgluster2ib
> 
> [root at fpsgluster ~]# cat /etc/hosts| grep ib
> 10.0.4.35      fpsglusterib
> 10.0.4.34      fpsgluster2ib
> 
> I haven't created the new volume yet as I can't get the peer probe to work
> off the Infiniband card. It's only seeing the NIC cards I currently have it
> hooked in to.
> 
> 
> On Feb 25, 2013, at 11:57 AM, harry mangalam <harry.mangalam at uci.edu>
> 
>  wrote:
> > It /might be/probably is/ DNS-related.
> > 
> > Are you trying to do this with RDMA or IPoIB?
> > 
> > If IPoIB, are ALL your /etc/hosts files in sync (IB names separate and
> > distinct from the ethernet interfaces) and responsive to the appropriate
> > interfaces?
> > 
> > Do the IB interfaces show up as distinct (and connected) on an 'ifconfig
> > -a' and 'ibstat' dump?
> > 
> > Do all the peers show up on an 'ibhosts' query?
> > 
> > What is the output of:
> > gluster volume status <your_volume>
> > and
> > gluster volume status <your_volume> detail
> > 
> > 
> > hjm
> > 
> > On Monday, February 25, 2013 07:46:00 PM Tony Saenz wrote:
> >> It shows this but it's still going through my NIC cards and not the
> >> Infiniband. (Checked the traffic on the cards themselves)
> >> 
> >> [root at fpsgluster ~]# gluster peer status
> >> Number of Peers: 1
> >> 
> >> Hostname: fpsgluster2
> >> Uuid: 9b7e7c2d-f05b-4cc8-b55a-571e383328d0
> >> State: Peer in Cluster (Connected)
> >> 
> >> On Feb 25, 2013, at 10:51 AM, Torbj?rn Thorsen <torbjorn at trollweb.no> 
wrote:
> >>> Your error message seems to indicate that the peer is already in the
> >>> storage pool ?
> >>> What is the output of "gluster peer status" ?
> >>> 
> >>> On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com>
> > 
> > wrote:
> >>>> Any help please? The regular NICs are fine which is what it currently
> >>>> sees but I'd like to move them over to the Infiniband cards.>>
> >>>> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com>
> > 
> > wrote:
> >>>>> Hey,
> >>>>> 
> >>>>> I was wondering if I could get a bit of help.. I installed a new
> >>>>> Infiniband card into my servers but I'm unable to get it to come up as
> >>>>> a peer. Is there something I'm missing?
> >>>>> 
> >>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
> >>>>> Probe on host fpsgluster2ib port 0 already in peer list
> >>>>> 
> >>>>> [root at fpsgluster testvault]# yum list installed | grep gluster
> >>>>> glusterfs.x86_64                       3.3.1-1.el6
> >>>>> installed
> >>>>> glusterfs-devel.x86_64                 3.3.1-1.el6
> >>>>> installed
> >>>>> glusterfs-fuse.x86_64                  3.3.1-1.el6
> >>>>> installed
> >>>>> glusterfs-geo-replication.x86_64       3.3.1-1.el6
> >>>>> installed
> >>>>> glusterfs-rdma.x86_64                  3.3.1-1.el6
> >>>>> installed
> >>>>> glusterfs-server.x86_64                3.3.1-1.el6
> >>>>> installed
> >>>>> 
> >>>>> Thanks.
> >>>> 
> >>>> _______________________________________________
> >>>> Gluster-users mailing list
> >>>> Gluster-users at gluster.org
> >>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> >>> 
> >>> --
> >>> Vennlig hilsen
> >>> Torbj?rn Thorsen
> >>> Utvikler / driftstekniker
> >>> 
> >>> Trollweb Solutions AS
> >>> - Professional Magento Partner
> >>> www.trollweb.no
> >>> 
> >>> Telefon dagtid: +47 51215300
> >>> Telefon kveld/helg: For kunder med Serviceavtale
> >>> 
> >>> Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
> >>> Postadresse: Maurholen 57, 4316 Sandnes
> >>> 
> >>> Husk at alle v?re standard-vilk?r alltid er gjeldende
> >> 
> >> _______________________________________________
> >> Gluster-users mailing list
> >> Gluster-users at gluster.org
> >> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> > 
> > ---
> > Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
> > [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
> > 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
> > MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
> > ---
> > "Something must be done. [X] is something. Therefore, we must do it."
> > Bruce Schneier, on American response to just about anything.
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

---
Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
[m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
415 South Circle View Dr, Irvine, CA, 92697 [shipping]
MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
---
"Something must be done. [X] is something. Therefore, we must do it."
Bruce Schneier, on American response to just about anything.

From tony at filmsolutions.com  Mon Feb 25 23:07:53 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Mon, 25 Feb 2013 23:07:53 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <2432025.sDFVujSrFS@stunted>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<6458650.1QYlMVX8eE@stunted>
	<1F275178-B77F-4416-8032-F542E268AED7@filmsolutions.com>
	<2432025.sDFVujSrFS@stunted>
Message-ID: <5E972C36-6632-440C-9AF3-CE9B793D5020@filmsolutions.com>

[root at fpsgluster ~]# gluster volume create testvault replica 2 transport rdma,tcp fpsglusterib:/mnt/testbrick1 fpsgluster2ib:/mnt/testbrick1 fpsglusterib:/mnt/testbrick2 fpsgluster2ib:/mnt/testbrick2 fpsglusterib:/mnt/testbrick3 fpsgluster2ib:/mnt/testbrick3
Host fpsgluster2ib not a friend

Not a friend error. I'm at a loss.

On Feb 25, 2013, at 2:40 PM, harry mangalam <harry.mangalam at uci.edu>
 wrote:

> That looks OK (but your 2 MTUs are mismatched - should fix that).
> 
>>          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1  <- 1st
>>          UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1 <- 2nd
>                                                ^^^^^
> IBDEV=ibX
> modprobe ib_umad 
> modprobe ib_ipoib
> echo connected > /sys/class/net/${IBDEV}/mode
> echo 65520 > /sys/class/net/${IBDEV}/mtu
> 
> how did you set up the peering? By name?  by IP#?
> (I assume pinging by hostname also works both ways?)
> 
> If you can't get the peers to ack, then what do the logs say on failure to:
> 
> peer probe <host>
>  or create the volume 
> gluster volume create <volname>    host1ib:/gl_part   host2ib:/gl_part
> 
> hjm
> 
> On Monday, February 25, 2013 09:50:02 PM Tony Saenz wrote:
>> Trying to first get this working with IPoIB
>> 
>> [root at fpsgluster ~]# ibhosts
>> Ca	: 0x00117500007937b2 ports 1 "fpsgluster2 qib0"
>> Ca	: 0x0011750000792af2 ports 1 "fpsgluster qib0"
>> 
>> I'm able to ping the other box from Infiniband to Infiniband card
>> 
>> Ifconfig uses the ioctl access method to get the full address information,
>> which limits hardware addresses to 8 bytes. Because Infiniband address has
>> 20 bytes, only the first 8 bytes are displayed correctly. Ifconfig is
>> obsolete! For replacement check ip.
>> ib0       Link encap:InfiniBand  HWaddr
>> 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00 inet
>> addr:10.0.4.35  Bcast:10.0.4.255  Mask:255.255.255.0 inet6 addr:
>> fe80::211:7500:79:2af2/64 Scope:Link
>>          UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1
>>          RX packets:1567 errors:0 dropped:0 overruns:0 frame:0
>>          TX packets:587 errors:0 dropped:24 overruns:0 carrier:0
>>          collisions:0 txqueuelen:256
>>          RX bytes:342622 (334.5 KiB)  TX bytes:96554 (94.2 KiB)
>> 
>> [root at fpsgluster2 ~]# ifconfig ib0
>> Ifconfig uses the ioctl access method to get the full address information,
>> which limits hardware addresses to 8 bytes. Because Infiniband address has
>> 20 bytes, only the first 8 bytes are displayed correctly. Ifconfig is
>> obsolete! For replacement check ip.
>> ib0       Link encap:InfiniBand  HWaddr
>> 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00 inet
>> addr:10.0.4.34  Bcast:10.0.4.255  Mask:255.255.255.0 inet6 addr:
>> fe80::211:7500:79:37b2/64 Scope:Link
>>          UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1
>>          RX packets:599 errors:0 dropped:0 overruns:0 frame:0
>>          TX packets:1558 errors:0 dropped:8 overruns:0 carrier:0
>>          collisions:0 txqueuelen:256
>>          RX bytes:95180 (92.9 KiB)  TX bytes:346728 (338.6 KiB)
>> 
>> [root at fpsgluster ~]# ping -I ib0 10.0.4.34
>> PING 10.0.4.34 (10.0.4.34) from 10.0.4.35 ib0: 56(84) bytes of data.
>> 64 bytes from 10.0.4.34: icmp_seq=1 ttl=64 time=12.6 ms
>> 64 bytes from 10.0.4.34: icmp_seq=2 ttl=64 time=0.184 ms
>> 
>> /etc/hosts looks correct
>> 
>> [root at fpsgluster2 ~]# cat /etc/hosts | grep ib
>> 10.0.4.35      fpsglusterib
>> 10.0.4.34      fpsgluster2ib
>> 
>> [root at fpsgluster ~]# cat /etc/hosts| grep ib
>> 10.0.4.35      fpsglusterib
>> 10.0.4.34      fpsgluster2ib
>> 
>> I haven't created the new volume yet as I can't get the peer probe to work
>> off the Infiniband card. It's only seeing the NIC cards I currently have it
>> hooked in to.
>> 
>> 
>> On Feb 25, 2013, at 11:57 AM, harry mangalam <harry.mangalam at uci.edu>
>> 
>> wrote:
>>> It /might be/probably is/ DNS-related.
>>> 
>>> Are you trying to do this with RDMA or IPoIB?
>>> 
>>> If IPoIB, are ALL your /etc/hosts files in sync (IB names separate and
>>> distinct from the ethernet interfaces) and responsive to the appropriate
>>> interfaces?
>>> 
>>> Do the IB interfaces show up as distinct (and connected) on an 'ifconfig
>>> -a' and 'ibstat' dump?
>>> 
>>> Do all the peers show up on an 'ibhosts' query?
>>> 
>>> What is the output of:
>>> gluster volume status <your_volume>
>>> and
>>> gluster volume status <your_volume> detail
>>> 
>>> 
>>> hjm
>>> 
>>> On Monday, February 25, 2013 07:46:00 PM Tony Saenz wrote:
>>>> It shows this but it's still going through my NIC cards and not the
>>>> Infiniband. (Checked the traffic on the cards themselves)
>>>> 
>>>> [root at fpsgluster ~]# gluster peer status
>>>> Number of Peers: 1
>>>> 
>>>> Hostname: fpsgluster2
>>>> Uuid: 9b7e7c2d-f05b-4cc8-b55a-571e383328d0
>>>> State: Peer in Cluster (Connected)
>>>> 
>>>> On Feb 25, 2013, at 10:51 AM, Torbj?rn Thorsen <torbjorn at trollweb.no> 
> wrote:
>>>>> Your error message seems to indicate that the peer is already in the
>>>>> storage pool ?
>>>>> What is the output of "gluster peer status" ?
>>>>> 
>>>>> On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com>
>>> 
>>> wrote:
>>>>>> Any help please? The regular NICs are fine which is what it currently
>>>>>> sees but I'd like to move them over to the Infiniband cards.>>
>>>>>> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com>
>>> 
>>> wrote:
>>>>>>> Hey,
>>>>>>> 
>>>>>>> I was wondering if I could get a bit of help.. I installed a new
>>>>>>> Infiniband card into my servers but I'm unable to get it to come up as
>>>>>>> a peer. Is there something I'm missing?
>>>>>>> 
>>>>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>>>>>> Probe on host fpsgluster2ib port 0 already in peer list
>>>>>>> 
>>>>>>> [root at fpsgluster testvault]# yum list installed | grep gluster
>>>>>>> glusterfs.x86_64                       3.3.1-1.el6
>>>>>>> installed
>>>>>>> glusterfs-devel.x86_64                 3.3.1-1.el6
>>>>>>> installed
>>>>>>> glusterfs-fuse.x86_64                  3.3.1-1.el6
>>>>>>> installed
>>>>>>> glusterfs-geo-replication.x86_64       3.3.1-1.el6
>>>>>>> installed
>>>>>>> glusterfs-rdma.x86_64                  3.3.1-1.el6
>>>>>>> installed
>>>>>>> glusterfs-server.x86_64                3.3.1-1.el6
>>>>>>> installed
>>>>>>> 
>>>>>>> Thanks.
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Gluster-users mailing list
>>>>>> Gluster-users at gluster.org
>>>>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>>>> 
>>>>> --
>>>>> Vennlig hilsen
>>>>> Torbj?rn Thorsen
>>>>> Utvikler / driftstekniker
>>>>> 
>>>>> Trollweb Solutions AS
>>>>> - Professional Magento Partner
>>>>> www.trollweb.no
>>>>> 
>>>>> Telefon dagtid: +47 51215300
>>>>> Telefon kveld/helg: For kunder med Serviceavtale
>>>>> 
>>>>> Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
>>>>> Postadresse: Maurholen 57, 4316 Sandnes
>>>>> 
>>>>> Husk at alle v?re standard-vilk?r alltid er gjeldende
>>>> 
>>>> _______________________________________________
>>>> Gluster-users mailing list
>>>> Gluster-users at gluster.org
>>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>> 
>>> ---
>>> Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
>>> [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
>>> 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
>>> MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
>>> ---
>>> "Something must be done. [X] is something. Therefore, we must do it."
>>> Bruce Schneier, on American response to just about anything.
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 
> ---
> Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
> [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
> 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
> MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
> ---
> "Something must be done. [X] is something. Therefore, we must do it."
> Bruce Schneier, on American response to just about anything.


From toby.corkindale at strategicdata.com.au  Mon Feb 25 23:25:26 2013
From: toby.corkindale at strategicdata.com.au (Toby Corkindale)
Date: Tue, 26 Feb 2013 10:25:26 +1100
Subject: [Gluster-users] Bug in log rotation for 3.3.1
In-Reply-To: <512B3401.3010503@lanning.cc>
References: <512572DF.3030200@strategicdata.com.au>
	<5126B93F.4070604@julianfamily.org>
	<512707A6.3070906@strategicdata.com.au>
	<2f4962de-2789-41a2-877c-df9d61b179f3@email.android.com>
	<512AEB8F.304@strategicdata.com.au> <512B3401.3010503@lanning.cc>
Message-ID: <512BF2E6.6090906@strategicdata.com.au>

On 25/02/13 20:50, Robert Hajime Lanning wrote:
> On 02/24/13 20:41, Toby Corkindale wrote:
>> In the meantime, could someone advise me on the correct way to tell
>> Glusterfs to rotate the logs for the bricks and mounts?
>
> Have you tried:
> # gluster volume log rotate <VOLNAME> [BRICK]

Is there a way to pass a wildcard value to the volname parameter?

I don't think this will have any effect on the clients, will it?


thanks,
Toby

From mike at cchtml.com  Tue Feb 26 07:50:35 2013
From: mike at cchtml.com (Michael Cronenworth)
Date: Tue, 26 Feb 2013 01:50:35 -0600
Subject: [Gluster-users] horrible write performance after upgrade from 3.2
	to 3.3
Message-ID: <512C694B.5060606@cchtml.com>

I have a brick on a MD RAID5 array formatted with ext4 on a gigabit network.

My brick is located at /srv/media.
dd if=/dev/zero of=/srv/media/test.zero reports 150MB/sec
dd reading reports 300MB/sec

I have used iperf to verify it is not a network adapter issue. I get 
1gbit/sec each way.

On GlusterFS 3.2 my read and write performance was as expected. 
100MB/sec each way.

On GlusterFS 3.3, my read speed is still 100MB/sec but my write speed 
never exceeds 10MB/sec. It seems something is purposely throttling my 
writes like I'm on a 100mbit network.

CPU usage on the server and client are around 25% during the transfer. 
No other processes are eating I/O.

Any ideas on why write performance is suffering?

gluster> volume info

Volume Name: media
Type: Distribute
Volume ID: 990a5d58-f76c-405c-a7bf-096e70b9fed3
Status: Started
Number of Bricks: 1
Transport-type: tcp
Bricks:
Brick1: 10.0.0.1:/srv/media
Options Reconfigured:
auth.allow: 10.0.0.*
nfs.disable: On
performance.cache-size: 128MB
performance.write-behind-window-size: 128MB

Thanks,
Michael

From mike at cchtml.com  Tue Feb 26 08:00:21 2013
From: mike at cchtml.com (Michael Cronenworth)
Date: Tue, 26 Feb 2013 02:00:21 -0600
Subject: [Gluster-users] horrible write performance after upgrade from
 3.2 to 3.3
In-Reply-To: <512C694B.5060606@cchtml.com>
References: <512C694B.5060606@cchtml.com>
Message-ID: <512C6B95.30405@cchtml.com>

On 02/26/2013 01:50 AM, Michael Cronenworth wrote:
> Any ideas on why write performance is suffering?

This is really strange. After my e-mail I tested it again and I see 
normal speeds now. I have not touched my brick settings.

If I can reproduce it I'll report back.

From B.Candler at pobox.com  Tue Feb 26 09:36:03 2013
From: B.Candler at pobox.com (Brian Candler)
Date: Tue, 26 Feb 2013 09:36:03 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
Message-ID: <20130226093603.GA61170@nsrc.org>

On Mon, Feb 25, 2013 at 06:28:01PM +0000, Tony Saenz wrote:
> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
...
> > [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
> > Probe on host fpsgluster2ib port 0 already in peer list

Probing only works in one direction. The HTML admin guide has been taken
down so I can only point you to the PDF:
http://www.gluster.org/wp-content/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-US.pdf

"use the probe command from a storage server that is already part of the
trusted storage pool."

That is, probe from existing cluster node to new node, not from new node to
cluster.

From flips01 at googlemail.com  Tue Feb 26 11:15:51 2013
From: flips01 at googlemail.com (Philip)
Date: Tue, 26 Feb 2013 12:15:51 +0100
Subject: [Gluster-users] Replicated Volume Crashed
Message-ID: <CAKDbnM5uCUT_7Y7zjWuBjPxa=Ta_=WHz9LAB1qqYbZfX_nBT5A@mail.gmail.com>

Hi,

I have a gluster volume that consists of 22Bricks and includes a single
folder with 3.6 Million files. Yesterday the volume crashed and turned out
to be completely unresposible and I was forced to perform a hard reboot on
all gluster servers because they were not able to execute a reboot command
issued by the shell because they were that heavy overloaded. Each gluster
server has 12 CPU cores and each of them was maxxed out.

The servers log looks like this: http://pastebin.com/rv0WQ14E
The client log looks like this: http://filebin.ca/YLiU6yxURKd/client-log.txt

The most interesting parts from the server log are:

2013-02-25 19:48:01.672143] W [socket.c:195:__socket_rwv]
0-tcp.adata-server: writev failed (Connection reset by peer)
[2013-02-25 19:48:01.742239] I [server-helpers.c:474:do_fd_cleanup]
0-adata-server: fd cleanup on /files/random-file.dat
[2013-02-25 19:48:01.749871] E [server.c:176:server_submit_reply]
(-->/usr/lib/glusterfs/3.3.1/xlator/features/marker.so(marker_lookup_cbk+0x103)
[0x7ffdfce19e53]
(-->/usr/lib/glusterfs/3.3.1/xlator/debug/io-stats.so(io_stats_lookup_cbk+0xff)
[0x7ffdfcc00f9f]
(-->/usr/lib/glusterfs/3.3.1/xlator/protocol/server.so(server_lookup_cbk+0x39d)
[0x7ffdfc9e480d]))) 0-: Reply submission failed
[2013-02-25 19:48:01.805132] I [server-helpers.c:330:do_lock_table_cleanup]
0-adata-server: finodelk released on /files/random-file.dat
[2013-02-25 19:59:46.438680] W [socket.c:195:__socket_rwv]
0-tcp.adata-server: readv failed (Connection timed out)
[2013-02-25 20:03:01.825280] I [server-helpers.c:394:do_lock_table_cleanup]
0-adata-server: entrylk released on /files
[2013-02-25 20:03:01.825309] I [server-helpers.c:474:do_fd_cleanup]
0-adata-server: fd cleanup on /files

The most interesting parts from the client log are:

[2013-02-25 19:44:43.568372] W [client3_1-fops.c:5306:client3_1_finodelk]
0-adata-client-7:  (b468c831-5c04-4bbd-8b03-b94d7f937e55) remote_fd is -1.
EBADFD
[2013-02-25 19:44:43.568405] W [client3_1-fops.c:4098:client3_1_flush]
0-adata-client-7:  (b468c831-5c04-4bbd-8b03-b94d7f937e55) remote_fd is -1.
EBADFD
[2013-02-25 19:44:55.423638] I
[afr-self-heal-common.c:1189:afr_sh_missing_entry_call_impunge_recreate]
0-adata-replicate-8: no missing files - /files/random-file.dat. proceeding
to metadata check
[2013-02-25 19:44:59.816525] I
[afr-self-heal-common.c:1941:afr_sh_post_nb_entrylk_conflicting_sh_cbk]
0-adata-replicate-1: Non blocking entrylks failed.
[2013-02-25 19:44:59.816554] E
[afr-self-heal-common.c:2160:afr_self_heal_completion_cbk]
0-adata-replicate-1: background  data missing-entry gfid self-heal failed
on /files/random-file.dat
[2013-02-25 19:47:01.548989] I
[afr-self-heal-common.c:1941:afr_sh_post_nb_entrylk_conflicting_sh_cbk]
0-adata-replicate-8: Non blocking entrylks failed.
[2013-02-25 19:47:01.548996] E
[afr-self-heal-common.c:2160:afr_self_heal_completion_cbk]
0-adata-replicate-8: background  data missing-entry gfid self-heal failed
on /files/random-file.dat
[2013-02-25 19:56:20.623378] I [dht-layout.c:593:dht_layout_normalize]
0-adata-dht: found anomalies in /files. holes=1 overlaps=0
[2013-02-25 19:56:20.623399] W [dht-layout.c:186:dht_layout_search]
0-adata-dht: no subvolume for hash (value) = 2967408968
[2013-02-25 19:56:20.623404] W [dht-selfheal.c:882:dht_selfheal_directory]
0-adata-dht: 1 subvolumes have unrecoverable errors
[2013-02-25 19:56:20.623413] E [dht-common.c:1372:dht_lookup] 0-adata-dht:
Failed to get hashed subvol for /files/random-file.dat
[2013-02-25 19:57:02.780965] W [fuse-bridge.c:292:fuse_entry_cbk]
0-glusterfs-fuse: 3960898: LOOKUP() /files/random-file.dat => -1 (Invalid
argument)
[2013-02-25 19:57:02.781385] W [dht-layout.c:186:dht_layout_search]
0-adata-dht: no subvolume for hash (value) = 2967408968
[2013-02-25 20:00:32.715869] I [dht-layout.c:593:dht_layout_normalize]
0-adata-dht: found anomalies in /files. holes=2 overlaps=0
[2013-02-25 20:00:32.715886] W [dht-selfheal.c:882:dht_selfheal_directory]
0-adata-dht: 2 subvolumes have unrecoverable errors
[2013-02-25 20:00:47.566817] I [dht-common.c:543:dht_revalidate_cbk]
0-adata-dht: subvolume adata-replicate-9 for /files returned -1
(Input/output error)

What could be the reason for this? I've seen that the folder which contains
the 3M files was locked and I am restructuring the directory layout so
there is a directory tree with approximately 100-500 files within every
directory. Would this prevent a lock of all files?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130226/a7508752/attachment.html>

From differentlocal at gmail.com  Wed Feb 27 09:37:53 2013
From: differentlocal at gmail.com (Nikita A Kardashin)
Date: Wed, 27 Feb 2013 15:37:53 +0600
Subject: [Gluster-users] GlusterFS performance
Message-ID: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>

Hello!

I have GlusterFS installation with parameters:

- 4 servers, connected by 1Gbit/s network (760-800 Mbit/s by iperf)
- Distributed-replicated volume with 4 bricks and 2x4 redundancy formula.
- Replicated volume with 2 bricks and 2x2 formula.

I found some trouble: if I try to copy huge amount of files (94000 files,
3Gb size), this process takes terribly long time (from 20 to 40 minutes). I
perform some tests and results is:

Directly to storage (single 2TB HDD): 158MB/s
Directly to storage (RAID1 of 2 HDDs): 190MB/s
To Replicated gluster volume: 89MB/s
To Distributed-replicated gluster volume: 49MB/s

Test command is: sync && echo 3 > /proc/sys/vm/drop_caches && dd
if=/dev/zero of=gluster.test.bin bs=1G count=1

Switching direct-io on and off doesn't have effect.
Playing with glusterfs options too.

What I can do with performance?

My volumes:

Volume Name: nginx
Type: Replicate
Volume ID: e3306431-e01d-41f8-8b2d-86a61837b0b2
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: control1:/storage/nginx
Brick2: control2:/storage/nginx

Volume Name: instances
Type: Distributed-Replicate
Volume ID: d32363fc-4b53-433c-87b7-ad51acfa4125
Status: Started
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: control1:/storage/instances
Brick2: control2:/storage/instances
Brick3: compute1:/storage/instances
Brick4: compute2:/storage/instances
Options Reconfigured:
cluster.self-heal-window-size: 1
cluster.data-self-heal-algorithm: diff
performance.stat-prefetch: 1
features.quota-timeout: 3600
performance.write-behind-window-size: 512MB
performance.cache-size: 1GB
performance.io-thread-count: 64
performance.flush-behind: on
performance.cache-min-file-size: 0
performance.write-behind: on

Mounted with default options by Gluster-FUSE.


-- 
With best regards,
differentlocal (www.differentlocal.ru | differentlocal at gmail.com),
System administrator.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/7ee8d468/attachment.html>

From adria.garcia-alzorriz at adam.es  Wed Feb 27 10:21:28 2013
From: adria.garcia-alzorriz at adam.es (=?iso-8859-1?q?Adri=E0_Garc=EDa-Alz=F3rriz?=)
Date: Wed, 27 Feb 2013 10:21:28 +0000 (UTC)
Subject: [Gluster-users] GlusterFS performance
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
Message-ID: <kgkmn8$rk4$1@ger.gmane.org>

El dia Wed, 27 Feb 2013 15:37:53 +0600, en/na Nikita A Kardashin va
escriure:

> I have GlusterFS installation with parameters:
> 
> - 4 servers, connected by 1Gbit/s network (760-800 Mbit/s by iperf)
> - Distributed-replicated volume with 4 bricks and 2x4 redundancy
> formula. - Replicated volume with 2 bricks and 2x2 formula.

(...)

> What I can do with performance?

What version are you using on it? I'd suggest you 3.3.0 since it offers 
me best i/o ratios.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: not available
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/5662e9a9/attachment.sig>

From differentlocal at gmail.com  Wed Feb 27 10:36:58 2013
From: differentlocal at gmail.com (Nikita A Kardashin)
Date: Wed, 27 Feb 2013 16:36:58 +0600
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <kgkmn8$rk4$1@ger.gmane.org>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<kgkmn8$rk4$1@ger.gmane.org>
Message-ID: <CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>

I am using 3.3.0.

Now I remove volume and re-create it with 4-replica count (without
distribution) and got 31.9 MB/s :(


2013/2/27 Adri? Garc?a-Alz?rriz <adria.garcia-alzorriz at adam.es>

> El dia Wed, 27 Feb 2013 15:37:53 +0600, en/na Nikita A Kardashin va
> escriure:
>
> > I have GlusterFS installation with parameters:
> >
> > - 4 servers, connected by 1Gbit/s network (760-800 Mbit/s by iperf)
> > - Distributed-replicated volume with 4 bricks and 2x4 redundancy
> > formula. - Replicated volume with 2 bricks and 2x2 formula.
>
> (...)
>
> > What I can do with performance?
>
> What version are you using on it? I'd suggest you 3.3.0 since it offers
> me best i/o ratios.
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>



-- 
With best regards,
differentlocal (www.differentlocal.ru | differentlocal at gmail.com),
System administrator.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/847069ab/attachment.html>

From afrunning at gmail.com  Wed Feb 27 12:53:08 2013
From: afrunning at gmail.com (Al)
Date: Wed, 27 Feb 2013 07:53:08 -0500
Subject: [Gluster-users] Isolating Gluster volumes with a firewall (amazon
	security groups)
Message-ID: <CAAVuYqEm89urZ6RXumNLvekviYD57k6GBrpncp_F-Yy1NEAcwg@mail.gmail.com>

Hi All,

I am looking to utilize gluster on amazon aws for shared storage among
web servers (Elastic Beanstalk).  However, since I plan on using the
gluster tier for numerous different beanstalk environments, I'd like
to isolate the systems from accessing each others data.  Since
Beanstalk uses dynamic IP addresses, I can't utilize the built in
auth.allow and auth.reject in gluster to isolate volumes.  Is it
acceptable to firewall (utilizing security groups in amazon) the
underlying bricks (24009+) to prevent cross volume access?

Thanks in advance,

Al

From mike at cchtml.com  Wed Feb 27 13:34:02 2013
From: mike at cchtml.com (Michael Cronenworth)
Date: Wed, 27 Feb 2013 07:34:02 -0600
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<kgkmn8$rk4$1@ger.gmane.org>
	<CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>
Message-ID: <512E0B4A.8070608@cchtml.com>

On 02/27/2013 04:36 AM, Nikita A Kardashin wrote:
> I am using 3.3.0.
>
> Now I remove volume and re-create it with 4-replica count (without
> distribution) and got 31.9 MB/s :(

What are your volume settings? Have you adjusted the cache sizes?

From nux at li.nux.ro  Wed Feb 27 13:34:05 2013
From: nux at li.nux.ro (Nux!)
Date: Wed, 27 Feb 2013 13:34:05 +0000
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
Message-ID: <0eaf7ee8abfdd648ded09cd2558150ca@li.nux.ro>

On 27.02.2013 09:37, Nikita A Kardashin wrote:
> To Replicated gluster volume: 89MB/s
> To Distributed-replicated gluster volume: 49MB/s
> 
> Test command is: sync && echo 3 > /proc/sys/vm/drop_caches && dd
> if=/dev/zero of=gluster.test.bin bs=1G count=1

Hello Nikita,

To me that sounds just about right, it's the kind of speed I get as 
well. If you think of it, what happens in the background is that the 1GB 
file is written not only from you to one of the servers, but also from 
this server to the other servers, so it gets properly 
replicated/distributed.
So depending on your setup, you are not writing 1GB file once, but 3-4 
times, hence the drop in speed.
You could squeeze a bit more out of it if you can create the volumes 
over the servers' secondary nic so servers-to-server traffic goes 
through there.

-- 
Sent from the Delta quadrant using Borg technology!

Nux!
www.nux.ro

From mike at cchtml.com  Wed Feb 27 13:34:45 2013
From: mike at cchtml.com (Michael Cronenworth)
Date: Wed, 27 Feb 2013 07:34:45 -0600
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <512E0B4A.8070608@cchtml.com>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<kgkmn8$rk4$1@ger.gmane.org>
	<CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>
	<512E0B4A.8070608@cchtml.com>
Message-ID: <512E0B75.4010103@cchtml.com>

On 02/27/2013 07:34 AM, Michael Cronenworth wrote:
>
> What are your volume settings? Have you adjusted the cache sizes?

Sorry.. I see your original post and the settings now.

From differentlocal at gmail.com  Wed Feb 27 13:53:59 2013
From: differentlocal at gmail.com (Nikita A Kardashin)
Date: Wed, 27 Feb 2013 19:53:59 +0600
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <512E0B75.4010103@cchtml.com>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<kgkmn8$rk4$1@ger.gmane.org>
	<CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>
	<512E0B4A.8070608@cchtml.com> <512E0B75.4010103@cchtml.com>
Message-ID: <CALA_TQhb4TxkBfrmFkyo4ybBM=eJFDTHNyGo13+uGGon71hndQ@mail.gmail.com>

I know. But I think, each file written four times on speed of storage
(190MB/s, if network is not overloaded), and overall performance is remains
on usable level.

And on two same (by hardware) servers in replicated, not distributed mode
with scheme "2 bricks, 2 servers, 1x2 redundancy" I got about 100MB/s write
performance without any tuning (with default settings).

Why on 4 servers with 2x2 formula I got only 50MB/s with any tuning
settings? What speed I get in planned implementation - 9 servers in
distributed and replicated mode with 3x3 redundancy? 5Mb/s?

Maybe some system and/or gluster tweals can help me, or I going by wrong
way?
My use-case is simple - distributed, redundant shared storage for Openstack
cloud.
Initially we planing to use 9 servers in 3x3 scheme, in future - much more.


2013/2/27 Michael Cronenworth <mike at cchtml.com>

> On 02/27/2013 07:34 AM, Michael Cronenworth wrote:
>
>>
>> What are your volume settings? Have you adjusted the cache sizes?
>>
>
> Sorry.. I see your original post and the settings now.
>
> ______________________________**_________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.**org/mailman/listinfo/gluster-**users<http://supercolony.gluster.org/mailman/listinfo/gluster-users>
>



-- 
With best regards,
differentlocal (www.differentlocal.ru | differentlocal at gmail.com),
System administrator.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/7498573f/attachment.html>

From jdarcy at redhat.com  Wed Feb 27 14:34:37 2013
From: jdarcy at redhat.com (Jeff Darcy)
Date: Wed, 27 Feb 2013 09:34:37 -0500
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <0eaf7ee8abfdd648ded09cd2558150ca@li.nux.ro>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<0eaf7ee8abfdd648ded09cd2558150ca@li.nux.ro>
Message-ID: <512E197D.5080304@redhat.com>

On 02/27/2013 08:34 AM, Nux! wrote:
> To me that sounds just about right, it's the kind of speed I get as 
> well. If you think of it, what happens in the background is that the 1GB 
> file is written not only from you to one of the servers, but also from 
> this server to the other servers, so it gets properly 
> replicated/distributed.

Actually not quite.  The data has to go directly from the client to each
of the servers, so that client's outbound bandwidth gets divided by N
replicas.  The way you describe would actually perform better, because
then you get to use that first server's outbound bandwidth as well, so
for N=2 you'd be running at full speed (though at a slight cost in
latency if the writes are synchronous).  At N=4 you'd be at 1/3 speed,
because one copy has to go from client to first server while three have
to go from that first server to the others.  Some systems even do
"chain" replication where each server sends only to one other, giving an
even more extreme tradeoff between bandwidth and latency.

In the scenario that started this, the expected I/O rate is wire speed
divided by number of replicas.  If the measured network performance is
100MB/s (800Mb/s) then with two replicas one would expect no better than
50MB/s and with four replicas no better than 25MB/s.  Any better results
are likely to be the results of caching, so that the real bandwidth
isn't actually being measured.  Worse results are often the result of
contention, such as very large buffers resulting in memory starvation
for something else.  I don't know of any reason why results would be
worse with distribution, and have never seen that effect myself.

From Robert.vanLeeuwen at spilgames.com  Wed Feb 27 14:46:28 2013
From: Robert.vanLeeuwen at spilgames.com (Robert van Leeuwen)
Date: Wed, 27 Feb 2013 14:46:28 +0000
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <CALA_TQhb4TxkBfrmFkyo4ybBM=eJFDTHNyGo13+uGGon71hndQ@mail.gmail.com>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<kgkmn8$rk4$1@ger.gmane.org>
	<CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>
	<512E0B4A.8070608@cchtml.com> <512E0B75.4010103@cchtml.com>,
	<CALA_TQhb4TxkBfrmFkyo4ybBM=eJFDTHNyGo13+uGGon71hndQ@mail.gmail.com>
Message-ID: <79E00D9220302D448C1D59B5224ED12D8D6810D5@EchoDB02.spil.local>

> Mounted with default options by Gluster-FUSE.

Gluster-FUSE client takes care of replication.
So the client makes sure the data is sent to all server nodes.

So with a replicate count of 2 the traffic from the client node doubles.
With 3 replica's you triple network load from a client perspective.
So a client maxes out around 30 MB/sec with a 1Gbit uplink.

You could try to trunk the network on your client but 10Gbit ethernet ( i suggest to use fiber, because of latency issues with copper 10Gbit) 
or Infiniband would be the way to go for high throughput.

Before going that way I would first make sure your real-life load is indeed bandwidth limited and not IOPS limited.
Copying a few files and looking at the speed usually does not say a lot about real-life perfomance.
(unless real-life is also just one person copying big files around)
Most workloads I see hit IOPS limits a long time before throughput.

Cheers,
Robert

From torbjorn at trollweb.no  Wed Feb 27 15:14:41 2013
From: torbjorn at trollweb.no (=?ISO-8859-1?Q?Torbj=F8rn_Thorsen?=)
Date: Wed, 27 Feb 2013 16:14:41 +0100
Subject: [Gluster-users] Performance in VM guests when hosting VM images on
	Gluster
Message-ID: <CAD2iGhVgoVWDYGAP8KUYY5NE9-_wXd7Q00cZzjdjqkvD8cdacQ@mail.gmail.com>

I'm seeing less-than-stellar performance on my Gluster deployment when
hosting VM images on the FUSE mount.
I've seen that this topic has surfaced before, but my googling and
perusing of the list archive haven't turned out very conclusive.

I'm on a 2-node distribute+replicate cluster, the clients use Gluster
via the FUSE mount.
torbjorn at storage01:~$ sudo gluster volume info

Volume Name: gluster0
Type: Distributed-Replicate
Volume ID: 81bbf681-ecdb-4866-9b45-41d5d2df7b35
Status: Started
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: storage01.gluster.trollweb.net:/srv/gluster/brick0
Brick2: storage02.gluster.trollweb.net:/srv/gluster/brick0
Brick3: storage01.gluster.trollweb.net:/srv/gluster/brick1
Brick4: storage02.gluster.trollweb.net:/srv/gluster/brick1

The naive dd case from one client, on the dom0, looks like this:
torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
if=/dev/zero of=bigfile bs=1024k count=2000
2097152000 bytes (2.1 GB) copied, 22.9161 s, 91.5 MB/s

The clients see each node on a separate 1Gbps NIC, so this is pretty
close to the expected transfer rate.

Writing with the sync flag, from dom0, looks like so:
torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
if=/dev/zero of=bigfile bs=1024k count=2000 oflag=sync
2097152000 bytes (2.1 GB) copied, 51.1271 s, 41.0 MB/s

If we use a file on the gluster mount as backing for a loop device,
and do a sync write:
torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
2097152000 bytes (2.1 GB) copied, 56.3729 s, 37.2 MB/s

The Xen instances are managed by Ganeti, using the loopback interface
over a file on Gluster.
Inside the Xen instance the performance is not quite what I was hoping.
torbjorn at hennec:~$ sudo dd if=/dev/zero of=bigfile bs=1024k count=2000
2097152000 bytes (2.1 GB) copied, 1267.39 s, 1.7 MB/s

The transfer rate is similar when using sync or direct flags with dd.

Are these expected performance levels ?
A couple of threads[1] talk about performance, and seem to indicate my
situation isn't unique.
However, I'm under the impression that other are using a similar setup
with much better performance.


[1]:
* http://www.gluster.org/pipermail/gluster-users/2012-January/032369.html
* http://www.gluster.org/pipermail/gluster-users/2012-July/033763.html

--
Vennlig hilsen
Torbj?rn Thorsen
Utvikler / driftstekniker

Trollweb Solutions AS
- Professional Magento Partner
www.trollweb.no

Telefon dagtid: +47 51215300
Telefon kveld/helg: For kunder med Serviceavtale

Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
Postadresse: Maurholen 57, 4316 Sandnes

Husk at alle v?re standard-vilk?r alltid er gjeldende

From B.Candler at pobox.com  Wed Feb 27 15:29:30 2013
From: B.Candler at pobox.com (Brian Candler)
Date: Wed, 27 Feb 2013 15:29:30 +0000
Subject: [Gluster-users] GlusterFS performance
In-Reply-To: <79E00D9220302D448C1D59B5224ED12D8D6810D5@EchoDB02.spil.local>
References: <CALA_TQjVRx1Nb6hUMZL1g8W3fgT8X_kv7d3p5tkma_RQhYY3Rg@mail.gmail.com>
	<kgkmn8$rk4$1@ger.gmane.org>
	<CALA_TQjxuxraZWLBsAGBC9KfEpTYqxjjcrMFxbQi3XFDYewWJw@mail.gmail.com>
	<512E0B4A.8070608@cchtml.com> <512E0B75.4010103@cchtml.com>
	<CALA_TQhb4TxkBfrmFkyo4ybBM=eJFDTHNyGo13+uGGon71hndQ@mail.gmail.com>
	<79E00D9220302D448C1D59B5224ED12D8D6810D5@EchoDB02.spil.local>
Message-ID: <20130227152930.GA35643@nsrc.org>

On Wed, Feb 27, 2013 at 02:46:28PM +0000, Robert van Leeuwen wrote:
> You could try to trunk the network on your client but 10Gbit ethernet ( i suggest to use fiber, because of latency issues with copper 10Gbit) 

Aside: 10G with SFP+ direct-attach cables also works well, even though it's
copper.  The latency problem is with 10Gbase-T (RJ45 / CAT6(a))

From twake at cola.iges.org  Wed Feb 27 17:31:37 2013
From: twake at cola.iges.org (Thomas Wakefield)
Date: Wed, 27 Feb 2013 12:31:37 -0500
Subject: [Gluster-users] Slow read performance
Message-ID: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>

Help please-


I am running 3.3.1 on Centos using a 10GB network.  I get reasonable write speeds, although I think they could be faster.  But my read speeds are REALLY slow.

Executive summary:

On gluster client-
Writes average about 700-800MB/s
Reads average about 70-80MB/s

On server-
Writes average about 1-1.5GB/s
Reads average about 2-3GB/s

Any thoughts?



Here are some additional details:

Nothing interesting in any of the log files, everything is very quite.
All servers had no other load, and all clients are performing the same way.


Volume Name: shared
Type: Distribute
Volume ID: de11cc19-0085-41c3-881e-995cca244620
Status: Started
Number of Bricks: 26
Transport-type: tcp
Bricks:
Brick1: fs-disk2:/storage/disk2a
Brick2: fs-disk2:/storage/disk2b
Brick3: fs-disk2:/storage/disk2d
Brick4: fs-disk2:/storage/disk2e
Brick5: fs-disk2:/storage/disk2f
Brick6: fs-disk2:/storage/disk2g
Brick7: fs-disk2:/storage/disk2h
Brick8: fs-disk2:/storage/disk2i
Brick9: fs-disk2:/storage/disk2j
Brick10: fs-disk2:/storage/disk2k
Brick11: fs-disk2:/storage/disk2l
Brick12: fs-disk2:/storage/disk2m
Brick13: fs-disk2:/storage/disk2n
Brick14: fs-disk2:/storage/disk2o
Brick15: fs-disk2:/storage/disk2p
Brick16: fs-disk2:/storage/disk2q
Brick17: fs-disk2:/storage/disk2r
Brick18: fs-disk2:/storage/disk2s
Brick19: fs-disk2:/storage/disk2t
Brick20: fs-disk2:/storage/disk2u
Brick21: fs-disk2:/storage/disk2v
Brick22: fs-disk2:/storage/disk2w
Brick23: fs-disk2:/storage/disk2x
Brick24: fs-disk3:/storage/disk3a
Brick25: fs-disk3:/storage/disk3b
Brick26: fs-disk3:/storage/disk3c
Options Reconfigured:
performance.write-behind: on
performance.read-ahead: on
performance.io-cache: on
performance.stat-prefetch: on
performance.quick-read: on
cluster.min-free-disk: 500GB
nfs.disable: off


sysctl.conf settings for 10GBe
# increase TCP max buffer size settable using setsockopt()
net.core.rmem_max = 67108864 
net.core.wmem_max = 67108864 
# increase Linux autotuning TCP buffer limit
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864
# increase the length of the processor input queue
net.core.netdev_max_backlog = 250000
# recommended default congestion control is htcp 
net.ipv4.tcp_congestion_control=htcp
# recommended for hosts with jumbo frames enabled
net.ipv4.tcp_mtu_probing=1






Thomas W.
Sr.  Systems Administrator COLA/IGES
twake at cola.iges.org
Affiliate Computer Scientist GMU

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/2d08810b/attachment.html>

From d.a.bretherton at reading.ac.uk  Wed Feb 27 18:43:40 2013
From: d.a.bretherton at reading.ac.uk (Dan Bretherton)
Date: Wed, 27 Feb 2013 18:43:40 +0000
Subject: [Gluster-users] I/O error repaired only by owner or root access
In-Reply-To: <512B9502.8040709@reading.ac.uk>
References: <10257952.36.1361534190609.JavaMail.raj@california>
	<512B643F.6030706@reading.ac.uk>
	<suvppzo75rj.fsf@nock.cfmi.georgetown.edu>
	<512B9502.8040709@reading.ac.uk>
Message-ID: <512E53DC.6020604@reading.ac.uk>


--
Dan Bretherton
ESSC Computer System Manager
Department of Meteorology
Harry Pitt Building, 3 Earley Gate
University of Reading
Reading, RG6 7BE (or RG6 6AL for postal service deliveries)
UK
Tel. +44 118 378 5205, Fax: +44 118 378 6413
-- 
## Please sponsor me to run in VSO's 30km Race to the Eye ##
##        http://www.justgiving.com/DanBretherton         ##

On 02/25/2013 04:44 PM, Dan Bretherton wrote:
> On 02/25/2013 03:49 PM, Shawn Nock wrote:
>> Dan Bretherton <d.a.bretherton at reading.ac.uk> writes:
>>
>>> Hello Rajesh- Here are the permissions. The path in question is a
>>> directory.
>>>
>>> [sms05dab at jupiter ~]$ ls -ld /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl
>>> drwxr-xr-x 60 vq901510 nemo 110592 Feb 23 04:37
>>> /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN/Ctl [sms05dab at jupiter ~]$ ls -ld
>>> /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN lrwxrwxrwx 1 gcs nemo 49 Feb 1
>>> 2012 /users/gcs/WORK/ORCA1/ORCA1-R07-MEAN ->
>>> /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN [sms05dab at jupiter
>>> ~]$ ls -ld /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>>> drwxr-xr-x 27 gcs nemo 99210 Feb 23 03:14
>>> /data/pegasus/users/gcs/WORK/ORCA1/ORCA1-R07-MEAN
>>>
>>> As you can see the parent directory in this case was a symlink but
>>> that's not significant.  I ran the "ls -l" commands using my account -
>>> sms05dab, but the problem was originally reported by user vq901510.
>>> until I did "ls -l" as root neither of us could access the directory,
>>> because the parent directory was owned by user gcs.  Usually the
>>> problem is related to ownership of the file or directory itself.  This
>>> is the first time I have seen the I/O error caused by parent directory
>>> permissions.
>>>
>>> This problem seems to have started following an add-brick operation a
>>> few weeks ago, after which I started "gluster volume rebalance
>>> <VOLNAME> fix-layout" (which is still running). It occurred to me that
>>> the problem could be related to link files, many of which need to be
>>> rewritten following add-brick operations.  This could explain why the
>>> ownership of the parent directory is significant, because users
>>> sms05dab and vq901510 don't have permission to write in the parent
>>> directory owned by user gcs.  Normally this wouldn't be a problem
>>> because only read access to other users' data is required, but it
>>> appears as though read access was being denied because the new link
>>> file couldn't be written by unprivileged users.  Is this a plausible
>>> explanation of the I/O error do you think?
>> This sounds like my recent bug:
>> https://bugzilla.redhat.com/show_bug.cgi?id=913699
>>
>> In the bug, I said that writing on the fuse mount of one of the brick
>> servers fixed the problem.... but those were the only hosts I was
>> attempting access as root.
>>
> Thanks Shawn. To confirm that we are seeing the same bug I will try 
> accessing affected files from a FUSE mount on a server the next time 
> it happens.
> -Dan.
>
I have updated the bug report with another recent example.  In this most 
recent case, attempting to open a file for reading as an unprivileged 
user resulted in the error "Invalid argument", although "ls -l" worked 
without error.  This happened on a compute server and via a GlusterFS 
client mount point on a GlusterFS storage server. Changing the ownership 
of the parent directory to give the unprivileged user write access to 
the directory (making it writeable by group) allowed the user to open 
the file for reading.
-Dan.

From driver at megahappy.net  Wed Feb 27 19:46:28 2013
From: driver at megahappy.net (Bryan Whitehead)
Date: Wed, 27 Feb 2013 11:46:28 -0800
Subject: [Gluster-users] Slow read performance
In-Reply-To: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
References: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
Message-ID: <CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>

How are you doing the read/write tests on the fuse/glusterfs mountpoint?
Many small files will be slow because all the time is spent coordinating
locks.


On Wed, Feb 27, 2013 at 9:31 AM, Thomas Wakefield <twake at cola.iges.org>wrote:

> Help please-
>
>
> I am running 3.3.1 on Centos using a 10GB network.  I get reasonable write
> speeds, although I think they could be faster.  But my read speeds are
> REALLY slow.
>
> Executive summary:
>
> On gluster client-
> Writes average about 700-800MB/s
> Reads average about 70-80MB/s
>
> On server-
> Writes average about 1-1.5GB/s
> Reads average about 2-3GB/s
>
> Any thoughts?
>
>
>
> Here are some additional details:
>
> Nothing interesting in any of the log files, everything is very quite.
> All servers had no other load, and all clients are performing the same way.
>
>
> Volume Name: shared
> Type: Distribute
> Volume ID: de11cc19-0085-41c3-881e-995cca244620
> Status: Started
> Number of Bricks: 26
> Transport-type: tcp
> Bricks:
> Brick1: fs-disk2:/storage/disk2a
> Brick2: fs-disk2:/storage/disk2b
> Brick3: fs-disk2:/storage/disk2d
> Brick4: fs-disk2:/storage/disk2e
> Brick5: fs-disk2:/storage/disk2f
> Brick6: fs-disk2:/storage/disk2g
> Brick7: fs-disk2:/storage/disk2h
> Brick8: fs-disk2:/storage/disk2i
> Brick9: fs-disk2:/storage/disk2j
> Brick10: fs-disk2:/storage/disk2k
> Brick11: fs-disk2:/storage/disk2l
> Brick12: fs-disk2:/storage/disk2m
> Brick13: fs-disk2:/storage/disk2n
> Brick14: fs-disk2:/storage/disk2o
> Brick15: fs-disk2:/storage/disk2p
> Brick16: fs-disk2:/storage/disk2q
> Brick17: fs-disk2:/storage/disk2r
> Brick18: fs-disk2:/storage/disk2s
> Brick19: fs-disk2:/storage/disk2t
> Brick20: fs-disk2:/storage/disk2u
> Brick21: fs-disk2:/storage/disk2v
> Brick22: fs-disk2:/storage/disk2w
> Brick23: fs-disk2:/storage/disk2x
> Brick24: fs-disk3:/storage/disk3a
> Brick25: fs-disk3:/storage/disk3b
> Brick26: fs-disk3:/storage/disk3c
> Options Reconfigured:
> performance.write-behind: on
> performance.read-ahead: on
> performance.io-cache: on
> performance.stat-prefetch: on
> performance.quick-read: on
> cluster.min-free-disk: 500GB
> nfs.disable: off
>
>
> sysctl.conf settings for 10GBe
> # increase TCP max buffer size settable using setsockopt()
> net.core.rmem_max = 67108864
> net.core.wmem_max = 67108864
> # increase Linux autotuning TCP buffer limit
> net.ipv4.tcp_rmem = 4096 87380 67108864
> net.ipv4.tcp_wmem = 4096 65536 67108864
> # increase the length of the processor input queue
> net.core.netdev_max_backlog = 250000
> # recommended default congestion control is htcp
> net.ipv4.tcp_congestion_control=htcp
> # recommended for hosts with jumbo frames enabled
> net.ipv4.tcp_mtu_probing=1
>
>
>
>
>
>
> Thomas W.
> Sr.  Systems Administrator COLA/IGES
> twake at cola.iges.org
> Affiliate Computer Scientist GMU
>
>
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/306efec0/attachment-0001.html>

From bfoster at redhat.com  Wed Feb 27 20:46:17 2013
From: bfoster at redhat.com (Brian Foster)
Date: Wed, 27 Feb 2013 15:46:17 -0500
Subject: [Gluster-users] Performance in VM guests when hosting VM images
 on Gluster
In-Reply-To: <CAD2iGhVgoVWDYGAP8KUYY5NE9-_wXd7Q00cZzjdjqkvD8cdacQ@mail.gmail.com>
References: <CAD2iGhVgoVWDYGAP8KUYY5NE9-_wXd7Q00cZzjdjqkvD8cdacQ@mail.gmail.com>
Message-ID: <512E7099.9050308@redhat.com>

On 02/27/2013 10:14 AM, Torbj?rn Thorsen wrote:
> I'm seeing less-than-stellar performance on my Gluster deployment when
> hosting VM images on the FUSE mount.
> I've seen that this topic has surfaced before, but my googling and
> perusing of the list archive haven't turned out very conclusive.
> 
> I'm on a 2-node distribute+replicate cluster, the clients use Gluster
> via the FUSE mount.
> torbjorn at storage01:~$ sudo gluster volume info
> 
> Volume Name: gluster0
> Type: Distributed-Replicate
> Volume ID: 81bbf681-ecdb-4866-9b45-41d5d2df7b35
> Status: Started
> Number of Bricks: 2 x 2 = 4
> Transport-type: tcp
> Bricks:
> Brick1: storage01.gluster.trollweb.net:/srv/gluster/brick0
> Brick2: storage02.gluster.trollweb.net:/srv/gluster/brick0
> Brick3: storage01.gluster.trollweb.net:/srv/gluster/brick1
> Brick4: storage02.gluster.trollweb.net:/srv/gluster/brick1
> 
> The naive dd case from one client, on the dom0, looks like this:
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
> if=/dev/zero of=bigfile bs=1024k count=2000
> 2097152000 bytes (2.1 GB) copied, 22.9161 s, 91.5 MB/s
> 
> The clients see each node on a separate 1Gbps NIC, so this is pretty
> close to the expected transfer rate.
> 
> Writing with the sync flag, from dom0, looks like so:
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
> if=/dev/zero of=bigfile bs=1024k count=2000 oflag=sync
> 2097152000 bytes (2.1 GB) copied, 51.1271 s, 41.0 MB/s
> 
> If we use a file on the gluster mount as backing for a loop device,
> and do a sync write:
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
> if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
> 2097152000 bytes (2.1 GB) copied, 56.3729 s, 37.2 MB/s
> 

What you might want to try is compare each case with gluster profiling
enabled on your volume (e.g., run a 'gluster ... profile info' to clear
the interval stats, run your test, run another 'profile info' and see
how many write requests occurred, divide the amount of data transferred
by the number of requests).

Running similar tests on a couple random servers around here brings me
from 70-80MB/s down to 10MB/s over loop. The profile data clearly shows
that loop is breaking what were previously 128k (max) write requests
into 4k requests. I don't know enough about the block layer to say why
that occurs, but I'd be suspicious of the combination of the block
interface on top of a filesystem (fuse) with synchronous request
submission (no caching, writes are immediately submitted to the client
fs). That said, I'm on an older kernel (or an older loop driver anyways,
I think) and your throughput above doesn't seem to be much worse with
loop alone...

Brian

> The Xen instances are managed by Ganeti, using the loopback interface
> over a file on Gluster.
> Inside the Xen instance the performance is not quite what I was hoping.
> torbjorn at hennec:~$ sudo dd if=/dev/zero of=bigfile bs=1024k count=2000
> 2097152000 bytes (2.1 GB) copied, 1267.39 s, 1.7 MB/s
> 
> The transfer rate is similar when using sync or direct flags with dd.
> 
> Are these expected performance levels ?
> A couple of threads[1] talk about performance, and seem to indicate my
> situation isn't unique.
> However, I'm under the impression that other are using a similar setup
> with much better performance.
> 
> 
> [1]:
> * http://www.gluster.org/pipermail/gluster-users/2012-January/032369.html
> * http://www.gluster.org/pipermail/gluster-users/2012-July/033763.html
> 
> --
> Vennlig hilsen
> Torbj?rn Thorsen
> Utvikler / driftstekniker
> 
> Trollweb Solutions AS
> - Professional Magento Partner
> www.trollweb.no
> 
> Telefon dagtid: +47 51215300
> Telefon kveld/helg: For kunder med Serviceavtale
> 
> Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
> Postadresse: Maurholen 57, 4316 Sandnes
> 
> Husk at alle v?re standard-vilk?r alltid er gjeldende
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 


From twake at cola.iges.org  Wed Feb 27 20:56:35 2013
From: twake at cola.iges.org (Thomas Wakefield)
Date: Wed, 27 Feb 2013 15:56:35 -0500
Subject: [Gluster-users] Slow read performance
In-Reply-To: <CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>
References: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
	<CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>
Message-ID: <CA303739-FED2-4BE6-9F1B-674036C028CF@cola.iges.org>

I have tested everything, small and large files.  I have used file sizes ranging from 128k up to multiple GB files.  All the reads are bad.



Here is a fairly exhaustive iozone auto test:

                                                            random  random    bkwd   record   stride                                   
              KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread
              64       4   40222   63492    26868    30060    1620   71037    1572    70570    31294    77096    72475   14736    13928
              64       8   99207  116366    13591    13513    3214   97690    3155   109978    28920   152018   158480   18936    17625
              64      16  230257  253766    25156    28713   10867  223732    8873   244297    54796   303383   312204   15062    13545
              64      32  255943  234481  5735102  7100397   11897  318502   13681   347801    24214   695778   528618   25838    28094
              64      64  214096  681644  6421025  7100397   27453  292156   28117   621657    27338   376062   512471   28569    32534
             128       4   74329   75468    26428    41089    1131   72857    1118    66976     1597    73778    78343   13351    13026
             128       8  100862  135170    24966    16734    2617  118966    2560   120406    39156   125121   146613   16177    16180
             128      16  115114  253983    28212    17854    5307  246180    5431   229843    47335   255920   271173   27256    24445
             128      32  256042  391360    39848    64258   11329  290230    9905   429563    38176   490380   463696   20917    19219
             128      64  248573  592699  4557257  6812590   19583  452366   29263   603357    42967   814915   692017   76327    37604
             128     128  921183  526444  5603747  5379161   45614  390222   65441   826202    41384   662962  1040839   78526    39023
             256       4   76212   77337    40295    32125    1289   71866    1261    64645     1436    57309    53048   23073    29550
             256       8  126922  141976    26237    25130    2566  128058    2565   138981     2985   125060   133603   22840    24955
             256      16  242883  263636    41850    24371    4902  250009    5290   248792    89353   243821   247303   26965    26199
             256      32  409074  439732    40101    39335   11953  436870   11209   430218    83743   409542   479390   30821    27750
             256      64  259935  571502    64840    71847   22537  617161   23383   392047    91852   672010   802614   41673    53111
             256     128  847597  812329   185517    83198   49383  708831   44668   794889    74267  1180188  1662639   54303    41018
             256     256  481324  709299  5217259  5320671   44668  719277   40954   808050    41302   790209   771473   62224    35754
             512       4   77667   75226    35102    29696    1337   66262    1451    67680     1413    69265    69142   42084    27897
             512       8  134311  144341    30144    24646    2102  134143    2209   134699     2296   108110   128616   25104    29123
             512      16  200085  248787    30235    25697    4196  247240    4179   256116     4768   250003   226436   32351    28455
             512      32  330341  439805    26440    39284    8744  457611    8006   424168   125953   425935   448813   27660    26951
             512      64  483906  733729    48747    41121   16032  555938   17424   587256   187343   366977   735740   41700    41548
             512     128  836636  907717    69359    94921   42443  761031   36828   964378   123165   651383   695697   58368    44459
             512     256  520879  860437   145534   135523   40267  847532   31585   663252    69696  1270846  1492545   48822    48092
             512     512  782951  973118  3099691  2942541   42328  871966   46218   911184    49791   953248  1036527   52723    48347
            1024       4   76218   69362    36431    28711    1137   66171    1174    68938     1125    70566    70845   34942    28914
            1024       8  126045  140524    37836    15664    2698  126000    2557   125566     2567   110858   127255   26764    27945
            1024      16  243398  261429    40238    23263    3987  246400    3882   260746     4093   236652   236874   31429    25076
            1024      32  383109  422076    41731    41605    8277  473441    7775   415261     8588   394765   407306   40089    28537
            1024      64  590145  619156    39623    53267   15051  722717   14624   753000   257294   597784   620946   38619    44073
            1024     128 1077836 1124099    56192    64916   36851 1102176   37198  1082454   281548   829175   792604   47975    51913
            1024     256  941918 1074331    72783    81450   26778 1099636   32395  1060013   183218  1024121   995171   44371    45448
            1024     512  697483 1130312   100324   114682   48215 1041758   41480  1058967    90156   994020  1563622   56328    46370
            1024    1024  931702 1087111  4609294  4199201   44191  949834   45594   970656    56674   933525  1075676   44876    46115
            2048       4   71438   67066    58319    38913    1147   44147    1043    42916      967    66416    67205   45953    96750
            2048       8  141926  134567    61101    55445    2596   77528    2564    80402     4258   124211   120747   53888   100337
            2048      16  254344  255585    71550    74500    5410  139365    5201   141484     5171   205521   213113   67048    57304
            2048      32  397833  411261    56676    80027   10440  260034   10126   230238    10814   391665   383379   79333    60877
            2048      64  595167  687205    64262    87327   20772  456430   19960   477064    23190   540220   563096   86812    92565
            2048     128  833585  933403   121926   118621   37700  690020   37575   733254   567449   712337   734006   92011   104934
            2048     256  799003  949499   143688   125659   40871  892757   37977   880494   458281   836263   901375  131332   110237
            2048     512  979936 1040724   120896   138013   54381  859783   48721   780491   279203  1068824  1087085   97886    98078
            2048    1024  901754  987938    53352    53043   72727 1054522   68269   992275   181253  1309480  1524983  121600    95585
            2048    2048  831890 1021540  4257067  3302797   75672  984203   80181   826209    94278   966920  1027159  111832   105921
            4096       4   66195   67316    62171    74785    1328   28963    1329    26397     1223    71470    69317   55903    84915
            4096       8  122221  120057    90537    60958    2598   47312    2468    59783     2640   128674   127872   41285    40422
            4096      16  238321  239251    29336    32121    4153   89262    3986    96930     4608   229970   237108   55039    56983
            4096      32  417110  421356    30974    50000    8382  156676    7886   153841     7900   359585   367288   26611    25952
            4096      64  648008  668066    32193    29389   14830  273265   14822   282211    19653   581898   620798   51281    50218
            4096     128  779422  848564    55594    60253   37108  451296   35908   491361    37567   738163   728059   67681    66440
            4096     256  865623  886986    71368    63947   44255  645961   42689   719491   736707   819696   837641   57059    60347
            4096     512  852099  889650    68870    73891   31185  845224   30259   830153   392334   910442   961983   60083    55558
            4096    1024  710357  867810    29377    29522   49954  846640   43665   926298   213677   986226  1115445   55130    59205
            4096    2048  826479  908420    43191    42075   59684  904022   58601   855664   115105  1418322  1524415   60548    66066
            4096    4096  793351  855111  3232454  3673419   66018  861413   48833   847852    45914   852268   842075   42980    48374
            8192       4   67340   69421    42198    31740     994   23251    1166    16813      837    73827    73126   25169    29610
            8192       8  137150  125622    29131    36439    2051   44342    1988    48930     2315   134183   135367   31080    33573
            8192      16  237366  220826    24810    26584    3576   88004    3769    78717     4289   233751   235355   23302    28742
            8192      32  457447  454404    31594    27750    8141  142022    7846   143984     9322   353147   396188   34203    33265
            8192      64  670645  655259    28630    23255   16669  237476   16965   244968    15607   590365   575320   49998    43305
            8192     128  658676  760982    44197    47802   28693  379523   26614   378328    27184   720997   702038   51707    49733
            8192     256  643370  698683    56233    63165   28846  543952   27745   576739    44014   701007   725534   59611    58985
            8192     512  696884  776793    67258    52705   18711  698854   21004   694124   621695   784812   773331   43101    47659
            8192    1024  729664  810451   15470    15875   31318  801490   38123   812944   301222   804323   832765   54308    53376
            8192    2048  749217   68757    21914    22667   48971  783309   48132   782738   172848   907408   929324   51156    50565
            8192    4096  707677  763960    32063    31928   47809  751692   49560   786339    93445  1046761  1297876   48037    51680
            8192    8192  623817  746288  2815955  3137358   48722  741633   35428   753787    49626   803683   823800   48977    52895
           16384       4   72372   73651    34471    30788     960   23610     903    22316      891    71445    71138   56451    55129
           16384       8  137920  141704    50830    33857    1935   41934    2275    35588     3608   130757   137801   51621    48525
           16384      16  245369  242460    41808    29770    3605   75682    4355    75315     4767   241100   239693   53263    30785
           16384      32  448877  433956    31846    35010    7973  118181    8819   112703     8177   381734   391651   57749    63417
           16384      64  710831  700712    66792    68864   20176  209806   19034   207852    21255   589503   601379  104567   105162
           16384     128  836901  860867   104226   100373   40899  358865   40946   360562    39415   675968   691538   96086   105695
           16384     256  798081  828146   107103   120433   39084  595325   39050   593110    56925   763466   797859  109645   113414
           16384     512  810851  843931   113564   106202   35111  714831   46244   745947    53636   802902   760172  110492   100879
           16384    1024  726399  820219    22106    22987   53087  749053   54781   777705  1075341   772686   809723  100349    96619
           16384    2048  807772  856458    23920    23617   66320  829576   72105   740848   656379   864539   835446   93499   101714
           16384    4096  797470  840596    27270    28132   88784  834938   83428   836809   342898   953274   969699   96499   100657
           16384    8192  760063  812942    41022    40014  102020  812135  104063   820346   229870  1252409  1398306  100959   103793
           16384   16384  743107  825054  3074035  3551062   97740  826175  103597   803926    94681   820474   829106   97168    92018
           32768      64  713528  714585    71310    74296   21135  194177   21551   202998    21110   571030   595232   98598   103373
           32768     128  766698  820615   103028   102269   39013  328409   40325   346403    41084   684604   702513  103887   100689
           32768     256  766591  819138   107557   113878   42088  574878   41778   557989    55045   749922   754434  100964   104866
           32768     512  774365  846915   104255   106094   35280  728957   25829   734316    26990   750614   800955   55013    63750
           32768    1024  759113  806241    22033    22622   37322  761870   45003   766950    54519   755666   741339   70018    66255
           32768    2048  797566  860344    22893    23859   49973  778872   53509   779539   796674   817078   797585   56750    57047
           32768    4096  796693  798985    24267    25222   54410  781810   53053   802845   421421   832858   855811   56748    60507
           32768    8192  741543  795144    29254    29150   58678  830011   55545   833914   221323   914182   952916   56120    60014
           32768   16384  754641  819797    44183    45718   67293  785047   65078   807273   132136  1240414  1267713   56585    56167
           65536      64  663333  656588    32211    31925   17324  202043   15477   215981    15496   592126   597188   86431    93969
           65536     128  716218  791603    92035    95614   40330  330127   39295   346542    39469   697385   714453  105094    98327
           65536     256  769283  798701   110285   110085   38404  535730   39939   585023    52402   741206   749899   98671   100106
           65536     512  728151  814516   112550   108873   32284  690164   38911   708244    50047   778669   765303   98333    92728
           65536    1024  773504  827108    21542    20741   45901  775473   57282   807232    69359   770793   766736  101465   100905
           65536    2048  767661  853446    21077    21260   59825  762711   71985   802488    87290   772402   775685   96832   103189
           65536    4096  767669  828165    20681    21480   79798  829757   91755   838367  1379411   791248   783023  106433   112769
           65536    8192  739291  784946    23100    22454   85945  785134   87081   828489   642088   816781   798275   93307    95318
           65536   16384  736401  791669    26345    26384   93082  799356   91419   751766   385721   911919   884688  109668    96374
          131072      64  658361  661211    79790    81016   19460  182084   19325   195428    19553   551809   554426   96581    81979
          131072     128  759806  838994    51063    56976   33624  326699   36329   360593    35679   710602   716538   81725    51819
          131072     256  787375  843763    54664    65031   33847  557817   35093   581552    45163   750234   758452   82704    83472
          131072     512  777460  818049    87831    87110   27994  705621   31860   723071    38483   779489   793068   88281    86068
          131072    1024  737158  782780    19005    18897   36614  735240   38900   812074    52149   746598   762694   74376    82534
          131072    2048  777386  817330    24169    24693   55171  769662   56082   788848    55937   757716   763147   73508    82351
          131072    4096  751757  792201    25283    25173   48455  813945   56557   837418    59578   749106   757939   67846    70964
          131072    8192  743004  796673    20369    20327   88990  780925   95956   777211   951528   762755   739851   70850    69943
          131072   16384  766023  823611    22333    22142  108337  796852  110848   777704   718954   809861   802822  115896    82903
          262144      64  655769  695392    45973    45668   19083  209417   18768   207272    19133   601329   599267  107705    87261
          262144     128  750052  798999    72711    74713   37329  339480   36854   365587    37187   710898   714372  108400    78806
          262144     256  788355  815575    80897    75141   37080  549762   37521   574393    51731   780429   751131   97395    73842
          262144     512  714994  736649    80224    80195   32986  708196   36048   628508    44043   767699   781863  123267    83558
          262144    1024  793541  800826    20200    20428   45599  752027   48232   798229    57011   715643   763691   76741    73211
          262144    2048  699395  782935    20227    20519   65785  779219   80486   803403    93381   650054   712171  115611   108457
          262144    4096  629531  766261    20544    21563   83107  819917   93008   844103    90625   741879   754931   94114    94475
          262144    8192  731695  804112    21158    21589   82987  787869   84858   799483    95127   731822   737397   95950    95160
          262144   16384  703376  802552    21940    22371   87748  777024   90349   779626  1018679   765278   619969   96051    92734
          524288      64  649937  710342    73778    70893   19734  207179   19367   199709    19493   585910   581340   91902    91443
          524288     128  739270  805153    91507    90395   38547  319049   37988   358516    38123   712602   711814   92463    90851
          524288     256  751279  802496    89481    91022   38869  544097   38244   569142    56130   756367   752466   92677    92875
          524288     512  758393  819788    58085    73437   29030  700120   28767   739236    33925   770177   795879   58963    72689
          524288    1024  778385  818368    18822    18639   38774  798827   50898   799492    55679   801849   800917   92778    82686
          524288    2048  780594  834612    19936    19994   63147  769555   63007   802512    66322   772900   771314   86628    93485
          524288    4096  769806  812570    20165    20137   79896  783615   71217    80835    68550    80166    73733  122559   116844
          524288    8192  797504  887823   123824   124513   94520  844880   86441   710319    65077   636713   819868   87765    80121
          524288   16384  788468  858216    92580   117354  109583  850179   63966   856965   121151   789252   805040  123810   119088





On Feb 27, 2013, at 2:46 PM, Bryan Whitehead <driver at megahappy.net> wrote:

> How are you doing the read/write tests on the fuse/glusterfs mountpoint? Many small files will be slow because all the time is spent coordinating locks.
> 
> 
> On Wed, Feb 27, 2013 at 9:31 AM, Thomas Wakefield <twake at cola.iges.org> wrote:
> Help please-
> 
> 
> I am running 3.3.1 on Centos using a 10GB network.  I get reasonable write speeds, although I think they could be faster.  But my read speeds are REALLY slow.
> 
> Executive summary:
> 
> On gluster client-
> Writes average about 700-800MB/s
> Reads average about 70-80MB/s
> 
> On server-
> Writes average about 1-1.5GB/s
> Reads average about 2-3GB/s
> 
> Any thoughts?
> 
> 
> 
> Here are some additional details:
> 
> Nothing interesting in any of the log files, everything is very quite.
> All servers had no other load, and all clients are performing the same way.
> 
> 
> Volume Name: shared
> Type: Distribute
> Volume ID: de11cc19-0085-41c3-881e-995cca244620
> Status: Started
> Number of Bricks: 26
> Transport-type: tcp
> Bricks:
> Brick1: fs-disk2:/storage/disk2a
> Brick2: fs-disk2:/storage/disk2b
> Brick3: fs-disk2:/storage/disk2d
> Brick4: fs-disk2:/storage/disk2e
> Brick5: fs-disk2:/storage/disk2f
> Brick6: fs-disk2:/storage/disk2g
> Brick7: fs-disk2:/storage/disk2h
> Brick8: fs-disk2:/storage/disk2i
> Brick9: fs-disk2:/storage/disk2j
> Brick10: fs-disk2:/storage/disk2k
> Brick11: fs-disk2:/storage/disk2l
> Brick12: fs-disk2:/storage/disk2m
> Brick13: fs-disk2:/storage/disk2n
> Brick14: fs-disk2:/storage/disk2o
> Brick15: fs-disk2:/storage/disk2p
> Brick16: fs-disk2:/storage/disk2q
> Brick17: fs-disk2:/storage/disk2r
> Brick18: fs-disk2:/storage/disk2s
> Brick19: fs-disk2:/storage/disk2t
> Brick20: fs-disk2:/storage/disk2u
> Brick21: fs-disk2:/storage/disk2v
> Brick22: fs-disk2:/storage/disk2w
> Brick23: fs-disk2:/storage/disk2x
> Brick24: fs-disk3:/storage/disk3a
> Brick25: fs-disk3:/storage/disk3b
> Brick26: fs-disk3:/storage/disk3c
> Options Reconfigured:
> performance.write-behind: on
> performance.read-ahead: on
> performance.io-cache: on
> performance.stat-prefetch: on
> performance.quick-read: on
> cluster.min-free-disk: 500GB
> nfs.disable: off
> 
> 
> sysctl.conf settings for 10GBe
> # increase TCP max buffer size settable using setsockopt()
> net.core.rmem_max = 67108864 
> net.core.wmem_max = 67108864 
> # increase Linux autotuning TCP buffer limit
> net.ipv4.tcp_rmem = 4096 87380 67108864
> net.ipv4.tcp_wmem = 4096 65536 67108864
> # increase the length of the processor input queue
> net.core.netdev_max_backlog = 250000
> # recommended default congestion control is htcp 
> net.ipv4.tcp_congestion_control=htcp
> # recommended for hosts with jumbo frames enabled
> net.ipv4.tcp_mtu_probing=1
> 
> 
> 
> 
> 
> 
> Thomas W.
> Sr.  Systems Administrator COLA/IGES
> twake at cola.iges.org
> Affiliate Computer Scientist GMU
> 
> 
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/0d7f2801/attachment-0001.html>

From driver at megahappy.net  Wed Feb 27 22:31:38 2013
From: driver at megahappy.net (Bryan Whitehead)
Date: Wed, 27 Feb 2013 14:31:38 -0800
Subject: [Gluster-users] Slow read performance
In-Reply-To: <CA303739-FED2-4BE6-9F1B-674036C028CF@cola.iges.org>
References: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
	<CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>
	<CA303739-FED2-4BE6-9F1B-674036C028CF@cola.iges.org>
Message-ID: <CAA3XVTxO6icNKSKJ7M-wF4U6C9_ukivjhAD-e5RRHy726DigEQ@mail.gmail.com>

Every time you open/close a file or a directory you will have to wait for
locks which take time. This is totally expected.

Why don't you share what you want to do? iozone benchmarks look like crap
but serving qcow2 files to qemu works fantastic for me. What are you doing?
Make a benchmark that does that. If you are going to have many files with a
wide variety of sizes glusterfs/fuse might not be what you are looking for.


On Wed, Feb 27, 2013 at 12:56 PM, Thomas Wakefield <twake at cola.iges.org>wrote:

> I have tested everything, small and large files.  I have used file sizes
> ranging from 128k up to multiple GB files.  All the reads are bad.
>
>
>
> Here is a fairly exhaustive iozone auto test:
>
>                                                             random  random
>    bkwd   record   stride
>               KB  reclen   write rewrite    read    reread    read   write
>    read  rewrite     read   fwrite frewrite   fread  freread
>               64       4   40222   63492    26868    30060    1620   71037
>    1572    70570    31294    77096    72475   14736    13928
>               64       8   99207  116366    13591    13513    3214   97690
>    3155   109978    28920   152018   158480   18936    17625
>               64      16  230257  253766    25156    28713   10867  223732
>    8873   244297    54796   303383   312204   15062    13545
>               64      32  255943  234481  5735102  7100397   11897  318502
>   13681   347801    24214   695778   528618   25838    28094
>               64      64  214096  681644  6421025  7100397   27453  292156
>   28117   621657    27338   376062   512471   28569    32534
>              128       4   74329   75468    26428    41089    1131   72857
>    1118    66976     1597    73778    78343   13351    13026
>              128       8  100862  135170    24966    16734    2617  118966
>    2560   120406    39156   125121   146613   16177    16180
>              128      16  115114  253983    28212    17854    5307  246180
>    5431   229843    47335   255920   271173   27256    24445
>              128      32  256042  391360    39848    64258   11329  290230
>    9905   429563    38176   490380   463696   20917    19219
>              128      64  248573  592699  4557257  6812590   19583  452366
>   29263   603357    42967   814915   692017   76327    37604
>              128     128  921183  526444  5603747  5379161   45614  390222
>   65441   826202    41384   662962  1040839   78526    39023
>              256       4   76212   77337    40295    32125    1289   71866
>    1261    64645     1436    57309    53048   23073    29550
>              256       8  126922  141976    26237    25130    2566  128058
>    2565   138981     2985   125060   133603   22840    24955
>              256      16  242883  263636    41850    24371    4902  250009
>    5290   248792    89353   243821   247303   26965    26199
>              256      32  409074  439732    40101    39335   11953  436870
>   11209   430218    83743   409542   479390   30821    27750
>              256      64  259935  571502    64840    71847   22537  617161
>   23383   392047    91852   672010   802614   41673    53111
>              256     128  847597  812329   185517    83198   49383  708831
>   44668   794889    74267  1180188  1662639   54303    41018
>              256     256  481324  709299  5217259  5320671   44668  719277
>   40954   808050    41302   790209   771473   62224    35754
>              512       4   77667   75226    35102    29696    1337   66262
>    1451    67680     1413    69265    69142   42084    27897
>              512       8  134311  144341    30144    24646    2102  134143
>    2209   134699     2296   108110   128616   25104    29123
>              512      16  200085  248787    30235    25697    4196  247240
>    4179   256116     4768   250003   226436   32351    28455
>              512      32  330341  439805    26440    39284    8744  457611
>    8006   424168   125953   425935   448813   27660    26951
>              512      64  483906  733729    48747    41121   16032  555938
>   17424   587256   187343   366977   735740   41700    41548
>              512     128  836636  907717    69359    94921   42443  761031
>   36828   964378   123165   651383   695697   58368    44459
>              512     256  520879  860437   145534   135523   40267  847532
>   31585   663252    69696  1270846  1492545   48822    48092
>              512     512  782951  973118  3099691  2942541   42328  871966
>   46218   911184    49791   953248  1036527   52723    48347
>             1024       4   76218   69362    36431    28711    1137   66171
>    1174    68938     1125    70566    70845   34942    28914
>             1024       8  126045  140524    37836    15664    2698  126000
>    2557   125566     2567   110858   127255   26764    27945
>             1024      16  243398  261429    40238    23263    3987  246400
>    3882   260746     4093   236652   236874   31429    25076
>             1024      32  383109  422076    41731    41605    8277  473441
>    7775   415261     8588   394765   407306   40089    28537
>             1024      64  590145  619156    39623    53267   15051  722717
>   14624   753000   257294   597784   620946   38619    44073
>             1024     128 1077836 1124099    56192    64916   36851 1102176
>   37198  1082454   281548   829175   792604   47975    51913
>             1024     256  941918 1074331    72783    81450   26778 1099636
>   32395  1060013   183218  1024121   995171   44371    45448
>             1024     512  697483 1130312   100324   114682   48215 1041758
>   41480  1058967    90156   994020  1563622   56328    46370
>             1024    1024  931702 1087111  4609294  4199201   44191  949834
>   45594   970656    56674   933525  1075676   44876    46115
>             2048       4   71438   67066    58319    38913    1147   44147
>    1043    42916      967    66416    67205   45953    96750
>             2048       8  141926  134567    61101    55445    2596   77528
>    2564    80402     4258   124211   120747   53888   100337
>             2048      16  254344  255585    71550    74500    5410  139365
>    5201   141484     5171   205521   213113   67048    57304
>             2048      32  397833  411261    56676    80027   10440  260034
>   10126   230238    10814   391665   383379   79333    60877
>             2048      64  595167  687205    64262    87327   20772  456430
>   19960   477064    23190   540220   563096   86812    92565
>             2048     128  833585  933403   121926   118621   37700  690020
>   37575   733254   567449   712337   734006   92011   104934
>             2048     256  799003  949499   143688   125659   40871  892757
>   37977   880494   458281   836263   901375  131332   110237
>             2048     512  979936 1040724   120896   138013   54381  859783
>   48721   780491   279203  1068824  1087085   97886    98078
>             2048    1024  901754  987938    53352    53043   72727 1054522
>   68269   992275   181253  1309480  1524983  121600    95585
>             2048    2048  831890 1021540  4257067  3302797   75672  984203
>   80181   826209    94278   966920  1027159  111832   105921
>             4096       4   66195   67316    62171    74785    1328   28963
>    1329    26397     1223    71470    69317   55903    84915
>             4096       8  122221  120057    90537    60958    2598   47312
>    2468    59783     2640   128674   127872   41285    40422
>             4096      16  238321  239251    29336    32121    4153   89262
>    3986    96930     4608   229970   237108   55039    56983
>             4096      32  417110  421356    30974    50000    8382  156676
>    7886   153841     7900   359585   367288   26611    25952
>             4096      64  648008  668066    32193    29389   14830  273265
>   14822   282211    19653   581898   620798   51281    50218
>             4096     128  779422  848564    55594    60253   37108  451296
>   35908   491361    37567   738163   728059   67681    66440
>             4096     256  865623  886986    71368    63947   44255  645961
>   42689   719491   736707   819696   837641   57059    60347
>             4096     512  852099  889650    68870    73891   31185  845224
>   30259   830153   392334   910442   961983   60083    55558
>             4096    1024  710357  867810    29377    29522   49954  846640
>   43665   926298   213677   986226  1115445   55130    59205
>             4096    2048  826479  908420    43191    42075   59684  904022
>   58601   855664   115105  1418322  1524415   60548    66066
>             4096    4096  793351  855111  3232454  3673419   66018  861413
>   48833   847852    45914   852268   842075   42980    48374
>             8192       4   67340   69421    42198    31740     994   23251
>    1166    16813      837    73827    73126   25169    29610
>             8192       8  137150  125622    29131    36439    2051   44342
>    1988    48930     2315   134183   135367   31080    33573
>             8192      16  237366  220826    24810    26584    3576   88004
>    3769    78717     4289   233751   235355   23302    28742
>             8192      32  457447  454404    31594    27750    8141  142022
>    7846   143984     9322   353147   396188   34203    33265
>             8192      64  670645  655259    28630    23255   16669  237476
>   16965   244968    15607   590365   575320   49998    43305
>             8192     128  658676  760982    44197    47802   28693  379523
>   26614   378328    27184   720997   702038   51707    49733
>             8192     256  643370  698683    56233    63165   28846  543952
>   27745   576739    44014   701007   725534   59611    58985
>             8192     512  696884  776793    67258    52705   18711  698854
>   21004   694124   621695   784812   773331   43101    47659
>             8192    1024  729664  810451   15470    15875   31318  801490
>   38123   812944   301222   804323   832765   54308    53376
>             8192    2048  749217   68757    21914    22667   48971  783309
>   48132   782738   172848   907408   929324   51156    50565
>             8192    4096  707677  763960    32063    31928   47809  751692
>   49560   786339    93445  1046761  1297876   48037    51680
>             8192    8192  623817  746288  2815955  3137358   48722  741633
>   35428   753787    49626   803683   823800   48977    52895
>            16384       4   72372   73651    34471    30788     960   23610
>     903    22316      891    71445    71138   56451    55129
>            16384       8  137920  141704    50830    33857    1935   41934
>    2275    35588     3608   130757   137801   51621    48525
>            16384      16  245369  242460    41808    29770    3605   75682
>    4355    75315     4767   241100   239693   53263    30785
>            16384      32  448877  433956    31846    35010    7973  118181
>    8819   112703     8177   381734   391651   57749    63417
>            16384      64  710831  700712    66792    68864   20176  209806
>   19034   207852    21255   589503   601379  104567   105162
>            16384     128  836901  860867   104226   100373   40899  358865
>   40946   360562    39415   675968   691538   96086   105695
>            16384     256  798081  828146   107103   120433   39084  595325
>   39050   593110    56925   763466   797859  109645   113414
>            16384     512  810851  843931   113564   106202   35111  714831
>   46244   745947    53636   802902   760172  110492   100879
>            16384    1024  726399  820219    22106    22987   53087  749053
>   54781   777705  1075341   772686   809723  100349    96619
>            16384    2048  807772  856458    23920    23617   66320  829576
>   72105   740848   656379   864539   835446   93499   101714
>            16384    4096  797470  840596    27270    28132   88784  834938
>   83428   836809   342898   953274   969699   96499   100657
>            16384    8192  760063  812942    41022    40014  102020  812135
>  104063   820346   229870  1252409  1398306  100959   103793
>            16384   16384  743107  825054  3074035  3551062   97740  826175
>  103597   803926    94681   820474   829106   97168    92018
>            32768      64  713528  714585    71310    74296   21135  194177
>   21551   202998    21110   571030   595232   98598   103373
>            32768     128  766698  820615   103028   102269   39013  328409
>   40325   346403    41084   684604   702513  103887   100689
>            32768     256  766591  819138   107557   113878   42088  574878
>   41778   557989    55045   749922   754434  100964   104866
>            32768     512  774365  846915   104255   106094   35280  728957
>   25829   734316    26990   750614   800955   55013    63750
>            32768    1024  759113  806241    22033    22622   37322  761870
>   45003   766950    54519   755666   741339   70018    66255
>            32768    2048  797566  860344    22893    23859   49973  778872
>   53509   779539   796674   817078   797585   56750    57047
>            32768    4096  796693  798985    24267    25222   54410  781810
>   53053   802845   421421   832858   855811   56748    60507
>            32768    8192  741543  795144    29254    29150   58678  830011
>   55545   833914   221323   914182   952916   56120    60014
>            32768   16384  754641  819797    44183    45718   67293  785047
>   65078   807273   132136  1240414  1267713   56585    56167
>            65536      64  663333  656588    32211    31925   17324  202043
>   15477   215981    15496   592126   597188   86431    93969
>            65536     128  716218  791603    92035    95614   40330  330127
>   39295   346542    39469   697385   714453  105094    98327
>            65536     256  769283  798701   110285   110085   38404  535730
>   39939   585023    52402   741206   749899   98671   100106
>            65536     512  728151  814516   112550   108873   32284  690164
>   38911   708244    50047   778669   765303   98333    92728
>            65536    1024  773504  827108    21542    20741   45901  775473
>   57282   807232    69359   770793   766736  101465   100905
>            65536    2048  767661  853446    21077    21260   59825  762711
>   71985   802488    87290   772402   775685   96832   103189
>            65536    4096  767669  828165    20681    21480   79798  829757
>   91755   838367  1379411   791248   783023  106433   112769
>            65536    8192  739291  784946    23100    22454   85945  785134
>   87081   828489   642088   816781   798275   93307    95318
>            65536   16384  736401  791669    26345    26384   93082  799356
>   91419   751766   385721   911919   884688  109668    96374
>           131072      64  658361  661211    79790    81016   19460  182084
>   19325   195428    19553   551809   554426   96581    81979
>           131072     128  759806  838994    51063    56976   33624  326699
>   36329   360593    35679   710602   716538   81725    51819
>           131072     256  787375  843763    54664    65031   33847  557817
>   35093   581552    45163   750234   758452   82704    83472
>           131072     512  777460  818049    87831    87110   27994  705621
>   31860   723071    38483   779489   793068   88281    86068
>           131072    1024  737158  782780    19005    18897   36614  735240
>   38900   812074    52149   746598   762694   74376    82534
>           131072    2048  777386  817330    24169    24693   55171  769662
>   56082   788848    55937   757716   763147   73508    82351
>           131072    4096  751757  792201    25283    25173   48455  813945
>   56557   837418    59578   749106   757939   67846    70964
>           131072    8192  743004  796673    20369    20327   88990  780925
>   95956   777211   951528   762755   739851   70850    69943
>           131072   16384  766023  823611    22333    22142  108337  796852
>  110848   777704   718954   809861   802822  115896    82903
>           262144      64  655769  695392    45973    45668   19083  209417
>   18768   207272    19133   601329   599267  107705    87261
>           262144     128  750052  798999    72711    74713   37329  339480
>   36854   365587    37187   710898   714372  108400    78806
>           262144     256  788355  815575    80897    75141   37080  549762
>   37521   574393    51731   780429   751131   97395    73842
>           262144     512  714994  736649    80224    80195   32986  708196
>   36048   628508    44043   767699   781863  123267    83558
>           262144    1024  793541  800826    20200    20428   45599  752027
>   48232   798229    57011   715643   763691   76741    73211
>           262144    2048  699395  782935    20227    20519   65785  779219
>   80486   803403    93381   650054   712171  115611   108457
>           262144    4096  629531  766261    20544    21563   83107  819917
>   93008   844103    90625   741879   754931   94114    94475
>           262144    8192  731695  804112    21158    21589   82987  787869
>   84858   799483    95127   731822   737397   95950    95160
>           262144   16384  703376  802552    21940    22371   87748  777024
>   90349   779626  1018679   765278   619969   96051    92734
>           524288      64  649937  710342    73778    70893   19734  207179
>   19367   199709    19493   585910   581340   91902    91443
>           524288     128  739270  805153    91507    90395   38547  319049
>   37988   358516    38123   712602   711814   92463    90851
>           524288     256  751279  802496    89481    91022   38869  544097
>   38244   569142    56130   756367   752466   92677    92875
>           524288     512  758393  819788    58085    73437   29030  700120
>   28767   739236    33925   770177   795879   58963    72689
>           524288    1024  778385  818368    18822    18639   38774  798827
>   50898   799492    55679   801849   800917   92778    82686
>           524288    2048  780594  834612    19936    19994   63147  769555
>   63007   802512    66322   772900   771314   86628    93485
>           524288    4096  769806  812570    20165    20137   79896  783615
>   71217    80835    68550    80166    73733  122559   116844
>           524288    8192  797504  887823   123824   124513   94520  844880
>   86441   710319    65077   636713   819868   87765    80121
>           524288   16384  788468  858216    92580   117354  109583  850179
>   63966   856965   121151   789252   805040  123810   119088
>
>
>
>
>
> On Feb 27, 2013, at 2:46 PM, Bryan Whitehead <driver at megahappy.net> wrote:
>
> How are you doing the read/write tests on the fuse/glusterfs mountpoint?
> Many small files will be slow because all the time is spent coordinating
> locks.
>
>
> On Wed, Feb 27, 2013 at 9:31 AM, Thomas Wakefield <twake at cola.iges.org>wrote:
>
>> Help please-
>>
>>
>> I am running 3.3.1 on Centos using a 10GB network.  I get reasonable
>> write speeds, although I think they could be faster.  But my read speeds
>> are REALLY slow.
>>
>> Executive summary:
>>
>> On gluster client-
>> Writes average about 700-800MB/s
>> Reads average about 70-80MB/s
>>
>> On server-
>> Writes average about 1-1.5GB/s
>> Reads average about 2-3GB/s
>>
>> Any thoughts?
>>
>>
>>
>> Here are some additional details:
>>
>> Nothing interesting in any of the log files, everything is very quite.
>> All servers had no other load, and all clients are performing the same
>> way.
>>
>>
>> Volume Name: shared
>> Type: Distribute
>> Volume ID: de11cc19-0085-41c3-881e-995cca244620
>> Status: Started
>> Number of Bricks: 26
>> Transport-type: tcp
>> Bricks:
>> Brick1: fs-disk2:/storage/disk2a
>> Brick2: fs-disk2:/storage/disk2b
>> Brick3: fs-disk2:/storage/disk2d
>> Brick4: fs-disk2:/storage/disk2e
>> Brick5: fs-disk2:/storage/disk2f
>> Brick6: fs-disk2:/storage/disk2g
>> Brick7: fs-disk2:/storage/disk2h
>> Brick8: fs-disk2:/storage/disk2i
>> Brick9: fs-disk2:/storage/disk2j
>> Brick10: fs-disk2:/storage/disk2k
>> Brick11: fs-disk2:/storage/disk2l
>> Brick12: fs-disk2:/storage/disk2m
>> Brick13: fs-disk2:/storage/disk2n
>> Brick14: fs-disk2:/storage/disk2o
>> Brick15: fs-disk2:/storage/disk2p
>> Brick16: fs-disk2:/storage/disk2q
>> Brick17: fs-disk2:/storage/disk2r
>> Brick18: fs-disk2:/storage/disk2s
>> Brick19: fs-disk2:/storage/disk2t
>> Brick20: fs-disk2:/storage/disk2u
>> Brick21: fs-disk2:/storage/disk2v
>> Brick22: fs-disk2:/storage/disk2w
>> Brick23: fs-disk2:/storage/disk2x
>> Brick24: fs-disk3:/storage/disk3a
>> Brick25: fs-disk3:/storage/disk3b
>> Brick26: fs-disk3:/storage/disk3c
>> Options Reconfigured:
>> performance.write-behind: on
>> performance.read-ahead: on
>> performance.io-cache: on
>> performance.stat-prefetch: on
>> performance.quick-read: on
>> cluster.min-free-disk: 500GB
>> nfs.disable: off
>>
>>
>> sysctl.conf settings for 10GBe
>> # increase TCP max buffer size settable using setsockopt()
>> net.core.rmem_max = 67108864
>> net.core.wmem_max = 67108864
>> # increase Linux autotuning TCP buffer limit
>> net.ipv4.tcp_rmem = 4096 87380 67108864
>> net.ipv4.tcp_wmem = 4096 65536 67108864
>> # increase the length of the processor input queue
>> net.core.netdev_max_backlog = 250000
>> # recommended default congestion control is htcp
>> net.ipv4.tcp_congestion_control=htcp
>> # recommended for hosts with jumbo frames enabled
>> net.ipv4.tcp_mtu_probing=1
>>
>>
>>
>>
>>
>>
>>  Thomas W.
>> Sr.  Systems Administrator COLA/IGES
>> twake at cola.iges.org
>> Affiliate Computer Scientist GMU
>>
>>
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/582add75/attachment-0001.html>

From twake at iges.org  Wed Feb 27 23:45:07 2013
From: twake at iges.org (Thomas Wakefield)
Date: Wed, 27 Feb 2013 18:45:07 -0500
Subject: [Gluster-users] Slow read performance
In-Reply-To: <CAA3XVTxO6icNKSKJ7M-wF4U6C9_ukivjhAD-e5RRHy726DigEQ@mail.gmail.com>
References: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
	<CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>
	<CA303739-FED2-4BE6-9F1B-674036C028CF@cola.iges.org>
	<CAA3XVTxO6icNKSKJ7M-wF4U6C9_ukivjhAD-e5RRHy726DigEQ@mail.gmail.com>
Message-ID: <6DEE3D45-41D1-41A3-8A4F-823C96069BCE@iges.org>

I also get the same performance running iozone for large file sizes, iozone -u 1 -r 512k -s 2G -I -F.

Large file IO is what I need the system to do.  I am just shocked at the huge difference between local IO and gluster client IO.  I know there should be some difference, but 10x is unacceptable.

-Tom



On Feb 27, 2013, at 5:31 PM, Bryan Whitehead <driver at megahappy.net> wrote:

> Every time you open/close a file or a directory you will have to wait for locks which take time. This is totally expected.
> 
> Why don't you share what you want to do? iozone benchmarks look like crap but serving qcow2 files to qemu works fantastic for me. What are you doing? Make a benchmark that does that. If you are going to have many files with a wide variety of sizes glusterfs/fuse might not be what you are looking for.
> 
> 
> On Wed, Feb 27, 2013 at 12:56 PM, Thomas Wakefield <twake at cola.iges.org> wrote:
> I have tested everything, small and large files.  I have used file sizes ranging from 128k up to multiple GB files.  All the reads are bad.
> 
> 
> 
> Here is a fairly exhaustive iozone auto test:
> 
>                                                             random  random    bkwd   record   stride                                   
>               KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread
>               64       4   40222   63492    26868    30060    1620   71037    1572    70570    31294    77096    72475   14736    13928
>               64       8   99207  116366    13591    13513    3214   97690    3155   109978    28920   152018   158480   18936    17625
>               64      16  230257  253766    25156    28713   10867  223732    8873   244297    54796   303383   312204   15062    13545
>               64      32  255943  234481  5735102  7100397   11897  318502   13681   347801    24214   695778   528618   25838    28094
>               64      64  214096  681644  6421025  7100397   27453  292156   28117   621657    27338   376062   512471   28569    32534
>              128       4   74329   75468    26428    41089    1131   72857    1118    66976     1597    73778    78343   13351    13026
>              128       8  100862  135170    24966    16734    2617  118966    2560   120406    39156   125121   146613   16177    16180
>              128      16  115114  253983    28212    17854    5307  246180    5431   229843    47335   255920   271173   27256    24445
>              128      32  256042  391360    39848    64258   11329  290230    9905   429563    38176   490380   463696   20917    19219
>              128      64  248573  592699  4557257  6812590   19583  452366   29263   603357    42967   814915   692017   76327    37604
>              128     128  921183  526444  5603747  5379161   45614  390222   65441   826202    41384   662962  1040839   78526    39023
>              256       4   76212   77337    40295    32125    1289   71866    1261    64645     1436    57309    53048   23073    29550
>              256       8  126922  141976    26237    25130    2566  128058    2565   138981     2985   125060   133603   22840    24955
>              256      16  242883  263636    41850    24371    4902  250009    5290   248792    89353   243821   247303   26965    26199
>              256      32  409074  439732    40101    39335   11953  436870   11209   430218    83743   409542   479390   30821    27750
>              256      64  259935  571502    64840    71847   22537  617161   23383   392047    91852   672010   802614   41673    53111
>              256     128  847597  812329   185517    83198   49383  708831   44668   794889    74267  1180188  1662639   54303    41018
>              256     256  481324  709299  5217259  5320671   44668  719277   40954   808050    41302   790209   771473   62224    35754
>              512       4   77667   75226    35102    29696    1337   66262    1451    67680     1413    69265    69142   42084    27897
>              512       8  134311  144341    30144    24646    2102  134143    2209   134699     2296   108110   128616   25104    29123
>              512      16  200085  248787    30235    25697    4196  247240    4179   256116     4768   250003   226436   32351    28455
>              512      32  330341  439805    26440    39284    8744  457611    8006   424168   125953   425935   448813   27660    26951
>              512      64  483906  733729    48747    41121   16032  555938   17424   587256   187343   366977   735740   41700    41548
>              512     128  836636  907717    69359    94921   42443  761031   36828   964378   123165   651383   695697   58368    44459
>              512     256  520879  860437   145534   135523   40267  847532   31585   663252    69696  1270846  1492545   48822    48092
>              512     512  782951  973118  3099691  2942541   42328  871966   46218   911184    49791   953248  1036527   52723    48347
>             1024       4   76218   69362    36431    28711    1137   66171    1174    68938     1125    70566    70845   34942    28914
>             1024       8  126045  140524    37836    15664    2698  126000    2557   125566     2567   110858   127255   26764    27945
>             1024      16  243398  261429    40238    23263    3987  246400    3882   260746     4093   236652   236874   31429    25076
>             1024      32  383109  422076    41731    41605    8277  473441    7775   415261     8588   394765   407306   40089    28537
>             1024      64  590145  619156    39623    53267   15051  722717   14624   753000   257294   597784   620946   38619    44073
>             1024     128 1077836 1124099    56192    64916   36851 1102176   37198  1082454   281548   829175   792604   47975    51913
>             1024     256  941918 1074331    72783    81450   26778 1099636   32395  1060013   183218  1024121   995171   44371    45448
>             1024     512  697483 1130312   100324   114682   48215 1041758   41480  1058967    90156   994020  1563622   56328    46370
>             1024    1024  931702 1087111  4609294  4199201   44191  949834   45594   970656    56674   933525  1075676   44876    46115
>             2048       4   71438   67066    58319    38913    1147   44147    1043    42916      967    66416    67205   45953    96750
>             2048       8  141926  134567    61101    55445    2596   77528    2564    80402     4258   124211   120747   53888   100337
>             2048      16  254344  255585    71550    74500    5410  139365    5201   141484     5171   205521   213113   67048    57304
>             2048      32  397833  411261    56676    80027   10440  260034   10126   230238    10814   391665   383379   79333    60877
>             2048      64  595167  687205    64262    87327   20772  456430   19960   477064    23190   540220   563096   86812    92565
>             2048     128  833585  933403   121926   118621   37700  690020   37575   733254   567449   712337   734006   92011   104934
>             2048     256  799003  949499   143688   125659   40871  892757   37977   880494   458281   836263   901375  131332   110237
>             2048     512  979936 1040724   120896   138013   54381  859783   48721   780491   279203  1068824  1087085   97886    98078
>             2048    1024  901754  987938    53352    53043   72727 1054522   68269   992275   181253  1309480  1524983  121600    95585
>             2048    2048  831890 1021540  4257067  3302797   75672  984203   80181   826209    94278   966920  1027159  111832   105921
>             4096       4   66195   67316    62171    74785    1328   28963    1329    26397     1223    71470    69317   55903    84915
>             4096       8  122221  120057    90537    60958    2598   47312    2468    59783     2640   128674   127872   41285    40422
>             4096      16  238321  239251    29336    32121    4153   89262    3986    96930     4608   229970   237108   55039    56983
>             4096      32  417110  421356    30974    50000    8382  156676    7886   153841     7900   359585   367288   26611    25952
>             4096      64  648008  668066    32193    29389   14830  273265   14822   282211    19653   581898   620798   51281    50218
>             4096     128  779422  848564    55594    60253   37108  451296   35908   491361    37567   738163   728059   67681    66440
>             4096     256  865623  886986    71368    63947   44255  645961   42689   719491   736707   819696   837641   57059    60347
>             4096     512  852099  889650    68870    73891   31185  845224   30259   830153   392334   910442   961983   60083    55558
>             4096    1024  710357  867810    29377    29522   49954  846640   43665   926298   213677   986226  1115445   55130    59205
>             4096    2048  826479  908420    43191    42075   59684  904022   58601   855664   115105  1418322  1524415   60548    66066
>             4096    4096  793351  855111  3232454  3673419   66018  861413   48833   847852    45914   852268   842075   42980    48374
>             8192       4   67340   69421    42198    31740     994   23251    1166    16813      837    73827    73126   25169    29610
>             8192       8  137150  125622    29131    36439    2051   44342    1988    48930     2315   134183   135367   31080    33573
>             8192      16  237366  220826    24810    26584    3576   88004    3769    78717     4289   233751   235355   23302    28742
>             8192      32  457447  454404    31594    27750    8141  142022    7846   143984     9322   353147   396188   34203    33265
>             8192      64  670645  655259    28630    23255   16669  237476   16965   244968    15607   590365   575320   49998    43305
>             8192     128  658676  760982    44197    47802   28693  379523   26614   378328    27184   720997   702038   51707    49733
>             8192     256  643370  698683    56233    63165   28846  543952   27745   576739    44014   701007   725534   59611    58985
>             8192     512  696884  776793    67258    52705   18711  698854   21004   694124   621695   784812   773331   43101    47659
>             8192    1024  729664  810451   15470    15875   31318  801490   38123   812944   301222   804323   832765   54308    53376
>             8192    2048  749217   68757    21914    22667   48971  783309   48132   782738   172848   907408   929324   51156    50565
>             8192    4096  707677  763960    32063    31928   47809  751692   49560   786339    93445  1046761  1297876   48037    51680
>             8192    8192  623817  746288  2815955  3137358   48722  741633   35428   753787    49626   803683   823800   48977    52895
>            16384       4   72372   73651    34471    30788     960   23610     903    22316      891    71445    71138   56451    55129
>            16384       8  137920  141704    50830    33857    1935   41934    2275    35588     3608   130757   137801   51621    48525
>            16384      16  245369  242460    41808    29770    3605   75682    4355    75315     4767   241100   239693   53263    30785
>            16384      32  448877  433956    31846    35010    7973  118181    8819   112703     8177   381734   391651   57749    63417
>            16384      64  710831  700712    66792    68864   20176  209806   19034   207852    21255   589503   601379  104567   105162
>            16384     128  836901  860867   104226   100373   40899  358865   40946   360562    39415   675968   691538   96086   105695
>            16384     256  798081  828146   107103   120433   39084  595325   39050   593110    56925   763466   797859  109645   113414
>            16384     512  810851  843931   113564   106202   35111  714831   46244   745947    53636   802902   760172  110492   100879
>            16384    1024  726399  820219    22106    22987   53087  749053   54781   777705  1075341   772686   809723  100349    96619
>            16384    2048  807772  856458    23920    23617   66320  829576   72105   740848   656379   864539   835446   93499   101714
>            16384    4096  797470  840596    27270    28132   88784  834938   83428   836809   342898   953274   969699   96499   100657
>            16384    8192  760063  812942    41022    40014  102020  812135  104063   820346   229870  1252409  1398306  100959   103793
>            16384   16384  743107  825054  3074035  3551062   97740  826175  103597   803926    94681   820474   829106   97168    92018
>            32768      64  713528  714585    71310    74296   21135  194177   21551   202998    21110   571030   595232   98598   103373
>            32768     128  766698  820615   103028   102269   39013  328409   40325   346403    41084   684604   702513  103887   100689
>            32768     256  766591  819138   107557   113878   42088  574878   41778   557989    55045   749922   754434  100964   104866
>            32768     512  774365  846915   104255   106094   35280  728957   25829   734316    26990   750614   800955   55013    63750
>            32768    1024  759113  806241    22033    22622   37322  761870   45003   766950    54519   755666   741339   70018    66255
>            32768    2048  797566  860344    22893    23859   49973  778872   53509   779539   796674   817078   797585   56750    57047
>            32768    4096  796693  798985    24267    25222   54410  781810   53053   802845   421421   832858   855811   56748    60507
>            32768    8192  741543  795144    29254    29150   58678  830011   55545   833914   221323   914182   952916   56120    60014
>            32768   16384  754641  819797    44183    45718   67293  785047   65078   807273   132136  1240414  1267713   56585    56167
>            65536      64  663333  656588    32211    31925   17324  202043   15477   215981    15496   592126   597188   86431    93969
>            65536     128  716218  791603    92035    95614   40330  330127   39295   346542    39469   697385   714453  105094    98327
>            65536     256  769283  798701   110285   110085   38404  535730   39939   585023    52402   741206   749899   98671   100106
>            65536     512  728151  814516   112550   108873   32284  690164   38911   708244    50047   778669   765303   98333    92728
>            65536    1024  773504  827108    21542    20741   45901  775473   57282   807232    69359   770793   766736  101465   100905
>            65536    2048  767661  853446    21077    21260   59825  762711   71985   802488    87290   772402   775685   96832   103189
>            65536    4096  767669  828165    20681    21480   79798  829757   91755   838367  1379411   791248   783023  106433   112769
>            65536    8192  739291  784946    23100    22454   85945  785134   87081   828489   642088   816781   798275   93307    95318
>            65536   16384  736401  791669    26345    26384   93082  799356   91419   751766   385721   911919   884688  109668    96374
>           131072      64  658361  661211    79790    81016   19460  182084   19325   195428    19553   551809   554426   96581    81979
>           131072     128  759806  838994    51063    56976   33624  326699   36329   360593    35679   710602   716538   81725    51819
>           131072     256  787375  843763    54664    65031   33847  557817   35093   581552    45163   750234   758452   82704    83472
>           131072     512  777460  818049    87831    87110   27994  705621   31860   723071    38483   779489   793068   88281    86068
>           131072    1024  737158  782780    19005    18897   36614  735240   38900   812074    52149   746598   762694   74376    82534
>           131072    2048  777386  817330    24169    24693   55171  769662   56082   788848    55937   757716   763147   73508    82351
>           131072    4096  751757  792201    25283    25173   48455  813945   56557   837418    59578   749106   757939   67846    70964
>           131072    8192  743004  796673    20369    20327   88990  780925   95956   777211   951528   762755   739851   70850    69943
>           131072   16384  766023  823611    22333    22142  108337  796852  110848   777704   718954   809861   802822  115896    82903
>           262144      64  655769  695392    45973    45668   19083  209417   18768   207272    19133   601329   599267  107705    87261
>           262144     128  750052  798999    72711    74713   37329  339480   36854   365587    37187   710898   714372  108400    78806
>           262144     256  788355  815575    80897    75141   37080  549762   37521   574393    51731   780429   751131   97395    73842
>           262144     512  714994  736649    80224    80195   32986  708196   36048   628508    44043   767699   781863  123267    83558
>           262144    1024  793541  800826    20200    20428   45599  752027   48232   798229    57011   715643   763691   76741    73211
>           262144    2048  699395  782935    20227    20519   65785  779219   80486   803403    93381   650054   712171  115611   108457
>           262144    4096  629531  766261    20544    21563   83107  819917   93008   844103    90625   741879   754931   94114    94475
>           262144    8192  731695  804112    21158    21589   82987  787869   84858   799483    95127   731822   737397   95950    95160
>           262144   16384  703376  802552    21940    22371   87748  777024   90349   779626  1018679   765278   619969   96051    92734
>           524288      64  649937  710342    73778    70893   19734  207179   19367   199709    19493   585910   581340   91902    91443
>           524288     128  739270  805153    91507    90395   38547  319049   37988   358516    38123   712602   711814   92463    90851
>           524288     256  751279  802496    89481    91022   38869  544097   38244   569142    56130   756367   752466   92677    92875
>           524288     512  758393  819788    58085    73437   29030  700120   28767   739236    33925   770177   795879   58963    72689
>           524288    1024  778385  818368    18822    18639   38774  798827   50898   799492    55679   801849   800917   92778    82686
>           524288    2048  780594  834612    19936    19994   63147  769555   63007   802512    66322   772900   771314   86628    93485
>           524288    4096  769806  812570    20165    20137   79896  783615   71217    80835    68550    80166    73733  122559   116844
>           524288    8192  797504  887823   123824   124513   94520  844880   86441   710319    65077   636713   819868   87765    80121
>           524288   16384  788468  858216    92580   117354  109583  850179   63966   856965   121151   789252   805040  123810   119088
> 
> 
> 
> 
> 
> On Feb 27, 2013, at 2:46 PM, Bryan Whitehead <driver at megahappy.net> wrote:
> 
>> How are you doing the read/write tests on the fuse/glusterfs mountpoint? Many small files will be slow because all the time is spent coordinating locks.
>> 
>> 
>> On Wed, Feb 27, 2013 at 9:31 AM, Thomas Wakefield <twake at cola.iges.org> wrote:
>> Help please-
>> 
>> 
>> I am running 3.3.1 on Centos using a 10GB network.  I get reasonable write speeds, although I think they could be faster.  But my read speeds are REALLY slow.
>> 
>> Executive summary:
>> 
>> On gluster client-
>> Writes average about 700-800MB/s
>> Reads average about 70-80MB/s
>> 
>> On server-
>> Writes average about 1-1.5GB/s
>> Reads average about 2-3GB/s
>> 
>> Any thoughts?
>> 
>> 
>> 
>> Here are some additional details:
>> 
>> Nothing interesting in any of the log files, everything is very quite.
>> All servers had no other load, and all clients are performing the same way.
>> 
>> 
>> Volume Name: shared
>> Type: Distribute
>> Volume ID: de11cc19-0085-41c3-881e-995cca244620
>> Status: Started
>> Number of Bricks: 26
>> Transport-type: tcp
>> Bricks:
>> Brick1: fs-disk2:/storage/disk2a
>> Brick2: fs-disk2:/storage/disk2b
>> Brick3: fs-disk2:/storage/disk2d
>> Brick4: fs-disk2:/storage/disk2e
>> Brick5: fs-disk2:/storage/disk2f
>> Brick6: fs-disk2:/storage/disk2g
>> Brick7: fs-disk2:/storage/disk2h
>> Brick8: fs-disk2:/storage/disk2i
>> Brick9: fs-disk2:/storage/disk2j
>> Brick10: fs-disk2:/storage/disk2k
>> Brick11: fs-disk2:/storage/disk2l
>> Brick12: fs-disk2:/storage/disk2m
>> Brick13: fs-disk2:/storage/disk2n
>> Brick14: fs-disk2:/storage/disk2o
>> Brick15: fs-disk2:/storage/disk2p
>> Brick16: fs-disk2:/storage/disk2q
>> Brick17: fs-disk2:/storage/disk2r
>> Brick18: fs-disk2:/storage/disk2s
>> Brick19: fs-disk2:/storage/disk2t
>> Brick20: fs-disk2:/storage/disk2u
>> Brick21: fs-disk2:/storage/disk2v
>> Brick22: fs-disk2:/storage/disk2w
>> Brick23: fs-disk2:/storage/disk2x
>> Brick24: fs-disk3:/storage/disk3a
>> Brick25: fs-disk3:/storage/disk3b
>> Brick26: fs-disk3:/storage/disk3c
>> Options Reconfigured:
>> performance.write-behind: on
>> performance.read-ahead: on
>> performance.io-cache: on
>> performance.stat-prefetch: on
>> performance.quick-read: on
>> cluster.min-free-disk: 500GB
>> nfs.disable: off
>> 
>> 
>> sysctl.conf settings for 10GBe
>> # increase TCP max buffer size settable using setsockopt()
>> net.core.rmem_max = 67108864 
>> net.core.wmem_max = 67108864 
>> # increase Linux autotuning TCP buffer limit
>> net.ipv4.tcp_rmem = 4096 87380 67108864
>> net.ipv4.tcp_wmem = 4096 65536 67108864
>> # increase the length of the processor input queue
>> net.core.netdev_max_backlog = 250000
>> # recommended default congestion control is htcp 
>> net.ipv4.tcp_congestion_control=htcp
>> # recommended for hosts with jumbo frames enabled
>> net.ipv4.tcp_mtu_probing=1
>> 
>> 
>> 
>> 
>> 
>> 
>> Thomas W.
>> Sr.  Systems Administrator COLA/IGES
>> twake at cola.iges.org
>> Affiliate Computer Scientist GMU
>> 
>> 
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> 
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/75d5497a/attachment-0001.html>

From driver at megahappy.net  Thu Feb 28 00:41:08 2013
From: driver at megahappy.net (Bryan Whitehead)
Date: Wed, 27 Feb 2013 16:41:08 -0800
Subject: [Gluster-users] Slow read performance
In-Reply-To: <6DEE3D45-41D1-41A3-8A4F-823C96069BCE@iges.org>
References: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
	<CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>
	<CA303739-FED2-4BE6-9F1B-674036C028CF@cola.iges.org>
	<CAA3XVTxO6icNKSKJ7M-wF4U6C9_ukivjhAD-e5RRHy726DigEQ@mail.gmail.com>
	<6DEE3D45-41D1-41A3-8A4F-823C96069BCE@iges.org>
Message-ID: <CAA3XVTyWB8msVX+Vt1+g4PTGsxicnG4QuWcF28t+_bSJ233K1A@mail.gmail.com>

Are your figures 700-800M*Byte*/sec? Because that is probably as fast as
your 10G nic cards are able to do. You can test that by trying to push a
large amount of data over nc or ftp.

Might want to try Infiniband. 40G cards are pretty routine.


On Wed, Feb 27, 2013 at 3:45 PM, Thomas Wakefield <twake at iges.org> wrote:

> I also get the same performance running iozone for large file
> sizes, iozone -u 1 -r 512k -s 2G -I -F.
>
> Large file IO is what I need the system to do.  I am just shocked at the
> huge difference between local IO and gluster client IO.  I know there
> should be some difference, but 10x is unacceptable.
>
> -Tom
>
>
>
> On Feb 27, 2013, at 5:31 PM, Bryan Whitehead <driver at megahappy.net> wrote:
>
> Every time you open/close a file or a directory you will have to wait for
> locks which take time. This is totally expected.
>
> Why don't you share what you want to do? iozone benchmarks look like crap
> but serving qcow2 files to qemu works fantastic for me. What are you doing?
> Make a benchmark that does that. If you are going to have many files with a
> wide variety of sizes glusterfs/fuse might not be what you are looking for.
>
>
> On Wed, Feb 27, 2013 at 12:56 PM, Thomas Wakefield <twake at cola.iges.org>wrote:
>
>> I have tested everything, small and large files.  I have used file sizes
>> ranging from 128k up to multiple GB files.  All the reads are bad.
>>
>>
>>
>> Here is a fairly exhaustive iozone auto test:
>>
>>                                                             random
>>  random    bkwd   record   stride
>>               KB  reclen   write rewrite    read    reread    read
>> write    read  rewrite     read   fwrite frewrite   fread  freread
>>               64       4   40222   63492    26868    30060    1620
>> 71037    1572    70570    31294    77096    72475   14736    13928
>>               64       8   99207  116366    13591    13513    3214
>> 97690    3155   109978    28920   152018   158480   18936    17625
>>               64      16  230257  253766    25156    28713   10867
>>  223732    8873   244297    54796   303383   312204   15062    13545
>>               64      32  255943  234481  5735102  7100397   11897
>>  318502   13681   347801    24214   695778   528618   25838    28094
>>               64      64  214096  681644  6421025  7100397   27453
>>  292156   28117   621657    27338   376062   512471   28569    32534
>>              128       4   74329   75468    26428    41089    1131
>> 72857    1118    66976     1597    73778    78343   13351    13026
>>              128       8  100862  135170    24966    16734    2617
>>  118966    2560   120406    39156   125121   146613   16177    16180
>>              128      16  115114  253983    28212    17854    5307
>>  246180    5431   229843    47335   255920   271173   27256    24445
>>              128      32  256042  391360    39848    64258   11329
>>  290230    9905   429563    38176   490380   463696   20917    19219
>>              128      64  248573  592699  4557257  6812590   19583
>>  452366   29263   603357    42967   814915   692017   76327    37604
>>              128     128  921183  526444  5603747  5379161   45614
>>  390222   65441   826202    41384   662962  1040839   78526    39023
>>              256       4   76212   77337    40295    32125    1289
>> 71866    1261    64645     1436    57309    53048   23073    29550
>>              256       8  126922  141976    26237    25130    2566
>>  128058    2565   138981     2985   125060   133603   22840    24955
>>              256      16  242883  263636    41850    24371    4902
>>  250009    5290   248792    89353   243821   247303   26965    26199
>>              256      32  409074  439732    40101    39335   11953
>>  436870   11209   430218    83743   409542   479390   30821    27750
>>              256      64  259935  571502    64840    71847   22537
>>  617161   23383   392047    91852   672010   802614   41673    53111
>>              256     128  847597  812329   185517    83198   49383
>>  708831   44668   794889    74267  1180188  1662639   54303    41018
>>              256     256  481324  709299  5217259  5320671   44668
>>  719277   40954   808050    41302   790209   771473   62224    35754
>>              512       4   77667   75226    35102    29696    1337
>> 66262    1451    67680     1413    69265    69142   42084    27897
>>              512       8  134311  144341    30144    24646    2102
>>  134143    2209   134699     2296   108110   128616   25104    29123
>>              512      16  200085  248787    30235    25697    4196
>>  247240    4179   256116     4768   250003   226436   32351    28455
>>              512      32  330341  439805    26440    39284    8744
>>  457611    8006   424168   125953   425935   448813   27660    26951
>>              512      64  483906  733729    48747    41121   16032
>>  555938   17424   587256   187343   366977   735740   41700    41548
>>              512     128  836636  907717    69359    94921   42443
>>  761031   36828   964378   123165   651383   695697   58368    44459
>>              512     256  520879  860437   145534   135523   40267
>>  847532   31585   663252    69696  1270846  1492545   48822    48092
>>              512     512  782951  973118  3099691  2942541   42328
>>  871966   46218   911184    49791   953248  1036527   52723    48347
>>             1024       4   76218   69362    36431    28711    1137
>> 66171    1174    68938     1125    70566    70845   34942    28914
>>             1024       8  126045  140524    37836    15664    2698
>>  126000    2557   125566     2567   110858   127255   26764    27945
>>             1024      16  243398  261429    40238    23263    3987
>>  246400    3882   260746     4093   236652   236874   31429    25076
>>             1024      32  383109  422076    41731    41605    8277
>>  473441    7775   415261     8588   394765   407306   40089    28537
>>             1024      64  590145  619156    39623    53267   15051
>>  722717   14624   753000   257294   597784   620946   38619    44073
>>             1024     128 1077836 1124099    56192    64916   36851
>> 1102176   37198  1082454   281548   829175   792604   47975    51913
>>             1024     256  941918 1074331    72783    81450   26778
>> 1099636   32395  1060013   183218  1024121   995171   44371    45448
>>             1024     512  697483 1130312   100324   114682   48215
>> 1041758   41480  1058967    90156   994020  1563622   56328    46370
>>             1024    1024  931702 1087111  4609294  4199201   44191
>>  949834   45594   970656    56674   933525  1075676   44876    46115
>>             2048       4   71438   67066    58319    38913    1147
>> 44147    1043    42916      967    66416    67205   45953    96750
>>             2048       8  141926  134567    61101    55445    2596
>> 77528    2564    80402     4258   124211   120747   53888   100337
>>             2048      16  254344  255585    71550    74500    5410
>>  139365    5201   141484     5171   205521   213113   67048    57304
>>             2048      32  397833  411261    56676    80027   10440
>>  260034   10126   230238    10814   391665   383379   79333    60877
>>             2048      64  595167  687205    64262    87327   20772
>>  456430   19960   477064    23190   540220   563096   86812    92565
>>             2048     128  833585  933403   121926   118621   37700
>>  690020   37575   733254   567449   712337   734006   92011   104934
>>             2048     256  799003  949499   143688   125659   40871
>>  892757   37977   880494   458281   836263   901375  131332   110237
>>             2048     512  979936 1040724   120896   138013   54381
>>  859783   48721   780491   279203  1068824  1087085   97886    98078
>>             2048    1024  901754  987938    53352    53043   72727
>> 1054522   68269   992275   181253  1309480  1524983  121600    95585
>>             2048    2048  831890 1021540  4257067  3302797   75672
>>  984203   80181   826209    94278   966920  1027159  111832   105921
>>             4096       4   66195   67316    62171    74785    1328
>> 28963    1329    26397     1223    71470    69317   55903    84915
>>             4096       8  122221  120057    90537    60958    2598
>> 47312    2468    59783     2640   128674   127872   41285    40422
>>             4096      16  238321  239251    29336    32121    4153
>> 89262    3986    96930     4608   229970   237108   55039    56983
>>             4096      32  417110  421356    30974    50000    8382
>>  156676    7886   153841     7900   359585   367288   26611    25952
>>             4096      64  648008  668066    32193    29389   14830
>>  273265   14822   282211    19653   581898   620798   51281    50218
>>             4096     128  779422  848564    55594    60253   37108
>>  451296   35908   491361    37567   738163   728059   67681    66440
>>             4096     256  865623  886986    71368    63947   44255
>>  645961   42689   719491   736707   819696   837641   57059    60347
>>             4096     512  852099  889650    68870    73891   31185
>>  845224   30259   830153   392334   910442   961983   60083    55558
>>             4096    1024  710357  867810    29377    29522   49954
>>  846640   43665   926298   213677   986226  1115445   55130    59205
>>             4096    2048  826479  908420    43191    42075   59684
>>  904022   58601   855664   115105  1418322  1524415   60548    66066
>>             4096    4096  793351  855111  3232454  3673419   66018
>>  861413   48833   847852    45914   852268   842075   42980    48374
>>             8192       4   67340   69421    42198    31740     994
>> 23251    1166    16813      837    73827    73126   25169    29610
>>             8192       8  137150  125622    29131    36439    2051
>> 44342    1988    48930     2315   134183   135367   31080    33573
>>             8192      16  237366  220826    24810    26584    3576
>> 88004    3769    78717     4289   233751   235355   23302    28742
>>             8192      32  457447  454404    31594    27750    8141
>>  142022    7846   143984     9322   353147   396188   34203    33265
>>             8192      64  670645  655259    28630    23255   16669
>>  237476   16965   244968    15607   590365   575320   49998    43305
>>             8192     128  658676  760982    44197    47802   28693
>>  379523   26614   378328    27184   720997   702038   51707    49733
>>             8192     256  643370  698683    56233    63165   28846
>>  543952   27745   576739    44014   701007   725534   59611    58985
>>             8192     512  696884  776793    67258    52705   18711
>>  698854   21004   694124   621695   784812   773331   43101    47659
>>             8192    1024  729664  810451   15470    15875   31318  801490
>>   38123   812944   301222   804323   832765   54308    53376
>>             8192    2048  749217   68757    21914    22667   48971
>>  783309   48132   782738   172848   907408   929324   51156    50565
>>             8192    4096  707677  763960    32063    31928   47809
>>  751692   49560   786339    93445  1046761  1297876   48037    51680
>>             8192    8192  623817  746288  2815955  3137358   48722
>>  741633   35428   753787    49626   803683   823800   48977    52895
>>            16384       4   72372   73651    34471    30788     960
>> 23610     903    22316      891    71445    71138   56451    55129
>>            16384       8  137920  141704    50830    33857    1935
>> 41934    2275    35588     3608   130757   137801   51621    48525
>>            16384      16  245369  242460    41808    29770    3605
>> 75682    4355    75315     4767   241100   239693   53263    30785
>>            16384      32  448877  433956    31846    35010    7973
>>  118181    8819   112703     8177   381734   391651   57749    63417
>>            16384      64  710831  700712    66792    68864   20176
>>  209806   19034   207852    21255   589503   601379  104567   105162
>>            16384     128  836901  860867   104226   100373   40899
>>  358865   40946   360562    39415   675968   691538   96086   105695
>>            16384     256  798081  828146   107103   120433   39084
>>  595325   39050   593110    56925   763466   797859  109645   113414
>>            16384     512  810851  843931   113564   106202   35111
>>  714831   46244   745947    53636   802902   760172  110492   100879
>>            16384    1024  726399  820219    22106    22987   53087
>>  749053   54781   777705  1075341   772686   809723  100349    96619
>>            16384    2048  807772  856458    23920    23617   66320
>>  829576   72105   740848   656379   864539   835446   93499   101714
>>            16384    4096  797470  840596    27270    28132   88784
>>  834938   83428   836809   342898   953274   969699   96499   100657
>>            16384    8192  760063  812942    41022    40014  102020
>>  812135  104063   820346   229870  1252409  1398306  100959   103793
>>            16384   16384  743107  825054  3074035  3551062   97740
>>  826175  103597   803926    94681   820474   829106   97168    92018
>>            32768      64  713528  714585    71310    74296   21135
>>  194177   21551   202998    21110   571030   595232   98598   103373
>>            32768     128  766698  820615   103028   102269   39013
>>  328409   40325   346403    41084   684604   702513  103887   100689
>>            32768     256  766591  819138   107557   113878   42088
>>  574878   41778   557989    55045   749922   754434  100964   104866
>>            32768     512  774365  846915   104255   106094   35280
>>  728957   25829   734316    26990   750614   800955   55013    63750
>>            32768    1024  759113  806241    22033    22622   37322
>>  761870   45003   766950    54519   755666   741339   70018    66255
>>            32768    2048  797566  860344    22893    23859   49973
>>  778872   53509   779539   796674   817078   797585   56750    57047
>>            32768    4096  796693  798985    24267    25222   54410
>>  781810   53053   802845   421421   832858   855811   56748    60507
>>            32768    8192  741543  795144    29254    29150   58678
>>  830011   55545   833914   221323   914182   952916   56120    60014
>>            32768   16384  754641  819797    44183    45718   67293
>>  785047   65078   807273   132136  1240414  1267713   56585    56167
>>            65536      64  663333  656588    32211    31925   17324
>>  202043   15477   215981    15496   592126   597188   86431    93969
>>            65536     128  716218  791603    92035    95614   40330
>>  330127   39295   346542    39469   697385   714453  105094    98327
>>            65536     256  769283  798701   110285   110085   38404
>>  535730   39939   585023    52402   741206   749899   98671   100106
>>            65536     512  728151  814516   112550   108873   32284
>>  690164   38911   708244    50047   778669   765303   98333    92728
>>            65536    1024  773504  827108    21542    20741   45901
>>  775473   57282   807232    69359   770793   766736  101465   100905
>>            65536    2048  767661  853446    21077    21260   59825
>>  762711   71985   802488    87290   772402   775685   96832   103189
>>            65536    4096  767669  828165    20681    21480   79798
>>  829757   91755   838367  1379411   791248   783023  106433   112769
>>            65536    8192  739291  784946    23100    22454   85945
>>  785134   87081   828489   642088   816781   798275   93307    95318
>>            65536   16384  736401  791669    26345    26384   93082
>>  799356   91419   751766   385721   911919   884688  109668    96374
>>           131072      64  658361  661211    79790    81016   19460
>>  182084   19325   195428    19553   551809   554426   96581    81979
>>           131072     128  759806  838994    51063    56976   33624
>>  326699   36329   360593    35679   710602   716538   81725    51819
>>           131072     256  787375  843763    54664    65031   33847
>>  557817   35093   581552    45163   750234   758452   82704    83472
>>           131072     512  777460  818049    87831    87110   27994
>>  705621   31860   723071    38483   779489   793068   88281    86068
>>           131072    1024  737158  782780    19005    18897   36614
>>  735240   38900   812074    52149   746598   762694   74376    82534
>>           131072    2048  777386  817330    24169    24693   55171
>>  769662   56082   788848    55937   757716   763147   73508    82351
>>           131072    4096  751757  792201    25283    25173   48455
>>  813945   56557   837418    59578   749106   757939   67846    70964
>>           131072    8192  743004  796673    20369    20327   88990
>>  780925   95956   777211   951528   762755   739851   70850    69943
>>           131072   16384  766023  823611    22333    22142  108337
>>  796852  110848   777704   718954   809861   802822  115896    82903
>>           262144      64  655769  695392    45973    45668   19083
>>  209417   18768   207272    19133   601329   599267  107705    87261
>>           262144     128  750052  798999    72711    74713   37329
>>  339480   36854   365587    37187   710898   714372  108400    78806
>>           262144     256  788355  815575    80897    75141   37080
>>  549762   37521   574393    51731   780429   751131   97395    73842
>>           262144     512  714994  736649    80224    80195   32986
>>  708196   36048   628508    44043   767699   781863  123267    83558
>>           262144    1024  793541  800826    20200    20428   45599
>>  752027   48232   798229    57011   715643   763691   76741    73211
>>           262144    2048  699395  782935    20227    20519   65785
>>  779219   80486   803403    93381   650054   712171  115611   108457
>>           262144    4096  629531  766261    20544    21563   83107
>>  819917   93008   844103    90625   741879   754931   94114    94475
>>           262144    8192  731695  804112    21158    21589   82987
>>  787869   84858   799483    95127   731822   737397   95950    95160
>>           262144   16384  703376  802552    21940    22371   87748
>>  777024   90349   779626  1018679   765278   619969   96051    92734
>>           524288      64  649937  710342    73778    70893   19734
>>  207179   19367   199709    19493   585910   581340   91902    91443
>>           524288     128  739270  805153    91507    90395   38547
>>  319049   37988   358516    38123   712602   711814   92463    90851
>>           524288     256  751279  802496    89481    91022   38869
>>  544097   38244   569142    56130   756367   752466   92677    92875
>>           524288     512  758393  819788    58085    73437   29030
>>  700120   28767   739236    33925   770177   795879   58963    72689
>>           524288    1024  778385  818368    18822    18639   38774
>>  798827   50898   799492    55679   801849   800917   92778    82686
>>           524288    2048  780594  834612    19936    19994   63147
>>  769555   63007   802512    66322   772900   771314   86628    93485
>>           524288    4096  769806  812570    20165    20137   79896
>>  783615   71217    80835    68550    80166    73733  122559   116844
>>           524288    8192  797504  887823   123824   124513   94520
>>  844880   86441   710319    65077   636713   819868   87765    80121
>>           524288   16384  788468  858216    92580   117354  109583
>>  850179   63966   856965   121151   789252   805040  123810   119088
>>
>>
>>
>>
>>
>> On Feb 27, 2013, at 2:46 PM, Bryan Whitehead <driver at megahappy.net>
>> wrote:
>>
>> How are you doing the read/write tests on the fuse/glusterfs mountpoint?
>> Many small files will be slow because all the time is spent coordinating
>> locks.
>>
>>
>> On Wed, Feb 27, 2013 at 9:31 AM, Thomas Wakefield <twake at cola.iges.org>wrote:
>>
>>> Help please-
>>>
>>>
>>> I am running 3.3.1 on Centos using a 10GB network.  I get reasonable
>>> write speeds, although I think they could be faster.  But my read speeds
>>> are REALLY slow.
>>>
>>> Executive summary:
>>>
>>> On gluster client-
>>> Writes average about 700-800MB/s
>>> Reads average about 70-80MB/s
>>>
>>> On server-
>>> Writes average about 1-1.5GB/s
>>> Reads average about 2-3GB/s
>>>
>>> Any thoughts?
>>>
>>>
>>>
>>> Here are some additional details:
>>>
>>> Nothing interesting in any of the log files, everything is very quite.
>>> All servers had no other load, and all clients are performing the same
>>> way.
>>>
>>>
>>> Volume Name: shared
>>> Type: Distribute
>>> Volume ID: de11cc19-0085-41c3-881e-995cca244620
>>> Status: Started
>>> Number of Bricks: 26
>>> Transport-type: tcp
>>> Bricks:
>>> Brick1: fs-disk2:/storage/disk2a
>>> Brick2: fs-disk2:/storage/disk2b
>>> Brick3: fs-disk2:/storage/disk2d
>>> Brick4: fs-disk2:/storage/disk2e
>>> Brick5: fs-disk2:/storage/disk2f
>>> Brick6: fs-disk2:/storage/disk2g
>>> Brick7: fs-disk2:/storage/disk2h
>>> Brick8: fs-disk2:/storage/disk2i
>>> Brick9: fs-disk2:/storage/disk2j
>>> Brick10: fs-disk2:/storage/disk2k
>>> Brick11: fs-disk2:/storage/disk2l
>>> Brick12: fs-disk2:/storage/disk2m
>>> Brick13: fs-disk2:/storage/disk2n
>>> Brick14: fs-disk2:/storage/disk2o
>>>  Brick15: fs-disk2:/storage/disk2p
>>> Brick16: fs-disk2:/storage/disk2q
>>> Brick17: fs-disk2:/storage/disk2r
>>> Brick18: fs-disk2:/storage/disk2s
>>> Brick19: fs-disk2:/storage/disk2t
>>> Brick20: fs-disk2:/storage/disk2u
>>> Brick21: fs-disk2:/storage/disk2v
>>> Brick22: fs-disk2:/storage/disk2w
>>> Brick23: fs-disk2:/storage/disk2x
>>> Brick24: fs-disk3:/storage/disk3a
>>> Brick25: fs-disk3:/storage/disk3b
>>>  Brick26: fs-disk3:/storage/disk3c
>>> Options Reconfigured:
>>> performance.write-behind: on
>>> performance.read-ahead: on
>>> performance.io-cache: on
>>> performance.stat-prefetch: on
>>> performance.quick-read: on
>>> cluster.min-free-disk: 500GB
>>> nfs.disable: off
>>>
>>>
>>> sysctl.conf settings for 10GBe
>>> # increase TCP max buffer size settable using setsockopt()
>>> net.core.rmem_max = 67108864
>>> net.core.wmem_max = 67108864
>>> # increase Linux autotuning TCP buffer limit
>>> net.ipv4.tcp_rmem = 4096 87380 67108864
>>> net.ipv4.tcp_wmem = 4096 65536 67108864
>>> # increase the length of the processor input queue
>>> net.core.netdev_max_backlog = 250000
>>> # recommended default congestion control is htcp
>>> net.ipv4.tcp_congestion_control=htcp
>>> # recommended for hosts with jumbo frames enabled
>>> net.ipv4.tcp_mtu_probing=1
>>>
>>>
>>>
>>>
>>>
>>>
>>>  Thomas W.
>>> Sr.  Systems Administrator COLA/IGES
>>> twake at cola.iges.org
>>> Affiliate Computer Scientist GMU
>>>
>>>
>>> _______________________________________________
>>> Gluster-users mailing list
>>> Gluster-users at gluster.org
>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>>
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/32869bda/attachment-0001.html>

From twake at iges.org  Thu Feb 28 02:29:54 2013
From: twake at iges.org (Thomas Wakefield)
Date: Wed, 27 Feb 2013 21:29:54 -0500
Subject: [Gluster-users] Slow read performance
In-Reply-To: <CAA3XVTyWB8msVX+Vt1+g4PTGsxicnG4QuWcF28t+_bSJ233K1A@mail.gmail.com>
References: <C9F612F7-3311-4C98-9AB1-3D93F7D7FDD6@cola.iges.org>
	<CAA3XVTyB_RpRrYGYnd2LJ_yvKdLSLb7hsd_6tqR5DO77671K8A@mail.gmail.com>
	<CA303739-FED2-4BE6-9F1B-674036C028CF@cola.iges.org>
	<CAA3XVTxO6icNKSKJ7M-wF4U6C9_ukivjhAD-e5RRHy726DigEQ@mail.gmail.com>
	<6DEE3D45-41D1-41A3-8A4F-823C96069BCE@iges.org>
	<CAA3XVTyWB8msVX+Vt1+g4PTGsxicnG4QuWcF28t+_bSJ233K1A@mail.gmail.com>
Message-ID: <8A62055F-986A-4418-B7E1-8B9D0552E9A9@iges.org>

Bryan-

Yes I can write at 700-800MBytes/sec, but i can only read at 70-80 MBytes/sec.  I would be very happy if I could get it to read at the same speed it can write at.  And the 70-80 is sequential, not random for reads, same exact test commands on the disk server are in the 2+GB/s range, so I know the disk server can do it.

-Tom


On Feb 27, 2013, at 7:41 PM, Bryan Whitehead <driver at megahappy.net> wrote:

> Are your figures 700-800MByte/sec? Because that is probably as fast as your 10G nic cards are able to do. You can test that by trying to push a large amount of data over nc or ftp.
> 
> Might want to try Infiniband. 40G cards are pretty routine.
> 
> 
> On Wed, Feb 27, 2013 at 3:45 PM, Thomas Wakefield <twake at iges.org> wrote:
> I also get the same performance running iozone for large file sizes, iozone -u 1 -r 512k -s 2G -I -F.
> 
> Large file IO is what I need the system to do.  I am just shocked at the huge difference between local IO and gluster client IO.  I know there should be some difference, but 10x is unacceptable.
> 
> -Tom
> 
> 
> 
> On Feb 27, 2013, at 5:31 PM, Bryan Whitehead <driver at megahappy.net> wrote:
> 
>> Every time you open/close a file or a directory you will have to wait for locks which take time. This is totally expected.
>> 
>> Why don't you share what you want to do? iozone benchmarks look like crap but serving qcow2 files to qemu works fantastic for me. What are you doing? Make a benchmark that does that. If you are going to have many files with a wide variety of sizes glusterfs/fuse might not be what you are looking for.
>> 
>> 
>> On Wed, Feb 27, 2013 at 12:56 PM, Thomas Wakefield <twake at cola.iges.org> wrote:
>> I have tested everything, small and large files.  I have used file sizes ranging from 128k up to multiple GB files.  All the reads are bad.
>> 
>> 
>> 
>> Here is a fairly exhaustive iozone auto test:
>> 
>>                                                             random  random    bkwd   record   stride                                   
>>               KB  reclen   write rewrite    read    reread    read   write    read  rewrite     read   fwrite frewrite   fread  freread
>>               64       4   40222   63492    26868    30060    1620   71037    1572    70570    31294    77096    72475   14736    13928
>>               64       8   99207  116366    13591    13513    3214   97690    3155   109978    28920   152018   158480   18936    17625
>>               64      16  230257  253766    25156    28713   10867  223732    8873   244297    54796   303383   312204   15062    13545
>>               64      32  255943  234481  5735102  7100397   11897  318502   13681   347801    24214   695778   528618   25838    28094
>>               64      64  214096  681644  6421025  7100397   27453  292156   28117   621657    27338   376062   512471   28569    32534
>>              128       4   74329   75468    26428    41089    1131   72857    1118    66976     1597    73778    78343   13351    13026
>>              128       8  100862  135170    24966    16734    2617  118966    2560   120406    39156   125121   146613   16177    16180
>>              128      16  115114  253983    28212    17854    5307  246180    5431   229843    47335   255920   271173   27256    24445
>>              128      32  256042  391360    39848    64258   11329  290230    9905   429563    38176   490380   463696   20917    19219
>>              128      64  248573  592699  4557257  6812590   19583  452366   29263   603357    42967   814915   692017   76327    37604
>>              128     128  921183  526444  5603747  5379161   45614  390222   65441   826202    41384   662962  1040839   78526    39023
>>              256       4   76212   77337    40295    32125    1289   71866    1261    64645     1436    57309    53048   23073    29550
>>              256       8  126922  141976    26237    25130    2566  128058    2565   138981     2985   125060   133603   22840    24955
>>              256      16  242883  263636    41850    24371    4902  250009    5290   248792    89353   243821   247303   26965    26199
>>              256      32  409074  439732    40101    39335   11953  436870   11209   430218    83743   409542   479390   30821    27750
>>              256      64  259935  571502    64840    71847   22537  617161   23383   392047    91852   672010   802614   41673    53111
>>              256     128  847597  812329   185517    83198   49383  708831   44668   794889    74267  1180188  1662639   54303    41018
>>              256     256  481324  709299  5217259  5320671   44668  719277   40954   808050    41302   790209   771473   62224    35754
>>              512       4   77667   75226    35102    29696    1337   66262    1451    67680     1413    69265    69142   42084    27897
>>              512       8  134311  144341    30144    24646    2102  134143    2209   134699     2296   108110   128616   25104    29123
>>              512      16  200085  248787    30235    25697    4196  247240    4179   256116     4768   250003   226436   32351    28455
>>              512      32  330341  439805    26440    39284    8744  457611    8006   424168   125953   425935   448813   27660    26951
>>              512      64  483906  733729    48747    41121   16032  555938   17424   587256   187343   366977   735740   41700    41548
>>              512     128  836636  907717    69359    94921   42443  761031   36828   964378   123165   651383   695697   58368    44459
>>              512     256  520879  860437   145534   135523   40267  847532   31585   663252    69696  1270846  1492545   48822    48092
>>              512     512  782951  973118  3099691  2942541   42328  871966   46218   911184    49791   953248  1036527   52723    48347
>>             1024       4   76218   69362    36431    28711    1137   66171    1174    68938     1125    70566    70845   34942    28914
>>             1024       8  126045  140524    37836    15664    2698  126000    2557   125566     2567   110858   127255   26764    27945
>>             1024      16  243398  261429    40238    23263    3987  246400    3882   260746     4093   236652   236874   31429    25076
>>             1024      32  383109  422076    41731    41605    8277  473441    7775   415261     8588   394765   407306   40089    28537
>>             1024      64  590145  619156    39623    53267   15051  722717   14624   753000   257294   597784   620946   38619    44073
>>             1024     128 1077836 1124099    56192    64916   36851 1102176   37198  1082454   281548   829175   792604   47975    51913
>>             1024     256  941918 1074331    72783    81450   26778 1099636   32395  1060013   183218  1024121   995171   44371    45448
>>             1024     512  697483 1130312   100324   114682   48215 1041758   41480  1058967    90156   994020  1563622   56328    46370
>>             1024    1024  931702 1087111  4609294  4199201   44191  949834   45594   970656    56674   933525  1075676   44876    46115
>>             2048       4   71438   67066    58319    38913    1147   44147    1043    42916      967    66416    67205   45953    96750
>>             2048       8  141926  134567    61101    55445    2596   77528    2564    80402     4258   124211   120747   53888   100337
>>             2048      16  254344  255585    71550    74500    5410  139365    5201   141484     5171   205521   213113   67048    57304
>>             2048      32  397833  411261    56676    80027   10440  260034   10126   230238    10814   391665   383379   79333    60877
>>             2048      64  595167  687205    64262    87327   20772  456430   19960   477064    23190   540220   563096   86812    92565
>>             2048     128  833585  933403   121926   118621   37700  690020   37575   733254   567449   712337   734006   92011   104934
>>             2048     256  799003  949499   143688   125659   40871  892757   37977   880494   458281   836263   901375  131332   110237
>>             2048     512  979936 1040724   120896   138013   54381  859783   48721   780491   279203  1068824  1087085   97886    98078
>>             2048    1024  901754  987938    53352    53043   72727 1054522   68269   992275   181253  1309480  1524983  121600    95585
>>             2048    2048  831890 1021540  4257067  3302797   75672  984203   80181   826209    94278   966920  1027159  111832   105921
>>             4096       4   66195   67316    62171    74785    1328   28963    1329    26397     1223    71470    69317   55903    84915
>>             4096       8  122221  120057    90537    60958    2598   47312    2468    59783     2640   128674   127872   41285    40422
>>             4096      16  238321  239251    29336    32121    4153   89262    3986    96930     4608   229970   237108   55039    56983
>>             4096      32  417110  421356    30974    50000    8382  156676    7886   153841     7900   359585   367288   26611    25952
>>             4096      64  648008  668066    32193    29389   14830  273265   14822   282211    19653   581898   620798   51281    50218
>>             4096     128  779422  848564    55594    60253   37108  451296   35908   491361    37567   738163   728059   67681    66440
>>             4096     256  865623  886986    71368    63947   44255  645961   42689   719491   736707   819696   837641   57059    60347
>>             4096     512  852099  889650    68870    73891   31185  845224   30259   830153   392334   910442   961983   60083    55558
>>             4096    1024  710357  867810    29377    29522   49954  846640   43665   926298   213677   986226  1115445   55130    59205
>>             4096    2048  826479  908420    43191    42075   59684  904022   58601   855664   115105  1418322  1524415   60548    66066
>>             4096    4096  793351  855111  3232454  3673419   66018  861413   48833   847852    45914   852268   842075   42980    48374
>>             8192       4   67340   69421    42198    31740     994   23251    1166    16813      837    73827    73126   25169    29610
>>             8192       8  137150  125622    29131    36439    2051   44342    1988    48930     2315   134183   135367   31080    33573
>>             8192      16  237366  220826    24810    26584    3576   88004    3769    78717     4289   233751   235355   23302    28742
>>             8192      32  457447  454404    31594    27750    8141  142022    7846   143984     9322   353147   396188   34203    33265
>>             8192      64  670645  655259    28630    23255   16669  237476   16965   244968    15607   590365   575320   49998    43305
>>             8192     128  658676  760982    44197    47802   28693  379523   26614   378328    27184   720997   702038   51707    49733
>>             8192     256  643370  698683    56233    63165   28846  543952   27745   576739    44014   701007   725534   59611    58985
>>             8192     512  696884  776793    67258    52705   18711  698854   21004   694124   621695   784812   773331   43101    47659
>>             8192    1024  729664  810451   15470    15875   31318  801490   38123   812944   301222   804323   832765   54308    53376
>>             8192    2048  749217   68757    21914    22667   48971  783309   48132   782738   172848   907408   929324   51156    50565
>>             8192    4096  707677  763960    32063    31928   47809  751692   49560   786339    93445  1046761  1297876   48037    51680
>>             8192    8192  623817  746288  2815955  3137358   48722  741633   35428   753787    49626   803683   823800   48977    52895
>>            16384       4   72372   73651    34471    30788     960   23610     903    22316      891    71445    71138   56451    55129
>>            16384       8  137920  141704    50830    33857    1935   41934    2275    35588     3608   130757   137801   51621    48525
>>            16384      16  245369  242460    41808    29770    3605   75682    4355    75315     4767   241100   239693   53263    30785
>>            16384      32  448877  433956    31846    35010    7973  118181    8819   112703     8177   381734   391651   57749    63417
>>            16384      64  710831  700712    66792    68864   20176  209806   19034   207852    21255   589503   601379  104567   105162
>>            16384     128  836901  860867   104226   100373   40899  358865   40946   360562    39415   675968   691538   96086   105695
>>            16384     256  798081  828146   107103   120433   39084  595325   39050   593110    56925   763466   797859  109645   113414
>>            16384     512  810851  843931   113564   106202   35111  714831   46244   745947    53636   802902   760172  110492   100879
>>            16384    1024  726399  820219    22106    22987   53087  749053   54781   777705  1075341   772686   809723  100349    96619
>>            16384    2048  807772  856458    23920    23617   66320  829576   72105   740848   656379   864539   835446   93499   101714
>>            16384    4096  797470  840596    27270    28132   88784  834938   83428   836809   342898   953274   969699   96499   100657
>>            16384    8192  760063  812942    41022    40014  102020  812135  104063   820346   229870  1252409  1398306  100959   103793
>>            16384   16384  743107  825054  3074035  3551062   97740  826175  103597   803926    94681   820474   829106   97168    92018
>>            32768      64  713528  714585    71310    74296   21135  194177   21551   202998    21110   571030   595232   98598   103373
>>            32768     128  766698  820615   103028   102269   39013  328409   40325   346403    41084   684604   702513  103887   100689
>>            32768     256  766591  819138   107557   113878   42088  574878   41778   557989    55045   749922   754434  100964   104866
>>            32768     512  774365  846915   104255   106094   35280  728957   25829   734316    26990   750614   800955   55013    63750
>>            32768    1024  759113  806241    22033    22622   37322  761870   45003   766950    54519   755666   741339   70018    66255
>>            32768    2048  797566  860344    22893    23859   49973  778872   53509   779539   796674   817078   797585   56750    57047
>>            32768    4096  796693  798985    24267    25222   54410  781810   53053   802845   421421   832858   855811   56748    60507
>>            32768    8192  741543  795144    29254    29150   58678  830011   55545   833914   221323   914182   952916   56120    60014
>>            32768   16384  754641  819797    44183    45718   67293  785047   65078   807273   132136  1240414  1267713   56585    56167
>>            65536      64  663333  656588    32211    31925   17324  202043   15477   215981    15496   592126   597188   86431    93969
>>            65536     128  716218  791603    92035    95614   40330  330127   39295   346542    39469   697385   714453  105094    98327
>>            65536     256  769283  798701   110285   110085   38404  535730   39939   585023    52402   741206   749899   98671   100106
>>            65536     512  728151  814516   112550   108873   32284  690164   38911   708244    50047   778669   765303   98333    92728
>>            65536    1024  773504  827108    21542    20741   45901  775473   57282   807232    69359   770793   766736  101465   100905
>>            65536    2048  767661  853446    21077    21260   59825  762711   71985   802488    87290   772402   775685   96832   103189
>>            65536    4096  767669  828165    20681    21480   79798  829757   91755   838367  1379411   791248   783023  106433   112769
>>            65536    8192  739291  784946    23100    22454   85945  785134   87081   828489   642088   816781   798275   93307    95318
>>            65536   16384  736401  791669    26345    26384   93082  799356   91419   751766   385721   911919   884688  109668    96374
>>           131072      64  658361  661211    79790    81016   19460  182084   19325   195428    19553   551809   554426   96581    81979
>>           131072     128  759806  838994    51063    56976   33624  326699   36329   360593    35679   710602   716538   81725    51819
>>           131072     256  787375  843763    54664    65031   33847  557817   35093   581552    45163   750234   758452   82704    83472
>>           131072     512  777460  818049    87831    87110   27994  705621   31860   723071    38483   779489   793068   88281    86068
>>           131072    1024  737158  782780    19005    18897   36614  735240   38900   812074    52149   746598   762694   74376    82534
>>           131072    2048  777386  817330    24169    24693   55171  769662   56082   788848    55937   757716   763147   73508    82351
>>           131072    4096  751757  792201    25283    25173   48455  813945   56557   837418    59578   749106   757939   67846    70964
>>           131072    8192  743004  796673    20369    20327   88990  780925   95956   777211   951528   762755   739851   70850    69943
>>           131072   16384  766023  823611    22333    22142  108337  796852  110848   777704   718954   809861   802822  115896    82903
>>           262144      64  655769  695392    45973    45668   19083  209417   18768   207272    19133   601329   599267  107705    87261
>>           262144     128  750052  798999    72711    74713   37329  339480   36854   365587    37187   710898   714372  108400    78806
>>           262144     256  788355  815575    80897    75141   37080  549762   37521   574393    51731   780429   751131   97395    73842
>>           262144     512  714994  736649    80224    80195   32986  708196   36048   628508    44043   767699   781863  123267    83558
>>           262144    1024  793541  800826    20200    20428   45599  752027   48232   798229    57011   715643   763691   76741    73211
>>           262144    2048  699395  782935    20227    20519   65785  779219   80486   803403    93381   650054   712171  115611   108457
>>           262144    4096  629531  766261    20544    21563   83107  819917   93008   844103    90625   741879   754931   94114    94475
>>           262144    8192  731695  804112    21158    21589   82987  787869   84858   799483    95127   731822   737397   95950    95160
>>           262144   16384  703376  802552    21940    22371   87748  777024   90349   779626  1018679   765278   619969   96051    92734
>>           524288      64  649937  710342    73778    70893   19734  207179   19367   199709    19493   585910   581340   91902    91443
>>           524288     128  739270  805153    91507    90395   38547  319049   37988   358516    38123   712602   711814   92463    90851
>>           524288     256  751279  802496    89481    91022   38869  544097   38244   569142    56130   756367   752466   92677    92875
>>           524288     512  758393  819788    58085    73437   29030  700120   28767   739236    33925   770177   795879   58963    72689
>>           524288    1024  778385  818368    18822    18639   38774  798827   50898   799492    55679   801849   800917   92778    82686
>>           524288    2048  780594  834612    19936    19994   63147  769555   63007   802512    66322   772900   771314   86628    93485
>>           524288    4096  769806  812570    20165    20137   79896  783615   71217    80835    68550    80166    73733  122559   116844
>>           524288    8192  797504  887823   123824   124513   94520  844880   86441   710319    65077   636713   819868   87765    80121
>>           524288   16384  788468  858216    92580   117354  109583  850179   63966   856965   121151   789252   805040  123810   119088
>> 
>> 
>> 
>> 
>> 
>> On Feb 27, 2013, at 2:46 PM, Bryan Whitehead <driver at megahappy.net> wrote:
>> 
>>> How are you doing the read/write tests on the fuse/glusterfs mountpoint? Many small files will be slow because all the time is spent coordinating locks.
>>> 
>>> 
>>> On Wed, Feb 27, 2013 at 9:31 AM, Thomas Wakefield <twake at cola.iges.org> wrote:
>>> Help please-
>>> 
>>> 
>>> I am running 3.3.1 on Centos using a 10GB network.  I get reasonable write speeds, although I think they could be faster.  But my read speeds are REALLY slow.
>>> 
>>> Executive summary:
>>> 
>>> On gluster client-
>>> Writes average about 700-800MB/s
>>> Reads average about 70-80MB/s
>>> 
>>> On server-
>>> Writes average about 1-1.5GB/s
>>> Reads average about 2-3GB/s
>>> 
>>> Any thoughts?
>>> 
>>> 
>>> 
>>> Here are some additional details:
>>> 
>>> Nothing interesting in any of the log files, everything is very quite.
>>> All servers had no other load, and all clients are performing the same way.
>>> 
>>> 
>>> Volume Name: shared
>>> Type: Distribute
>>> Volume ID: de11cc19-0085-41c3-881e-995cca244620
>>> Status: Started
>>> Number of Bricks: 26
>>> Transport-type: tcp
>>> Bricks:
>>> Brick1: fs-disk2:/storage/disk2a
>>> Brick2: fs-disk2:/storage/disk2b
>>> Brick3: fs-disk2:/storage/disk2d
>>> Brick4: fs-disk2:/storage/disk2e
>>> Brick5: fs-disk2:/storage/disk2f
>>> Brick6: fs-disk2:/storage/disk2g
>>> Brick7: fs-disk2:/storage/disk2h
>>> Brick8: fs-disk2:/storage/disk2i
>>> Brick9: fs-disk2:/storage/disk2j
>>> Brick10: fs-disk2:/storage/disk2k
>>> Brick11: fs-disk2:/storage/disk2l
>>> Brick12: fs-disk2:/storage/disk2m
>>> Brick13: fs-disk2:/storage/disk2n
>>> Brick14: fs-disk2:/storage/disk2o
>>> Brick15: fs-disk2:/storage/disk2p
>>> Brick16: fs-disk2:/storage/disk2q
>>> Brick17: fs-disk2:/storage/disk2r
>>> Brick18: fs-disk2:/storage/disk2s
>>> Brick19: fs-disk2:/storage/disk2t
>>> Brick20: fs-disk2:/storage/disk2u
>>> Brick21: fs-disk2:/storage/disk2v
>>> Brick22: fs-disk2:/storage/disk2w
>>> Brick23: fs-disk2:/storage/disk2x
>>> Brick24: fs-disk3:/storage/disk3a
>>> Brick25: fs-disk3:/storage/disk3b
>>> Brick26: fs-disk3:/storage/disk3c
>>> Options Reconfigured:
>>> performance.write-behind: on
>>> performance.read-ahead: on
>>> performance.io-cache: on
>>> performance.stat-prefetch: on
>>> performance.quick-read: on
>>> cluster.min-free-disk: 500GB
>>> nfs.disable: off
>>> 
>>> 
>>> sysctl.conf settings for 10GBe
>>> # increase TCP max buffer size settable using setsockopt()
>>> net.core.rmem_max = 67108864 
>>> net.core.wmem_max = 67108864 
>>> # increase Linux autotuning TCP buffer limit
>>> net.ipv4.tcp_rmem = 4096 87380 67108864
>>> net.ipv4.tcp_wmem = 4096 65536 67108864
>>> # increase the length of the processor input queue
>>> net.core.netdev_max_backlog = 250000
>>> # recommended default congestion control is htcp 
>>> net.ipv4.tcp_congestion_control=htcp
>>> # recommended for hosts with jumbo frames enabled
>>> net.ipv4.tcp_mtu_probing=1
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> Thomas W.
>>> Sr.  Systems Administrator COLA/IGES
>>> twake at cola.iges.org
>>> Affiliate Computer Scientist GMU
>>> 
>>> 
>>> _______________________________________________
>>> Gluster-users mailing list
>>> Gluster-users at gluster.org
>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>> 
>> 
>> 
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130227/5d93b97f/attachment-0001.html>

From rahul51.s at tcs.com  Thu Feb 28 09:18:38 2013
From: rahul51.s at tcs.com (Rahul51 S)
Date: Thu, 28 Feb 2013 14:48:38 +0530
Subject: [Gluster-users] Issue in using volume sync command
Message-ID: <OFD8DA558C.A5A2F591-ON65257B20.003279CE-65257B20.00332542@tcs.com>

Hi All,

I am using gluster3.4.0 alpha on the two nodes communicating through TCP. 
I am facing an issue giving the volume sync command. Below is the error 
that I am getting 

[root at supernova /]# gluster volume sync 10.137.108.165 testvol
Sync volume may make data inaccessible while the sync is in progress. Do 
you want to continue? (y/n) y
volume sync: failed: Another transaction could be in progress. Please try 
again after sometime.

I tried giving the the command after some time, but couldn't succeed.
I stopped the volume and then gave the same command, but it again failed 
with the same error. 
Logs are not pointing to anything about another transaction going on as 
mentioned in the error.

Could you please shed some light on the usage of the volume sync command


Regards
Rahul Shrivastava
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130228/7757a30a/attachment.html>

From torbjorn at trollweb.no  Thu Feb 28 10:58:21 2013
From: torbjorn at trollweb.no (=?ISO-8859-1?Q?Torbj=F8rn_Thorsen?=)
Date: Thu, 28 Feb 2013 11:58:21 +0100
Subject: [Gluster-users] Performance in VM guests when hosting VM images
 on Gluster
In-Reply-To: <512E7099.9050308@redhat.com>
References: <CAD2iGhVgoVWDYGAP8KUYY5NE9-_wXd7Q00cZzjdjqkvD8cdacQ@mail.gmail.com>
	<512E7099.9050308@redhat.com>
Message-ID: <CAD2iGhWZyFEeW=6hrNgzB63P6P_V5BTW2zgSjk2=+PNqTWDsyQ@mail.gmail.com>

On Wed, Feb 27, 2013 at 9:46 PM, Brian Foster <bfoster at redhat.com> wrote:
> On 02/27/2013 10:14 AM, Torbj?rn Thorsen wrote:
>> I'm seeing less-than-stellar performance on my Gluster deployment when
>> hosting VM images on the FUSE mount.
<snip>
>> If we use a file on the gluster mount as backing for a loop device,
>> and do a sync write:
>> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
>> if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
>> 2097152000 bytes (2.1 GB) copied, 56.3729 s, 37.2 MB/s
>>
>
> What you might want to try is compare each case with gluster profiling
> enabled on your volume (e.g., run a 'gluster ... profile info' to clear
> the interval stats, run your test, run another 'profile info' and see
> how many write requests occurred, divide the amount of data transferred
> by the number of requests).
>
> Running similar tests on a couple random servers around here brings me
> from 70-80MB/s down to 10MB/s over loop. The profile data clearly shows
> that loop is breaking what were previously 128k (max) write requests
> into 4k requests. I don't know enough about the block layer to say why
> that occurs, but I'd be suspicious of the combination of the block
> interface on top of a filesystem (fuse) with synchronous request
> submission (no caching, writes are immediately submitted to the client
> fs). That said, I'm on an older kernel (or an older loop driver anyways,
> I think) and your throughput above doesn't seem to be much worse with
> loop alone...
>
> Brian

I'm not familiar with the profiling feature, but I think I'm seeing
the same thing,
requests being fractured in smaller ones.

However, by chance I found something which seems to impact the
performance even more.
I wanted to retry the dd-to-loop-device-with-sync today, the same one
I pasted yesterday.
However, today it was quite different.

torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
303038464 bytes (303 MB) copied, 123.95 s, 2.4 MB/s
^C

So I unmounted the loop device and mounted it again, and re-ran the test.

torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo losetup -d /dev/loop1
torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo losetup -f
loopback.img
torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
2097152000 bytes (2.1 GB) copied, 55.9117 s, 37.5 MB/s

The situation inside the Xen instance was similar, although with
different numbers.

After being on, but mostly idle, for ~5 days.:
torbjorn at hennec:~$ sudo dd if=/dev/zero of=bigfile bs=1024k count=2000
oflag=direct
28311552 bytes (28 MB) copied, 35.1314 s, 806 kB/s
^C

After reboot and a fresh loop device:
torbjorn at hennec:~$ sudo dd if=/dev/zero of=bigfile bs=1024k count=2000
oflag=direct
814743552 bytes (815 MB) copied, 34.7441 s, 23.4 MB/s
^C

These numbers might indicate that loop device performance degrades over time.
However, I haven't seen this on local filesystems, so is this possibly
only with files on Gluster or FUSE ?

I'm on Debian stable, so things aren't exactly box fresh.
torbjorn at xen01:~$ dpkg -l | grep "^ii  linux-image-$(uname -r)"
ii  linux-image-2.6.32-5-xen-amd64      2.6.32-46
Linux 2.6.32 for 64-bit PCs, Xen dom0 support

I'm not sure how to debug the Gluster -> FUSE -> loop device interaction,
but I might try a newer kernel on the client.

From michael.kushnir at nih.gov  Thu Feb 28 15:05:38 2013
From: michael.kushnir at nih.gov (Kushnir, Michael (NIH/NLM/LHC) [C])
Date: Thu, 28 Feb 2013 15:05:38 +0000
Subject: [Gluster-users] Duplicate files on bricks of distributed (not
	replicated) volme
Message-ID: <2B177685F5DDAB479F03287189FD8C53096677AC@MLBXV09.nih.gov>

Good morning everyone, 

I am running GlusterFS v. 3.3.1. I have a test server set up with a distributed (not distributed replicated) volume comprised of 5 bricks. 

I have a directory in the GlusterFS with ~7000 xml files. When mounted over fuse, GlusterFS reports the correct number of files. 

However, when I "ls" the directory on the bricks, brick 1-4 each report ~1400 files, but brick 5 reports ~7000 files including duplicates of files on other bricks. 

How should I go about fixing a situation like this? Is a rebalance the right way to handle this? Will it take care of the duplicates?

Thanks,
Michael

______________________________________________________________________________________
Michael Kushnir
System Architect / Engineer 
Communications Engineering Branch
Lister Hill National Center for Biomedical Communications
National Library of Medicine

8600 Rockville Pike, Building 38A, Floor 10
Besthesda, MD 20894

Phone: 301-435-3219
Email: michael.kushnir at nih.gov 





From bfoster at redhat.com  Thu Feb 28 15:54:06 2013
From: bfoster at redhat.com (Brian Foster)
Date: Thu, 28 Feb 2013 10:54:06 -0500
Subject: [Gluster-users] Performance in VM guests when hosting VM images
 on Gluster
In-Reply-To: <CAD2iGhWZyFEeW=6hrNgzB63P6P_V5BTW2zgSjk2=+PNqTWDsyQ@mail.gmail.com>
References: <CAD2iGhVgoVWDYGAP8KUYY5NE9-_wXd7Q00cZzjdjqkvD8cdacQ@mail.gmail.com>
	<512E7099.9050308@redhat.com>
	<CAD2iGhWZyFEeW=6hrNgzB63P6P_V5BTW2zgSjk2=+PNqTWDsyQ@mail.gmail.com>
Message-ID: <512F7D9E.6010506@redhat.com>

On 02/28/2013 05:58 AM, Torbj?rn Thorsen wrote:
> On Wed, Feb 27, 2013 at 9:46 PM, Brian Foster <bfoster at redhat.com> wrote:
>> On 02/27/2013 10:14 AM, Torbj?rn Thorsen wrote:
>>> I'm seeing less-than-stellar performance on my Gluster deployment when
>>> hosting VM images on the FUSE mount.
...
> 
> I'm not familiar with the profiling feature, but I think I'm seeing
> the same thing,
> requests being fractured in smaller ones.
> 

gluster profiling is pretty straight forward. Just run the commands as
described and you can dump some stats on the workload the volume is seeing:

http://www.gluster.org/community/documentation/index.php/Gluster_3.2:_Running_Gluster_Volume_Profile_Command

The 'info' command will print the stats since the last info invocation,
so you can easily compare results between different workloads provided
the volume is otherwise idle.

> However, by chance I found something which seems to impact the
> performance even more.
> I wanted to retry the dd-to-loop-device-with-sync today, the same one
> I pasted yesterday.
> However, today it was quite different.
> 
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
> if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
> 303038464 bytes (303 MB) copied, 123.95 s, 2.4 MB/s
> ^C
> 

I started testing on a slightly more up to date VM. I'm seeing fairly
consistent 10MB/s with sync I/O. This is with a loop device over a a
file on a locally mounted gluster volume.

> So I unmounted the loop device and mounted it again, and re-ran the test.
> 
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo losetup -d /dev/loop1
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo losetup -f
> loopback.img
> torbjorn at xen01:/srv/ganeti/shared-file-storage/tmp$ sudo dd
> if=/dev/zero of=/dev/loop1 bs=1024k count=2000 oflag=sync
> 2097152000 bytes (2.1 GB) copied, 55.9117 s, 37.5 MB/s
> 

I can reproduce something like this when dealing with non-sync I/O.
Smaller overall writes (relative to available cache) run much faster and
larger write tend to normalize to a lower value. Using xfs_io instead of
dd shows that writes are in fact hitting cache (e.g., smaller writes
complete at 1.5GB/s, larger writes normalize to 35MB/s when we've
dirtied enough memory and flushing/reclaim kicks in). It also appears
that a close() on the loop device results in aggressively flushing
whatever data hasn't been flushed (something fuse also does on open()).
My non-sync results in dd tend to jump around, so perhaps that is a
reason why.

> The situation inside the Xen instance was similar, although with
> different numbers.
> 
> After being on, but mostly idle, for ~5 days.:
> torbjorn at hennec:~$ sudo dd if=/dev/zero of=bigfile bs=1024k count=2000
> oflag=direct
> 28311552 bytes (28 MB) copied, 35.1314 s, 806 kB/s
> ^C
> 
> After reboot and a fresh loop device:
> torbjorn at hennec:~$ sudo dd if=/dev/zero of=bigfile bs=1024k count=2000
> oflag=direct
> 814743552 bytes (815 MB) copied, 34.7441 s, 23.4 MB/s
> ^C
> 
> These numbers might indicate that loop device performance degrades over time.
> However, I haven't seen this on local filesystems, so is this possibly
> only with files on Gluster or FUSE ?

I would expect this kind of behavior when caching is involved, as
described above, but I'm not quite sure what would cause it with sync I/O.

> 
> I'm on Debian stable, so things aren't exactly box fresh.
> torbjorn at xen01:~$ dpkg -l | grep "^ii  linux-image-$(uname -r)"
> ii  linux-image-2.6.32-5-xen-amd64      2.6.32-46
> Linux 2.6.32 for 64-bit PCs, Xen dom0 support
> 
> I'm not sure how to debug the Gluster -> FUSE -> loop device interaction,
> but I might try a newer kernel on the client.
> 


From lanning at lanning.cc  Thu Feb 28 18:16:41 2013
From: lanning at lanning.cc (Robert Hajime Lanning)
Date: Thu, 28 Feb 2013 10:16:41 -0800
Subject: [Gluster-users] Mail list archive bug
Message-ID: <512F9F09.6040509@lanning.cc>

The following email has been truncated in half, from what I believe is a
miss-detection of the start of a new message.

http://www.gluster.org/pipermail/gluster-users/2013-February/035572.html

Where it ends in the archive, it should continue with "From skimming 
through the code..."

/^From /

-- 
Mr. Flibble
King of the Potato People

From tony at filmsolutions.com  Thu Feb 28 18:32:39 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Thu, 28 Feb 2013 18:32:39 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <20130226093603.GA61170@nsrc.org>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
	<20130226093603.GA61170@nsrc.org>
Message-ID: <50528EC4-B140-4AF5-ABB2-C34DC80565BB@filmsolutions.com>

I finally got everything up. However, when transferring files the server locks up.. df hangs etc. In order to get things working I have to kill off processes and unmount for the box to start responding. I put everything back on the NIC cards and transferring files works as expected.

Any ideas?

On Feb 26, 2013, at 1:36 AM, Brian Candler <B.Candler at pobox.com> wrote:

> On Mon, Feb 25, 2013 at 06:28:01PM +0000, Tony Saenz wrote:
>> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
> ...
>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>> Probe on host fpsgluster2ib port 0 already in peer list
> 
> Probing only works in one direction. The HTML admin guide has been taken
> down so I can only point you to the PDF:
> http://www.gluster.org/wp-content/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-US.pdf
> 
> "use the probe command from a storage server that is already part of the
> trusted storage pool."
> 
> That is, probe from existing cluster node to new node, not from new node to
> cluster.


From joe at julianfamily.org  Thu Feb 28 18:53:12 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 28 Feb 2013 10:53:12 -0800
Subject: [Gluster-users] Peer Probe
In-Reply-To: <5E972C36-6632-440C-9AF3-CE9B793D5020@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<6458650.1QYlMVX8eE@stunted>
	<1F275178-B77F-4416-8032-F542E268AED7@filmsolutions.com>
	<2432025.sDFVujSrFS@stunted>
	<5E972C36-6632-440C-9AF3-CE9B793D5020@filmsolutions.com>
Message-ID: <512FA798.9000607@julianfamily.org>

You have a host, fpsgluster, that's a peer. It's assigned a uuid.
You peer probe a different hostname, fpsglusterib, that  reports the 
same uuid - hence, already a peer.
You could remove it and re-add it with the new hostname unless you've 
created volumes using the non-ib hostname. If you created any volumes 
using fpsgluster, that hostname is now locked in. If you try to change 
the hostname for that peer by probing it from another server, it should 
fail because there's a volume using that hostname. It can't /add/ the 
new hostname because that uuid is already in use.

If you want to brute-force it there's two ways of making it happen. Both 
involve downtime.

 1. delete the volume(s)
    delete the server(s) from the peer group (gluster peer detach ...)
    re-add the server(s) (gluster peer probe ...ib) remove the extended
    attributes that mark a brick as having been part of a volume (
    http://joejulian.name/blog/glusterfs-path-or-a-prefix-of-it-is-already-part-of-a-volume/
    )
    re-create the volume(s)
 2. stop your volume(s) (gluster volume stop ...)
    kill glusterd on all servers
    search and replace all instances of the hostname under
    /var/lib/glusterd on each server (find -type f /var/lib/glusterd |
    xargs sed -i 's/fpsgluster/fpsglusterib/g') (This example presumes
    that sed expression is safe. You'll have to ensure that for yourself.)
    start glusterd on all servers
    start your volume(s) (gluster volume start ...)

Method 1 is the "supported" method.


On 02/25/2013 03:07 PM, Tony Saenz wrote:
> [root at fpsgluster ~]# gluster volume create testvault replica 2 transport rdma,tcp fpsglusterib:/mnt/testbrick1 fpsgluster2ib:/mnt/testbrick1 fpsglusterib:/mnt/testbrick2 fpsgluster2ib:/mnt/testbrick2 fpsglusterib:/mnt/testbrick3 fpsgluster2ib:/mnt/testbrick3
> Host fpsgluster2ib not a friend
>
> Not a friend error. I'm at a loss.
>
> On Feb 25, 2013, at 2:40 PM, harry mangalam <harry.mangalam at uci.edu>
>   wrote:
>
>> That looks OK (but your 2 MTUs are mismatched - should fix that).
>>
>>>           UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1  <- 1st
>>>           UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1 <- 2nd
>>                                                 ^^^^^
>> IBDEV=ibX
>> modprobe ib_umad
>> modprobe ib_ipoib
>> echo connected > /sys/class/net/${IBDEV}/mode
>> echo 65520 > /sys/class/net/${IBDEV}/mtu
>>
>> how did you set up the peering? By name?  by IP#?
>> (I assume pinging by hostname also works both ways?)
>>
>> If you can't get the peers to ack, then what do the logs say on failure to:
>>
>> peer probe <host>
>>   or create the volume
>> gluster volume create <volname>    host1ib:/gl_part   host2ib:/gl_part
>>
>> hjm
>>
>> On Monday, February 25, 2013 09:50:02 PM Tony Saenz wrote:
>>> Trying to first get this working with IPoIB
>>>
>>> [root at fpsgluster ~]# ibhosts
>>> Ca	: 0x00117500007937b2 ports 1 "fpsgluster2 qib0"
>>> Ca	: 0x0011750000792af2 ports 1 "fpsgluster qib0"
>>>
>>> I'm able to ping the other box from Infiniband to Infiniband card
>>>
>>> Ifconfig uses the ioctl access method to get the full address information,
>>> which limits hardware addresses to 8 bytes. Because Infiniband address has
>>> 20 bytes, only the first 8 bytes are displayed correctly. Ifconfig is
>>> obsolete! For replacement check ip.
>>> ib0       Link encap:InfiniBand  HWaddr
>>> 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00 inet
>>> addr:10.0.4.35  Bcast:10.0.4.255  Mask:255.255.255.0 inet6 addr:
>>> fe80::211:7500:79:2af2/64 Scope:Link
>>>           UP BROADCAST RUNNING MULTICAST  MTU:2044  Metric:1
>>>           RX packets:1567 errors:0 dropped:0 overruns:0 frame:0
>>>           TX packets:587 errors:0 dropped:24 overruns:0 carrier:0
>>>           collisions:0 txqueuelen:256
>>>           RX bytes:342622 (334.5 KiB)  TX bytes:96554 (94.2 KiB)
>>>
>>> [root at fpsgluster2 ~]# ifconfig ib0
>>> Ifconfig uses the ioctl access method to get the full address information,
>>> which limits hardware addresses to 8 bytes. Because Infiniband address has
>>> 20 bytes, only the first 8 bytes are displayed correctly. Ifconfig is
>>> obsolete! For replacement check ip.
>>> ib0       Link encap:InfiniBand  HWaddr
>>> 80:00:00:03:FE:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00 inet
>>> addr:10.0.4.34  Bcast:10.0.4.255  Mask:255.255.255.0 inet6 addr:
>>> fe80::211:7500:79:37b2/64 Scope:Link
>>>           UP BROADCAST RUNNING MULTICAST  MTU:65520  Metric:1
>>>           RX packets:599 errors:0 dropped:0 overruns:0 frame:0
>>>           TX packets:1558 errors:0 dropped:8 overruns:0 carrier:0
>>>           collisions:0 txqueuelen:256
>>>           RX bytes:95180 (92.9 KiB)  TX bytes:346728 (338.6 KiB)
>>>
>>> [root at fpsgluster ~]# ping -I ib0 10.0.4.34
>>> PING 10.0.4.34 (10.0.4.34) from 10.0.4.35 ib0: 56(84) bytes of data.
>>> 64 bytes from 10.0.4.34: icmp_seq=1 ttl=64 time=12.6 ms
>>> 64 bytes from 10.0.4.34: icmp_seq=2 ttl=64 time=0.184 ms
>>>
>>> /etc/hosts looks correct
>>>
>>> [root at fpsgluster2 ~]# cat /etc/hosts | grep ib
>>> 10.0.4.35      fpsglusterib
>>> 10.0.4.34      fpsgluster2ib
>>>
>>> [root at fpsgluster ~]# cat /etc/hosts| grep ib
>>> 10.0.4.35      fpsglusterib
>>> 10.0.4.34      fpsgluster2ib
>>>
>>> I haven't created the new volume yet as I can't get the peer probe to work
>>> off the Infiniband card. It's only seeing the NIC cards I currently have it
>>> hooked in to.
>>>
>>>
>>> On Feb 25, 2013, at 11:57 AM, harry mangalam <harry.mangalam at uci.edu>
>>>
>>> wrote:
>>>> It /might be/probably is/ DNS-related.
>>>>
>>>> Are you trying to do this with RDMA or IPoIB?
>>>>
>>>> If IPoIB, are ALL your /etc/hosts files in sync (IB names separate and
>>>> distinct from the ethernet interfaces) and responsive to the appropriate
>>>> interfaces?
>>>>
>>>> Do the IB interfaces show up as distinct (and connected) on an 'ifconfig
>>>> -a' and 'ibstat' dump?
>>>>
>>>> Do all the peers show up on an 'ibhosts' query?
>>>>
>>>> What is the output of:
>>>> gluster volume status <your_volume>
>>>> and
>>>> gluster volume status <your_volume> detail
>>>>
>>>>
>>>> hjm
>>>>
>>>> On Monday, February 25, 2013 07:46:00 PM Tony Saenz wrote:
>>>>> It shows this but it's still going through my NIC cards and not the
>>>>> Infiniband. (Checked the traffic on the cards themselves)
>>>>>
>>>>> [root at fpsgluster ~]# gluster peer status
>>>>> Number of Peers: 1
>>>>>
>>>>> Hostname: fpsgluster2
>>>>> Uuid: 9b7e7c2d-f05b-4cc8-b55a-571e383328d0
>>>>> State: Peer in Cluster (Connected)
>>>>>
>>>>> On Feb 25, 2013, at 10:51 AM, Torbj?rn Thorsen <torbjorn at trollweb.no>
>> wrote:
>>>>>> Your error message seems to indicate that the peer is already in the
>>>>>> storage pool ?
>>>>>> What is the output of "gluster peer status" ?
>>>>>>
>>>>>> On Mon, Feb 25, 2013 at 7:28 PM, Tony Saenz <tony at filmsolutions.com>
>>>> wrote:
>>>>>>> Any help please? The regular NICs are fine which is what it currently
>>>>>>> sees but I'd like to move them over to the Infiniband cards.>>
>>>>>>> On Feb 22, 2013, at 1:50 PM, Anthony Saenz <tony at filmsolutions.com>
>>>> wrote:
>>>>>>>> Hey,
>>>>>>>>
>>>>>>>> I was wondering if I could get a bit of help.. I installed a new
>>>>>>>> Infiniband card into my servers but I'm unable to get it to come up as
>>>>>>>> a peer. Is there something I'm missing?
>>>>>>>>
>>>>>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>>>>>>> Probe on host fpsgluster2ib port 0 already in peer list
>>>>>>>>
>>>>>>>> [root at fpsgluster testvault]# yum list installed | grep gluster
>>>>>>>> glusterfs.x86_64                       3.3.1-1.el6
>>>>>>>> installed
>>>>>>>> glusterfs-devel.x86_64                 3.3.1-1.el6
>>>>>>>> installed
>>>>>>>> glusterfs-fuse.x86_64                  3.3.1-1.el6
>>>>>>>> installed
>>>>>>>> glusterfs-geo-replication.x86_64       3.3.1-1.el6
>>>>>>>> installed
>>>>>>>> glusterfs-rdma.x86_64                  3.3.1-1.el6
>>>>>>>> installed
>>>>>>>> glusterfs-server.x86_64                3.3.1-1.el6
>>>>>>>> installed
>>>>>>>>
>>>>>>>> Thanks.
>>>>>>> _______________________________________________
>>>>>>> Gluster-users mailing list
>>>>>>> Gluster-users at gluster.org
>>>>>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>>>>> --
>>>>>> Vennlig hilsen
>>>>>> Torbj?rn Thorsen
>>>>>> Utvikler / driftstekniker
>>>>>>
>>>>>> Trollweb Solutions AS
>>>>>> - Professional Magento Partner
>>>>>> www.trollweb.no
>>>>>>
>>>>>> Telefon dagtid: +47 51215300
>>>>>> Telefon kveld/helg: For kunder med Serviceavtale
>>>>>>
>>>>>> Bes?ksadresse: Luramyrveien 40, 4313 Sandnes
>>>>>> Postadresse: Maurholen 57, 4316 Sandnes
>>>>>>
>>>>>> Husk at alle v?re standard-vilk?r alltid er gjeldende
>>>>> _______________________________________________
>>>>> Gluster-users mailing list
>>>>> Gluster-users at gluster.org
>>>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>>>> ---
>>>> Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
>>>> [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
>>>> 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
>>>> MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
>>>> ---
>>>> "Something must be done. [X] is something. Therefore, we must do it."
>>>> Bruce Schneier, on American response to just about anything.
>>> _______________________________________________
>>> Gluster-users mailing list
>>> Gluster-users at gluster.org
>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
>> ---
>> Harry Mangalam - Research Computing, OIT, Rm 225 MSTB, UC Irvine
>> [m/c 2225] / 92697 Google Voice Multiplexer: (949) 478-4487
>> 415 South Circle View Dr, Irvine, CA, 92697 [shipping]
>> MSTB Lat/Long: (33.642025,-117.844414) (paste into Google Maps)
>> ---
>> "Something must be done. [X] is something. Therefore, we must do it."
>> Bruce Schneier, on American response to just about anything.
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130228/1ce3abd4/attachment-0001.html>

From joe at julianfamily.org  Thu Feb 28 18:55:18 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 28 Feb 2013 10:55:18 -0800
Subject: [Gluster-users] Peer Probe
In-Reply-To: <50528EC4-B140-4AF5-ABB2-C34DC80565BB@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
	<20130226093603.GA61170@nsrc.org>
	<50528EC4-B140-4AF5-ABB2-C34DC80565BB@filmsolutions.com>
Message-ID: <512FA816.2070703@julianfamily.org>

Are your bricks formatted ext4?

On 02/28/2013 10:32 AM, Tony Saenz wrote:
> I finally got everything up. However, when transferring files the server locks up.. df hangs etc. In order to get things working I have to kill off processes and unmount for the box to start responding. I put everything back on the NIC cards and transferring files works as expected.
>
> Any ideas?
>
> On Feb 26, 2013, at 1:36 AM, Brian Candler <B.Candler at pobox.com> wrote:
>
>> On Mon, Feb 25, 2013 at 06:28:01PM +0000, Tony Saenz wrote:
>>> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
>> ...
>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>>> Probe on host fpsgluster2ib port 0 already in peer list
>> Probing only works in one direction. The HTML admin guide has been taken
>> down so I can only point you to the PDF:
>> http://www.gluster.org/wp-content/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-US.pdf
>>
>> "use the probe command from a storage server that is already part of the
>> trusted storage pool."
>>
>> That is, probe from existing cluster node to new node, not from new node to
>> cluster.
> _______________________________________________
> Gluster-users mailing list
> Gluster-users at gluster.org
> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From tony at filmsolutions.com  Thu Feb 28 19:32:47 2013
From: tony at filmsolutions.com (Tony Saenz)
Date: Thu, 28 Feb 2013 19:32:47 +0000
Subject: [Gluster-users] Peer Probe
In-Reply-To: <512FA816.2070703@julianfamily.org>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
	<20130226093603.GA61170@nsrc.org>
	<50528EC4-B140-4AF5-ABB2-C34DC80565BB@filmsolutions.com>
	<512FA816.2070703@julianfamily.org>
Message-ID: <0A1FDA6E-CF05-4A11-8C7C-58086E0E178A@filmsolutions.com>

No, they're XFS and thanks for the other tip! Is there something missing? It's pretty consistent.. Reads are fine but as soon as I transfer/copy files the mount starts hanging and the server locks up.

On Feb 28, 2013, at 10:55 AM, Joe Julian <joe at julianfamily.org>
 wrote:

> Are your bricks formatted ext4?
> 
> On 02/28/2013 10:32 AM, Tony Saenz wrote:
>> I finally got everything up. However, when transferring files the server locks up.. df hangs etc. In order to get things working I have to kill off processes and unmount for the box to start responding. I put everything back on the NIC cards and transferring files works as expected.
>> 
>> Any ideas?
>> 
>> On Feb 26, 2013, at 1:36 AM, Brian Candler <B.Candler at pobox.com> wrote:
>> 
>>> On Mon, Feb 25, 2013 at 06:28:01PM +0000, Tony Saenz wrote:
>>>> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
>>> ...
>>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>>>> Probe on host fpsgluster2ib port 0 already in peer list
>>> Probing only works in one direction. The HTML admin guide has been taken
>>> down so I can only point you to the PDF:
>>> http://www.gluster.org/wp-content/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-US.pdf
>>> 
>>> "use the probe command from a storage server that is already part of the
>>> trusted storage pool."
>>> 
>>> That is, probe from existing cluster node to new node, not from new node to
>>> cluster.
>> _______________________________________________
>> Gluster-users mailing list
>> Gluster-users at gluster.org
>> http://supercolony.gluster.org/mailman/listinfo/gluster-users
> 


From bfoster at redhat.com  Thu Feb 28 19:36:09 2013
From: bfoster at redhat.com (Brian Foster)
Date: Thu, 28 Feb 2013 14:36:09 -0500
Subject: [Gluster-users] Mail list archive bug
In-Reply-To: <512F9F09.6040509@lanning.cc>
References: <512F9F09.6040509@lanning.cc>
Message-ID: <512FB1A9.9050600@redhat.com>

On 02/28/2013 01:16 PM, Robert Hajime Lanning wrote:
> The following email has been truncated in half, from what I believe is a
> miss-detection of the start of a new message.
> 
> http://www.gluster.org/pipermail/gluster-users/2013-February/035572.html
> 
> Where it ends in the archive, it should continue with "From skimming
> through the code..."
> 

Wow, that is strange. That mail is definitely cut off. Thanks for
catching that. FWIW, the following shows the complete mail:

http://www.mail-archive.com/gluster-users at gluster.org/msg11125.html

Brian

> /^From /
> 


From joe at julianfamily.org  Thu Feb 28 19:50:32 2013
From: joe at julianfamily.org (Joe Julian)
Date: Thu, 28 Feb 2013 11:50:32 -0800
Subject: [Gluster-users] Volume locking up when used over IB (was Re:
 Peer Probe)
In-Reply-To: <0A1FDA6E-CF05-4A11-8C7C-58086E0E178A@filmsolutions.com>
References: <A3C75D63-816A-497B-9789-FE5F38A6F636@filmsolutions.com>
	<34EA1AA9-BD1D-4C4F-A2D3-E5C55ADD360A@filmsolutions.com>
	<20130226093603.GA61170@nsrc.org>
	<50528EC4-B140-4AF5-ABB2-C34DC80565BB@filmsolutions.com>
	<512FA816.2070703@julianfamily.org>
	<0A1FDA6E-CF05-4A11-8C7C-58086E0E178A@filmsolutions.com>
Message-ID: <512FB508.9060701@julianfamily.org>

Where are you copying from? localhost mount, or some other client?

Check your client log. Look for clues there. If you can't find any then 
continue with these diagnostic techniques.

kill -USR1 <glusterfs pid>

That will create a dump file in /tmp of the client's state. If you can 
determine which brick it's stuck on, getting a dump of that brick's 
glusterfsd could be useful as well. Include that with a bug report. I 
still haven't figured out how to read that myself though.

Attach an strace and/or gdb to the client and/or brick server and see 
where it's locked up. Be sure to backtrace all threads in gdb. Again, 
include those with a bug report.

Finally, of course, there's the possibility that it's your IB drivers.

On 02/28/2013 11:32 AM, Tony Saenz wrote:
> No, they're XFS and thanks for the other tip! Is there something missing? It's pretty consistent.. Reads are fine but as soon as I transfer/copy files the mount starts hanging and the server locks up.
>
> On Feb 28, 2013, at 10:55 AM, Joe Julian <joe at julianfamily.org>
>   wrote:
>
>> Are your bricks formatted ext4?
>>
>> On 02/28/2013 10:32 AM, Tony Saenz wrote:
>>> I finally got everything up. However, when transferring files the server locks up.. df hangs etc. In order to get things working I have to kill off processes and unmount for the box to start responding. I put everything back on the NIC cards and transferring files works as expected.
>>>
>>> Any ideas?
>>>
>>> On Feb 26, 2013, at 1:36 AM, Brian Candler <B.Candler at pobox.com> wrote:
>>>
>>>> On Mon, Feb 25, 2013 at 06:28:01PM +0000, Tony Saenz wrote:
>>>>> Any help please? The regular NICs are fine which is what it currently sees but I'd like to move them over to the Infiniband cards.
>>>> ...
>>>>>> [root at fpsgluster testvault]# gluster peer probe fpsgluster2ib
>>>>>> Probe on host fpsgluster2ib port 0 already in peer list
>>>> Probing only works in one direction. The HTML admin guide has been taken
>>>> down so I can only point you to the PDF:
>>>> http://www.gluster.org/wp-content/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-US.pdf
>>>>
>>>> "use the probe command from a storage server that is already part of the
>>>> trusted storage pool."
>>>>
>>>> That is, probe from existing cluster node to new node, not from new node to
>>>> cluster.
>>> _______________________________________________
>>> Gluster-users mailing list
>>> Gluster-users at gluster.org
>>> http://supercolony.gluster.org/mailman/listinfo/gluster-users


From todd at stansell.org  Thu Feb 28 23:23:34 2013
From: todd at stansell.org (Todd Stansell)
Date: Thu, 28 Feb 2013 23:23:34 +0000
Subject: [Gluster-users] timestamps getting updated during self-heal after
	primary brick rebuild
Message-ID: <20130228232334.GA2813@underdog.stansell.org>

We're looking at using glusterfs to provide a shared filesystem between two
nodes, using just local disk.  They are both gluster servers as well as
clients.  This is on CentOS 5.9 64-bit.  The bricks are simply ext3
filesystems on top of LVM:

    /dev/mapper/VolGroup00-LogVol0 on /gfs0 type ext3 (rw,user_xattr)

We set up a test volume with:

    host14# gluster volume create gv0 replica 2 transport tcp host14:/gfs0 host13:/gfs0
    host14# gluster volume set gv0 nfs.disable on
    host14# gluster volume start gv0

This works just fine.  The issue is simulating hardware failure where we need
to rebuild an entire node.  In this case, we kickstart our server which creates
all fresh new filesystems.  We have a kickstart postinstall script that sets
the glusterd UUID of the server so that it never changes.  It then does a probe
of the other server, looks for existing volumes, sets up fstab entries for them
(to also act as a client) and also sets up an init script to force a full heal
every time the server boots just to ensure all data is replicated to both
nodes.  All of this works great when I'm rebuilding the second brick.

The issue I have is when we rebuild the server that hosts the primary brick
(host14:/gfs0).  It will come online and start copying data from host13:/gfs0,
but as it does so, it sets the timestamps of the files on host13:/gfs0 to the
time it healed the data on host14:/gfs0.  As a result, all files in the
filesystem end up with timestamps of when the first brick was healed.

I enabled client debug logs and the following indicates that it *thinks* it is
doing the right thing:

    after rebuilding gv0-client-1:
    [2013-02-28 00:01:37.264018] D [afr-self-heal-metadata.c:329:afr_sh_metadata_sync] 0-gv0-replicate-0: self-healing metadata of /data/bin/sync-data from gv0-client-0 to gv0-client-1

    after rebuilding gv0-client-0:
    [2013-02-28 00:17:03.578377] D [afr-self-heal-metadata.c:329:afr_sh_metadata_sync] 0-gv0-replicate-0: self-healing metadata of /data/bin/sync-data from gv0-client-1 to gv0-client-0

Unfortunately, in the second case, the timestamp of the files changed from:

    -r-xr-xr-x 1 root root 2717 Feb 27 23:32 /data/data/bin/sync-data*

 to:

    -r-xr-xr-x 1 root root 2717 Feb 28 00:17 /data/data/bin/sync-data*

And remember, there's nothing accessing any data in this volume so there's no
"client" access going on anywhere.  No changes happening on the filesystem,
other than self-heal screwing things up.

The only thing I could find in any logs that would indicate a problem was this
in the brick log:

    [2013-02-28 00:17:03.583063] D [posix.c:323:posix_do_utimes] 0-gv0-posix: /gfs0/data/bin/sync-data (Function not implemented)

I've also now built a Centos 6 host and verified that the same behavior
happens there, though I get a slightly different brick debug log (which makes
me think this has nothing to do with what I'm seeing):

    [2013-02-28 23:07:41.879440] D [posix.c:262:posix_do_chmod] 0-gv0-posix: /gfs0/data/bin/sync-data (Function not implemented)

Here's any basic info that might help folks know what's going on:

# rpm -qa | grep gluster
glusterfs-server-3.3.1-1.el5
glusterfs-3.3.1-1.el5
glusterfs-fuse-3.3.1-1.el5

# gluster volume info

Volume Name: gv0
Type: Replicate
Volume ID: 7cec2ba3-f69c-409a-a259-0d055792b11a
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: host14:/gfs0
Brick2: host13:/gfs0
Options Reconfigured:
diagnostics.brick-log-level: DEBUG
diagnostics.client-log-level: DEBUG
nfs.disable: on

Todd

From alex8224 at gmail.com  Thu Feb 28 11:05:04 2013
From: alex8224 at gmail.com (Zhang Alex)
Date: Thu, 28 Feb 2013 19:05:04 +0800
Subject: [Gluster-users] When i change hostname, geo-replication Faulty !
Message-ID: <CAFiWg2YwkD-Jp-6ZfqanbibzpATUbhjyf9M3eAK-h18ds8F1uw@mail.gmail.com>

my master gluster machine is 172.16.29.133 ,hostname glustermaster
,volumename: testvolume
my slave gluster machine is 172.16.29.134, hostname glusterslave
 volumename: testvolume

when i first setup geo-replication ,i using following command:

 gluster volume geo-replication testvolume
glusterslave:/ftp_home/testvolume start

it worked


but ,when change glusterslave's ip address to 172.16.26.135, it output
"Faulty!" What happened?

i' sure i update the host file and restart glusterd

how can i slove the problem or need another settings?


tks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130228/4d6a9938/attachment-0001.html>

From rahul51.s at tcs.com  Thu Feb 28 11:52:54 2013
From: rahul51.s at tcs.com (Rahul51 S)
Date: Thu, 28 Feb 2013 17:22:54 +0530
Subject: [Gluster-users] Issue in the syncing operation for the same file on
	two nodes
Message-ID: <OFF94C0269.46652A3E-ON65257B20.003EDBBA-65257B20.004144B0@tcs.com>

Hi All,

Below are the sequence of steps taken by me produce the issue

1) I created a file named DEMO of size 10K on one of the replicate gluster 
volume on SERVER 1 and it got replicated on the other gluster volume of 
SERVER 2 as usual.
2) After this, I unmounted the brick of SERVER 2 and stopped the glusterd 
service on SERVER 2.
3) I then filled the DEMO file to the size of 2M on the SERVER 1.
4) I mounted the brick and started the glusterd on SERVER 2.

I found that the DEMO file is still 10K in SERVER 2 and 2M in SERVER1 and 
it never got synced. 
It is only after I opened the DEMO file and closed it, the file got synced 
on both the servers. 
Ideally the DEMO file should sync when the glusterd service was started. 
But, this seems to be not happening. 
Could you please shed some light on this. 


Regards
Rahul Shrivastava
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130228/1add8ce5/attachment-0001.html>

From rahul51.s at tcs.com  Thu Feb 28 12:06:32 2013
From: rahul51.s at tcs.com (Rahul51 S)
Date: Thu, 28 Feb 2013 17:36:32 +0530
Subject: [Gluster-users] glusterFS issue in the syncing operation for the
	same files on two servers
Message-ID: <OF94C9E42C.EBF1D505-ON65257B20.004265B7-65257B20.0042845E@tcs.com>

Hi All,

Below are the sequence of steps taken by me produce the issue

1) I created a file named DEMO of size 10K on one of the replicate gluster 
volume on SERVER 1 and it got replicated on the other gluster volume of 
SERVER 2 as usual.
2) After this, I unmounted the brick of SERVER 2 and stopped the glusterd 
service on SERVER 2.
3) I then filled the DEMO file to the size of 2M on the SERVER 1.
4) I mounted the brick and started the glusterd on SERVER 2.

I found that the DEMO file is still 10K in SERVER 2 and 2M in SERVER1 and 
it never got synced. 
It is only after I opened the DEMO file and closed it, the file got synced 
on both the servers. 
Ideally the DEMO file should sync when the glusterd service was started. 
But, this seems to be not happening. 
Could you please shed some light on this. 


Regards
Rahul Shrivastava
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://supercolony.gluster.org/pipermail/gluster-users/attachments/20130228/94f6cfb1/attachment-0001.html>

